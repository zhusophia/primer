[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":" isn’t book ’re looking .First, book students classes. Everything book designed make experience students better. hope material may useful people outside class.Second, book changes time. --date possible.Third, highly opinionated matters . might share views.","code":""},{"path":"preamble.html","id":"preamble","chapter":"Preamble","heading":"Preamble","text":"","code":""},{"path":"preamble.html","id":"dedication","chapter":"Preamble","heading":"Dedication","text":"romantic, Kay —\nlove?\nNeed ask anyone tell us things?","code":""},{"path":"preamble.html","id":"acknowledgements","chapter":"Preamble","heading":"Acknowledgements","text":"work builds contributions many people R Open Source communities. particular, like acknowledge extensive material taken Diez, Barr, Çetinkaya-Rundel (2014), Grolemund Wickham (2017), Irizarry (2019), Kim Ismay (2019), Jenny Bryan (2019), Diez, Barr, Çetinkaya-Rundel (2014), Downey (2012), Grolemund Wickham (2017), Kuhn Silge (2020), Timbers, Campbell, Lee (2021), Legler Roback (2019).Alboukadel Kassambara, Andrew Tran, Thomas Mock others kindly allowed re-use /modification work.Thanks contributions Harvard students, colleagues random people met internet: Albert Rivero, Nicholas Dow, Celine Vendler, Sophia Zheng, Maria Burzillo, Robert McKenzie, Deborah Gonzalez, Beau Meche, Evelyn Cai, Miro Bergam, Jessica Edwards, Emma Freeman, Cassidy Bargell, Yao Yu, Vivian Zhang, Ishan Bhatt, Mak Famulari, Tahmid Ahmed, Eliot Min, Hannah Valencia, Asmer Safi, Erin Guetzloe, Shea Jenkins, Thomas Weiss, Diego Martinez, Andy Wang, Tyler Simko, Jake Berg, Connor Rust, Liam Rust, Alla Baranovsky, Carine Hajjar, Diego Arias, Stephanie Yao Tyler Simko.Also, Becca Gill, Ajay Malik, Heather Li, Nosa Lawani, Stephanie Saab, Nuo Wen Lei, Anmay Gupta Dario Anaya.Also, Kevin Xu, Anmay Gupta, Sophia Zhu, Arghayan Jeiyasarangkan, Yuhan Wu, Ryan Southward, George Pentchev, Ahmet Atilla Colak, Mahima Malhotra, Shreeram Patkar.like gratefully acknowledge funding Derek Bok Center Teaching Learning Harvard University, via Digital Teaching Fellows Learning Lab Undergraduate Fellows programs.David Kane(former) Preceptor Statistical Methods MathematicsDepartment GovernmentHarvard University","code":""},{"path":"preamble.html","id":"license","chapter":"Preamble","heading":"License","text":"work licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"getting-started.html","id":"getting-started","chapter":"Getting Started","heading":"Getting Started","text":"world confronts us. Make decisions must.","code":""},{"path":"getting-started.html","id":"installing-r-and-rstudio","chapter":"Getting Started","heading":"Installing R and RStudio","text":"\nFIGURE 0.1: Analogy difference R RStudio.\nThroughout book, assume using R via RStudio. R RStudio car’s engine dashboard.precisely, R programming language runs computations, RStudio integrated development environment (IDE) provides interface many convenient features. Just access speedometer, rearview mirrors, navigation system makes driving much easier, using RStudio’s interface makes using R much easier.Download install R RStudio (Desktop version) computer.first: Download install R.first: Download install R.second: Download install RStudio Desktop (free version).second: Download install RStudio Desktop (free version).","code":""},{"path":"getting-started.html","id":"using-r-via-rstudio","chapter":"Getting Started","heading":"Using R via RStudio","text":"\nFIGURE 0.2: Icons R versus RStudio computer.\nMuch don’t drive car interacting directly engine rather interacting elements car’s dashboard, won’t using R directly rather use RStudio’s interface. install R RStudio computer, ’ll two new programs (also called applications) can open. Always work RStudio directly R application.Let’s begin getting familiar RStudio. Open RStudio. see three panes, panels, dividing screen: Console pane, Files pane, Environment pane.workspace. Start big pane left:three panels (tabs) window, ’ll focusing Console Terminal. first start R, Console gives information version R. Console can type run R code. example, type 1 + 1 hit return, Console returns 2.Look top right:main two tabs ’ll using Environment Git (yet visible). Environment tab shows datasets variables currently loaded R. case, loaded dataset 3407 rows 5 columns variable x equal 5. , Environment empty. Let’s change . Go Console type:Now, hit return/enter see variable x equal 5 Environment tab. must always hit return/enter typing command, otherwise RStudio realize want R execute command. Look bottom right window:Files tab displays computer’s file system. create project later, tab automatically show contents project’s folder. Plots tab show preview plots make RStudio. Help discussed later.","code":"\nx <- 5"},{"path":"getting-started.html","id":"package-installation","chapter":"Getting Started","heading":"Package installation","text":"R packages, also known libraries, extend power R providing additional functions data.\nFIGURE 0.3: Analogy R versus R packages.\nR like new mobile phone: certain amount features use first time, doesn’t everything. R packages like apps can download onto phone.Consider analogy Instagram. new phone want share photo friends. need :Install app: Since phone new include Instagram app, need download app. . (might need future update app.)Open app: ’ve installed Instagram, need open . need every time use app.process similar R package. need :\nFIGURE 0.4: Installing versus loading R package\nInstall package: like installing app phone. packages installed default install R RStudio. Thus want use package first time, need install . ’ve installed package, likely won’t install unless want update newer version.“Load” package: “Loading” package like opening app phone. Packages “loaded” default start RStudio. need “load” package want use every time restart RStudio.Let’s install three packages need Primer. Console pane within RStudio, type:press Return/Enter keyboard. Note must include quotation marks around names packages. Note packages depend packages, automatically installed.One tricky aspect process R occasionally ask :Unless good reason , always answer “” question.R packages generally live one two places:CRAN (rhymes “clan”) mature, popular packages. Use install.packages(), .CRAN (rhymes “clan”) mature, popular packages. Use install.packages(), .Github experimental, less stable packages. Use remotes::install_github(). end section, install one package Github.Github experimental, less stable packages. Use remotes::install_github(). end section, install one package Github.","code":"\ninstall.packages(c(\"remotes\", \"tidyverse\", \"usethis\"))Do you want to install from sources the packages which \nneed compilation? (Yes/no/cancel)"},{"path":"getting-started.html","id":"package-loading","chapter":"Getting Started","heading":"Package loading","text":"Recall ’ve installed package, need “load” . using library() command.example, load remotes package, run following code Console. mean “run following code?” Either type copy--paste code Console hit enter/return key.running earlier code, blinking cursor appear next > symbol. (> generally referred “prompt.”) means successful remotes package now loaded ready use. , however, get red “error message” reads:haven’t successfully installed package. get error message, make sure install remotes package proceeding.historical reasons packages also known libraries, relevant command loading library().Note R occasionally ask want install packages. Almost time want , otherwise R asking .","code":"\nlibrary(remotes)Error in library(remotes) : there is no package called ‘remotes’"},{"path":"getting-started.html","id":"package-use","chapter":"Getting Started","heading":"Package use","text":"load package want use every time start RStudio. don’t load package attempting use one features, see error message like:different error message one just saw package installed yet. R telling trying use function package yet loaded. R doesn’t know “find” function want use.Let’s install package available CRAN: primer.tutorials. Copy paste following R Console:Depending computer/browser/locale, might fail, especially quotation marks paste overturn “curly.” case, type commands .Many new packages installed, including primer.data, provides data sets use Primer. may take minutes. something gets messed , often useful use remove.packages() command remove problematic package install .","code":"Error: could not find function\nlibrary(remotes)\nremotes::install_github(\"PPBDS/primer.tutorials\")"},{"path":"getting-started.html","id":"rstudio-set-up","chapter":"Getting Started","heading":"RStudio set up","text":"thank us later run commands now:changes default value RStudio start session afresh, junk leftover last session. makes much easier create reproducible analysis.","code":"\nlibrary(usethis)\nuse_blank_slate()"},{"path":"getting-started.html","id":"tutorials","chapter":"Getting Started","heading":"Tutorials","text":"chapter textbook, one tutorials available primer.tutorials package. order access tutorials, run library(primer.tutorials) R Console.can access tutorials via Tutorial pane top right tab RStudio. Click “Start tutorial” “Getting Started” tutorial. don’t see tutorials, try clicking “Home” button – little house symbol thin red roof upper right. may need restart R session. Click “Session” menu select “Restart R.”order expand window, can drag enlarge tutorial pane inside RStudio. order open pop-window, click “Show New Window” icon next home icon.may notice Jobs tab lower left create output tutorial starting . RStudio running code create tutorial. accidentally clicked “Start Tutorial” like stop job running, can click back arrow Jobs tab, press red stop sign icon.work saved RStudio sessions. can complete tutorial multiple sittings. completed tutorial, follow instructions tutorial Submit panel upload resulting file Canvas.Tutorials graded pass/fail. hard fail. long make honest attempt, pass easily.Now ? ways can close tutorial safely can quit RStudio session.clicked “Show new window” working tutorial pop-window, simply X pop-window.working tutorial inside Tutorial pane RStudio, simply press red stop sign icon.","code":""},{"path":"getting-started.html","id":"summary","chapter":"Getting Started","heading":"Summary","text":"done following:Installed latest versions R RStudio.Installed latest versions R RStudio.Installed, CRAN, three packages:Installed, CRAN, three packages:Installed, Github, package:Set RStudio preferences:","code":"\ninstall.packages(c(\"remotes\", \"tidyverse\", \"usethis\"))\nremotes::install_github(\"PPBDS/primer.tutorials\")\nusethis::use_blank_slate()"},{"path":"visualization.html","id":"visualization","chapter":"1 Visualization","heading":"1 Visualization","text":"Everyone loves visualizations.read chapter, completed associated tutorials, able create graphics like one data. Join us journey.","code":""},{"path":"visualization.html","id":"getting-started-1","chapter":"1 Visualization","heading":"1.1 Getting Started","text":"chapter focuses ggplot2, one core packages tidyverse. access datasets, help pages, functions use chapter, load tidyverse:one line code loads packages associated tidyverse, packages use almost every data analysis. first time load tidyverse, R report functions tidyverse conflict functions base R packages may loaded. (hide messages book ugly.)might get error message:happens, need install package:, run library(tidyverse) .","code":"\nlibrary(tidyverse)Error in library(tidyverse) : there is no package called ‘tidyverse’\ninstall.packages(\"tidyverse\")"},{"path":"visualization.html","id":"how-do-i-code-in-r","chapter":"1 Visualization","heading":"1.1.1 How do I code in R?","text":"R interpreted language. means type commands written R code. words, code/program R. use terms “coding” “programming” interchangeably.want slower introduction providing , check short book, Getting Used R, RStudio, R Markdown. Ismay Kennedy (2016) include screencast recordings can follow along pause learn. include introduction R Markdown, tool used reproducible research R.Remember: need practice every day. Learning code/program like learning foreign language. Daily practice sine qua non progress.","code":""},{"path":"visualization.html","id":"terminology","chapter":"1 Visualization","heading":"1.1.2 Terminology","text":"use different font distinguish regular text computer_code.Console pane: enter commands RStudio.Console pane: enter commands RStudio.Running code: act telling R put code action.Running code: act telling R put code action.Objects: values stored R. can assign values objects display contents objects. Use assignment operator <- . can choose almost name like object, long name begin number special character like +, *, -, /, ^, etc.Objects: values stored R. can assign values objects display contents objects. Use assignment operator <- . can choose almost name like object, long name begin number special character like +, *, -, /, ^, etc.example can save result 4 + 3 object named x.Now, whenever type x, value 4 + 3 appear:Data types: integers, doubles/numerics, logicals, characters.\nIntegers values like -1, 0, 2, 4092.\nDoubles numerics larger set values containing integers also fractions decimal values like -24.932 0.8.\nLogicals either TRUE FALSE. Characters text like “cabbage,” “Hamilton,” “Wire greatest TV show ever.”\nCharacters often denoted quotation marks around .\nIntegers values like -1, 0, 2, 4092.Doubles numerics larger set values containing integers also fractions decimal values like -24.932 0.8.Logicals either TRUE FALSE. Characters text like “cabbage,” “Hamilton,” “Wire greatest TV show ever.”Characters often denoted quotation marks around .Vectors: collection values. created using c() function, c stands combine.example, c(2, 4, 6, 8) creates four element vector numeric values.Factors: used represent “categorical data.” go detail variable types Chapter 2.Data frames: rectangular arrangements data. Think spreadsheet. Rows correspond units. Columns correspond variables. Modern data frames called tibbles.Boolean algebra: TRUE/FALSE statements mathematical operators < (less ), <= (less equal ), != (equal ). example, 4 + 2 >= 3 return TRUE, 3 + 5 <= 1 return FALSE.Boolean algebra: TRUE/FALSE statements mathematical operators < (less ), <= (less equal ), != (equal ). example, 4 + 2 >= 3 return TRUE, 3 + 5 <= 1 return FALSE.Inclusion: Tested %% operator. example, \"B\" %% c(\"\", \"B\") returns TRUE \"C\" %% c(\"\", \"B\") returns FALSE.Inclusion: Tested %% operator. example, \"B\" %% c(\"\", \"B\") returns TRUE \"C\" %% c(\"\", \"B\") returns FALSE.Equality: Tested using ==. example, 2 + 1 == 3 compares 2 + 1 3 legal R code, returning TRUE. hand, 2 + 1 = 3 return error can assign one number another.Equality: Tested using ==. example, 2 + 1 == 3 compares 2 + 1 3 legal R code, returning TRUE. hand, 2 + 1 = 3 return error can assign one number another.Logical operators: & representing “” well | representing “.” example, (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since least one two parts TRUE. hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since least one two parts TRUE.Logical operators: & representing “” well | representing “.” example, (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since least one two parts TRUE. hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since least one two parts TRUE.Functions: perform tasks, also called commands. take inputs called arguments return outputs. can either manually specify function’s arguments use function’s default values.Functions: perform tasks, also called commands. take inputs called arguments return outputs. can either manually specify function’s arguments use function’s default values.example, sqrt(64) function sqrt() 64 argument.Help files: provide documentation functions datasets. can bring help files adding ? name object run console.Help files: provide documentation functions datasets. can bring help files adding ? name object run console.Code comments: text placed # symbol. Nothing run # symbol, useful include comments code, always .Code comments: text placed # symbol. Nothing run # symbol, useful include comments code, always .Errors, warnings, messages: generally reported red font. error, code run. Read (/google) message try fix . Warnings don’t prevent code completing. example, create scatterplot based data two missing values, see warning:Errors, warnings, messages: generally reported red font. error, code run. Read (/google) message try fix . Warnings don’t prevent code completing. example, create scatterplot based data two missing values, see warning:Messages similar. cases, fix underlying issue warning/message goes away.","code":"\nx <- 4 + 3\nx## [1] 7\nc(1, 2, 3, 4)## [1] 1 2 3 4\nsqrt(64)## [1] 8Warning message:\nRemoved 2 rows containing missing values (geom_point).  "},{"path":"visualization.html","id":"examining-trains","chapter":"1 Visualization","heading":"1.1.3 Examining trains","text":"data comes us “spreadsheet”-type format. datasets called data frames tibbles R. Let’s explore trains tibble primer.data package. data comes Enos (2014), investigated attitudes toward immigration among Boston commuters.Let’s unpack output:tibble specific kind data frame. particular data frame 115 rows corresponding different units, meaning people case.tibble specific kind data frame. particular data frame 115 rows corresponding different units, meaning people case.tibble also 14 columns corresponding variables describe unit observation.tibble also 14 columns corresponding variables describe unit observation.see, default, top 10 rows columns. can see (fewer) rows columns using print() command:see, default, top 10 rows columns. can see (fewer) rows columns using print() command:n argument print() tells R number rows want see. width refers number characters print across screen. Want see every row every column? Try:Inf R object means “infinity.”","code":"\nlibrary(primer.data)\ntrains## # A tibble: 115 x 14\n##    treatment att_start att_end gender race  liberal party     age income line   \n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>   <int>  <dbl> <chr>  \n##  1 Treated          11      11 Female White FALSE   Democr…    31 135000 Framin…\n##  2 Treated           9      10 Female White FALSE   Republ…    34 105000 Framin…\n##  3 Treated           3       5 Male   White TRUE    Democr…    63 135000 Framin…\n##  4 Treated          11      11 Male   White FALSE   Democr…    45 300000 Framin…\n##  5 Control           8       5 Male   White TRUE    Democr…    55 135000 Framin…\n##  6 Treated          13      13 Female White FALSE   Democr…    37  87500 Framin…\n##  7 Control          13      13 Female White FALSE   Republ…    53  87500 Framin…\n##  8 Treated          10      11 Male   White FALSE   Democr…    36 135000 Framin…\n##  9 Control          12      12 Female White FALSE   Democr…    54 105000 Framin…\n## 10 Treated           9      10 Male   White FALSE   Republ…    42 135000 Framin…\n## # … with 105 more rows, and 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\nprint(trains, n = 15, width = 100)## # A tibble: 115 x 14\n##    treatment att_start att_end gender race  liberal party        age income\n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>      <int>  <dbl>\n##  1 Treated          11      11 Female White FALSE   Democrat      31 135000\n##  2 Treated           9      10 Female White FALSE   Republican    34 105000\n##  3 Treated           3       5 Male   White TRUE    Democrat      63 135000\n##  4 Treated          11      11 Male   White FALSE   Democrat      45 300000\n##  5 Control           8       5 Male   White TRUE    Democrat      55 135000\n##  6 Treated          13      13 Female White FALSE   Democrat      37  87500\n##  7 Control          13      13 Female White FALSE   Republican    53  87500\n##  8 Treated          10      11 Male   White FALSE   Democrat      36 135000\n##  9 Control          12      12 Female White FALSE   Democrat      54 105000\n## 10 Treated           9      10 Male   White FALSE   Republican    42 135000\n## 11 Control          10       9 Female White FALSE   Democrat      33 105000\n## 12 Treated          11       9 Male   White FALSE   Democrat      50 250000\n## 13 Treated          13      13 Male   White FALSE   Republican    24 105000\n## 14 Control           6       7 Male   White TRUE    Democrat      40  62500\n## 15 Control           8       8 Male   White TRUE    Democrat      53 300000\n##    line       station      hisp_perc ideology_start ideology_end\n##    <chr>      <chr>            <dbl>          <int>        <int>\n##  1 Framingham Grafton         0.0264              3            3\n##  2 Framingham Southborough    0.0154              4            4\n##  3 Framingham Grafton         0.0191              1            2\n##  4 Framingham Grafton         0.0191              4            4\n##  5 Framingham Grafton         0.0191              2            2\n##  6 Framingham Grafton         0.0231              5            5\n##  7 Framingham Grafton         0.0304              5            5\n##  8 Framingham Grafton         0.0247              4            4\n##  9 Framingham Grafton         0.0247              4            3\n## 10 Framingham Grafton         0.0259              4            4\n## 11 Framingham Grafton         0.0259              3            3\n## 12 Framingham Grafton         0.0259              5            4\n## 13 Framingham Grafton         0.0159              4            4\n## 14 Framingham Grafton         0.0159              1            1\n## 15 Framingham Southborough    0.0392              2            2\n## # … with 100 more rows\nprint(trains, n = Inf, width = Inf)"},{"path":"visualization.html","id":"exploring-tibbles","chapter":"1 Visualization","heading":"1.1.4 Exploring tibbles","text":"many ways get feel data contained tibble like trains.","code":""},{"path":"visualization.html","id":"view","chapter":"1 Visualization","heading":"1.1.4.1 view()","text":"Run view(trains) Console RStudio, either typing cutting--pasting Console pane. Explore tibble resulting pop viewer.view() allows us explore different variables listed columns. Observe many different types variables. variables quantitative. variables numerical nature. variables , including gender treatment, categorical.","code":""},{"path":"visualization.html","id":"glimpse","chapter":"1 Visualization","heading":"1.1.4.2 glimpse()","text":"can also explore tibble using glimpse().see first values variable row variable name. addition, data type variable given immediately variable’s name, inside < >.dbl refers “double,” computer terminology quantitative/numerical variables. int “integer.” fct refers “factor,” variable “nominal,” meaning member smallish number categories. chr character data.","code":"\nglimpse(trains)## Rows: 115\n## Columns: 14\n## $ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n## $ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n## $ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n## $ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n## $ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n## $ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n## $ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n## $ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n## $ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n## $ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n## $ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n## $ hisp_perc      <dbl> 0.026, 0.015, 0.019, 0.019, 0.019, 0.023, 0.030, 0.025,…\n## $ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n## $ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…"},{"path":"visualization.html","id":"operator","chapter":"1 Visualization","heading":"1.1.4.3 $ operator","text":"$ operator allows us extract single variable tibble return vector.","code":"\ntrains$age##   [1] 31 34 63 45 55 37 53 36 54 42 33 50 24 40 53 50 33 33 32 57 41 36 43 25 41\n##  [26] 33 44 46 41 28 36 37 38 48 20 52 38 45 55 38 45 44 36 29 42 43 54 39 31 50\n##  [51] 60 67 54 44 50 20 57 25 60 44 35 54 52 47 60 47 22 56 50 21 29 45 46 42 23\n##  [76] 29 60 41 30 61 21 46 53 45 46 63 21 31 35 22 68 27 22 30 59 56 32 35 23 60\n## [101] 50 31 43 30 54 52 52 50 37 27 55 42 68 52 50"},{"path":"visualization.html","id":"basic-plots","chapter":"1 Visualization","heading":"1.2 Basic Plots","text":"three essential components plot:data: dataset containing variables interest.geom: geometric object display, e.g., scatterplot, line, bar.aes: aesthetic attributes geometric object. important names variables x y axes. Additional attributes include color size. Aesthetic attributes mapped variables dataset.Consider basic scatterplot using data Enos (2014) 115 Boston commuters.Notice data aes specified call ggplot(), followed choice geom.Plots composed layers, combined using + sign. essential layer specifies type geometric object want plot involve: points, lines, bars, others. graph , geom used geom_point().+ sign comes end code line beginning. adding layers plot, start new line + code layer new line.","code":"\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()"},{"path":"visualization.html","id":"geom_point","chapter":"1 Visualization","heading":"1.2.1 geom_point()","text":"Scatterplots, also called bivariate plots, allow visualize relationship two numerical variables.Recall scatterplot .Let’s break code, piece--piece.data argument set trains via data = trains.data argument set trains via data = trains.aesthetic mapping set via mapping = aes(x = age, y = income). , map age x axis income y axis.aesthetic mapping set via mapping = aes(x = age, y = income). , map age x axis income y axis.geometric object specified using geom_point(), telling R want scatterplot. added layer using + sign.geometric object specified using geom_point(), telling R want scatterplot. added layer using + sign.specify geometric object, blank plot:addition mapping variables x y axes, can also map variables color.use function labs() add plot title, axis labels, subtitles, captions graph. default, R simply uses names variables axes legends. Add better titles labels.Note like geoms, add layer using + creating labs()plot. general, every plot give title axes labels. also add subtitle, purpose give short “main point” graphic. want viewer notice? also provide source data, usually via caption argument.Let’s now take tour useful geoms.","code":"\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income))\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income,\n                     color = party)) + \n  geom_point()\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point() +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")"},{"path":"visualization.html","id":"geom_jitter","chapter":"1 Visualization","heading":"1.2.2 geom_jitter()","text":"Consider different scatter plot using trains data.problem display “overplotting.” attitudes measured integers, know given point represents just one person dozen. two methods can use address overplotting: transparency jitter.Method 1: Changing transparencyWe can change transparency/opacity points using alpha argument within geom_point(). alpha argument can set value 0 1, 0 sets points 100% transparent 1 sets points 100% opaque. default, alpha set 1.Use new alpha value scatterplot.Note aes() surrounding alpha = 0.2. mapping variable aesthetic attribute, changing default setting alpha.Method 2: Jittering pointsWe can also decide jitter points plot. replacing geom_point() geom_jitter(). Keep mind jittering strictly visualization tool; even creating jittered scatterplot, original values saved data frame remain unchanged.order specify much jitter add, use width height arguments geom_jitter(). corresponds hard ’d like shake plot horizontal x-axis units vertical y-axis units, respectively. important add just enough jitter break overlap points, extent alter original pattern points.deciding whether jitter scatterplot use alpha argument geom_point(), know single right answer. suggest play around methods see one better emphasizes point trying make.","code":"\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point(alpha = 0.2) +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_jitter() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")"},{"path":"visualization.html","id":"geom_line","chapter":"1 Visualization","heading":"1.2.3 geom_line()","text":"Linegraphs show relationship two numerical variables variable x-axis, also called explanatory, predictive, independent variable, sequential nature. words, inherent ordering variable.common examples linegraphs notion time x-axis: hours, days, weeks, years, etc. Since time sequential, connect consecutive observations variable y-axis line. Linegraphs notion time x-axis also called time series plots.Let’s plot median duration unemployment United States last 50 years.Almost every aspect code used create plot identical scatter plots, except geom used.","code":"\nggplot(data = economics,\n       mapping = aes(x = date, y = uempmed)) +\n  geom_line() +\n  labs(title = \"Unemployment Duration in the United States: 1965 -- 2015\",\n       subtitle = \"Dramatic increase in duration after the Great Recesssion\",\n       x = \"Date\",\n       y = \"Median Duration in Weeks\",\n       caption = \"Source: FRED Economic Data\")"},{"path":"visualization.html","id":"geom_histogram","chapter":"1 Visualization","heading":"1.2.4 geom_histogram()","text":"histogram plot visualizes distribution numerical value.first cut x-axis series bins, bin represents range values.bin, count number observations fall range corresponding bin.draw bar whose height indicates corresponding count.Let’s consider income variable trains tibble. Pay attention changed two arguments ggplot(). removed data = mapping =. code still works R functions allow passing arguments position. first argument ggplot() data. don’t need tell R trains value data. R assumes passed first argument. Similarly, second argument ggplot() mapping, R assumes aes(x = income) value want mapping second item passed .Note message printed :stat_bin() using bins = 30. Pick better value binwidth.get message ran code . Try !message telling us histogram constructed using bins = 30 30 equally spaced bins. default value. Unless override default number bins number specify, R choose 30 default. important aspect making histogram, R insists informing message. make message go away specifying bin number , always .Let’s specify bins also add labels.Unlike scatterplots linegraphs, now one variable mapped aes(). , variable income. y-aesthetic histogram, count observations bin, gets computed automatically. Furthermore, geometric object layer now geom_histogram().can use fill argument change color actual bins. Let’s set fill “steelblue.”can also adjust number bins histogram one two ways:adjusting number bins via bins argument geom_histogram().adjusting number bins via bins argument geom_histogram().adjusting width bins via binwidth argument geom_histogram().adjusting width bins via binwidth argument geom_histogram().data, however, many unique values income, neither approach much effect. Replace income age want experiment options.","code":"\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")"},{"path":"visualization.html","id":"geom_bar","chapter":"1 Visualization","heading":"1.2.5 geom_bar()","text":"geom_bar() visualizes distribution categorical variable. simpler task creating histogram, simply counting different categories within categorical variable, also known levels categorical variable. Often best way visualize different counts, also known frequencies, barplot.geom_col() similar geom_bar(), except geom_col() requires calculate number observations category ahead time. geom_bar() calculation .","code":"\nggplot(data = trains, \n       mapping = aes(x = race)) +\n  geom_bar()"},{"path":"visualization.html","id":"no-pie-charts","chapter":"1 Visualization","heading":"1.2.5.1 No pie charts!","text":"One common plots used visualize distribution categorical data pie chart. may seem harmless enough, pie charts actually present problem humans unable judge angles well. Robbins (2013) argues overestimate angles greater 90 degrees underestimate angles less 90 degrees. words, difficult us determine relative size one piece pie compared another. use pie charts.","code":""},{"path":"visualization.html","id":"two-categorical-variables","chapter":"1 Visualization","heading":"1.2.5.2 Two categorical variables","text":"Another use barplots visualize joint distribution two categorical variables. (See Chapter 5 definition joint distribution.) Let’s look race, well treatment, trains data using fill argument inside aes() aesthetic mapping. Recall fill aesthetic corresponds color used fill bars.example stacked barplot. simple make, certain aspects ideal. example, difficult compare heights different colors bars, corresponding comparing number people different races within region.alternative stacked barplots side--side barplots, also known dodged barplots. code create side--side barplot includes position = \"dodge\" argument added inside geom_bar(). words, overriding default barplot type, stacked barplot, specifying side--side barplot instead.Whites -represented Control group even though treatment assigned random.","code":"\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar()\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar(position = \"dodge\")"},{"path":"visualization.html","id":"geom_smooth","chapter":"1 Visualization","heading":"1.2.6 geom_smooth()","text":"can add trend lines plots create using geom_smooth() function.Recall following scatterplot previous work.can add trend line graph adding layer geom_smooth(). Including trend lines allow us visualize relationship att_start att_end.Note message. R telling us need specify method formula argument, just way told us provide bins argument used geom_histogram() .Let’s add argument method = \"lm\", “lm” stands linear model. causes fitted line straight rather curved. Let’s also add argument formula = y ~ x. makes messages go away. , R giving us error . simply telling us options using since specify options .Always include enough detail code make messages disappear.Notice gray section surrounding line plotted. area called confidence interval, set 95% default. learn confidence intervals Chapter 5. can make shaded disappear adding se = FALSE another argument geom_smooth().","code":"\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth()## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x)"},{"path":"visualization.html","id":"geom_density","chapter":"1 Visualization","heading":"1.2.7 geom_density()","text":"Recall plot geom_histogram() section.Change geom_histogram() geom_density() make density plot, smoothed version histogram.values y-axis scaled total area curve equals one.","code":"\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\nggplot(trains, \n       aes(x = income)) +\n  geom_density() +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = NULL,\n       caption = \"Data source: Enos (2014)\")"},{"path":"visualization.html","id":"the-tidyverse","chapter":"1 Visualization","heading":"1.3 The Tidyverse","text":"Going forward, ggplot() code omit data = mapping = explicit naming arguments relying default ordering. time, include argument names , rule, . create many plots Primer omissions unlikely cause problems.","code":""},{"path":"visualization.html","id":"data-wrangling","chapter":"1 Visualization","heading":"1.3.1 Data wrangling","text":"can’t use beautiful plots learned previous chapter “wrangled” data convenient shape. Key wrangling functions include:filter(): pick rows want keep tibble.filter(): pick rows want keep tibble.select(): pick columns want keep tibble.select(): pick columns want keep tibble.arrange(): sort rows tibble, either ascending descending order.arrange(): sort rows tibble, either ascending descending order.mutate(): create new columns.mutate(): create new columns.group_by(): assign row tibble “group.” allows statistics calculated group separately. usually use group_by() summarize().group_by(): assign row tibble “group.” allows statistics calculated group separately. usually use group_by() summarize().summarize(): create new tibble comprised summary statistics one () rows grouped variable, tibble whole ungrouped.summarize(): create new tibble comprised summary statistics one () rows grouped variable, tibble whole ungrouped.","code":""},{"path":"visualization.html","id":"the-pipe-operator","chapter":"1 Visualization","heading":"1.3.2 The pipe operator: %>%","text":"pipe operator (%>%) allows us combine multiple operations R single sequential chain actions. Much like + sign come end line constructing plots — building plot layer--layer — pipe operator %>% come end line building data wrangling pipeline step--step. include pipe operator, R assumes next line code unrelated layers built get error.","code":""},{"path":"visualization.html","id":"filter-rows","chapter":"1 Visualization","heading":"1.3.3 filter() rows","text":"\nFIGURE 1.1: filter() reduces rows tibble.\nfilter() function works much like “Filter” option Microsoft Excel. allows specify criteria values variable dataset selects rows match criteria.result using filter() tibble just rows want. alter data, can good idea save result new data frame using <- assignment operator.Let’s break code. assigned new data object named trains_men via trains_men <-. assigned modified data frame trains_men, separate entity initial trains data frame. , however, written code trains <- trains overwritten already-existing tibble.start trains tibble filter() observations gender equals “Male” included. test equality using double equal sign == single equal sign =. words, filter(gender = \"Male\") produce error. convention across many programming languages.can use operators beyond just == operator.> “greater ”< “less ”>= “greater equal ”<= “less equal ”!= “equal .” ! indicates “.”Furthermore, can combine multiple criteria using operators make comparisons:| “”& “”example, let’s filter() trains tibble include women Republicans younger 40.Instead creating single criterion many parts, like &, can just separate parts comma. resulting tibble .","code":"\ntrains %>% \n  filter(gender == \"Male\")## # A tibble: 64 x 14\n##    treatment att_start att_end gender race  liberal party     age income line   \n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>   <int>  <dbl> <chr>  \n##  1 Treated           3       5 Male   White TRUE    Democr…    63 135000 Framin…\n##  2 Treated          11      11 Male   White FALSE   Democr…    45 300000 Framin…\n##  3 Control           8       5 Male   White TRUE    Democr…    55 135000 Framin…\n##  4 Treated          10      11 Male   White FALSE   Democr…    36 135000 Framin…\n##  5 Treated           9      10 Male   White FALSE   Republ…    42 135000 Framin…\n##  6 Treated          11       9 Male   White FALSE   Democr…    50 250000 Framin…\n##  7 Treated          13      13 Male   White FALSE   Republ…    24 105000 Framin…\n##  8 Control           6       7 Male   White TRUE    Democr…    40  62500 Framin…\n##  9 Control           8       8 Male   White TRUE    Democr…    53 300000 Framin…\n## 10 Treated          13      13 Male   Asian FALSE   Republ…    33 250000 Framin…\n## # … with 54 more rows, and 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains_men <- trains %>% \n  filter(gender == \"Male\")\ntrains %>% \n  filter(gender == \"Female\" & \n           party == \"Republican\" &\n           age < 40)## # A tibble: 3 x 14\n##   treatment att_start att_end gender race  liberal party      age income line   \n##   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>    <int>  <dbl> <chr>  \n## 1 Treated           9      10 Female White FALSE   Republi…    34 105000 Framin…\n## 2 Control          11      10 Female White FALSE   Republi…    21 135000 Frankl…\n## 3 Control          15      12 Female White FALSE   Republi…    21 250000 Frankl…\n## # … with 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains %>% \n  filter(gender == \"Female\",\n         party == \"Republican\",\n         age < 40)## # A tibble: 3 x 14\n##   treatment att_start att_end gender race  liberal party      age income line   \n##   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>    <int>  <dbl> <chr>  \n## 1 Treated           9      10 Female White FALSE   Republi…    34 105000 Framin…\n## 2 Control          11      10 Female White FALSE   Republi…    21 135000 Frankl…\n## 3 Control          15      12 Female White FALSE   Republi…    21 250000 Frankl…\n## # … with 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>"},{"path":"visualization.html","id":"select-variables","chapter":"1 Visualization","heading":"1.3.4 select variables","text":"\nFIGURE 1.2: select() reduces number columns tibble.\nUsing filter() function able pick specific rows (observations) tibble. select() function allows us pick specific columns (variables) instead.Use glimpse() see names variables trains:However, need two variables, say gender treatment. can select() just two:can drop, “de-select,” certain variables using minus (-) sign:can specify range columns using : operator.select() columns two specified variables.\nselect() function can also used rearrange columns used everything() helper function. can put treatment gender variables first :helper functions starts_with(), ends_with(), contains() can used select variables/columns match conditions. Examples:","code":"\nglimpse(trains)## Rows: 115\n## Columns: 14\n## $ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n## $ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n## $ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n## $ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n## $ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n## $ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n## $ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n## $ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n## $ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n## $ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n## $ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n## $ hisp_perc      <dbl> 0.026, 0.015, 0.019, 0.019, 0.019, 0.023, 0.030, 0.025,…\n## $ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n## $ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…\ntrains %>% \n  select(gender, treatment)## # A tibble: 115 x 2\n##    gender treatment\n##    <chr>  <fct>    \n##  1 Female Treated  \n##  2 Female Treated  \n##  3 Male   Treated  \n##  4 Male   Treated  \n##  5 Male   Control  \n##  6 Female Treated  \n##  7 Female Control  \n##  8 Male   Treated  \n##  9 Female Control  \n## 10 Male   Treated  \n## # … with 105 more rows\ntrains %>% \n  select(-gender, -liberal, -party, -age)## # A tibble: 115 x 10\n##    treatment att_start att_end race  income line       station      hisp_perc\n##    <fct>         <dbl>   <dbl> <chr>  <dbl> <chr>      <chr>            <dbl>\n##  1 Treated          11      11 White 135000 Framingham Grafton         0.0264\n##  2 Treated           9      10 White 105000 Framingham Southborough    0.0154\n##  3 Treated           3       5 White 135000 Framingham Grafton         0.0191\n##  4 Treated          11      11 White 300000 Framingham Grafton         0.0191\n##  5 Control           8       5 White 135000 Framingham Grafton         0.0191\n##  6 Treated          13      13 White  87500 Framingham Grafton         0.0231\n##  7 Control          13      13 White  87500 Framingham Grafton         0.0304\n##  8 Treated          10      11 White 135000 Framingham Grafton         0.0247\n##  9 Control          12      12 White 105000 Framingham Grafton         0.0247\n## 10 Treated           9      10 White 135000 Framingham Grafton         0.0259\n## # … with 105 more rows, and 2 more variables: ideology_start <int>,\n## #   ideology_end <int>\ntrains %>% \n  select(gender:age)## # A tibble: 115 x 5\n##    gender race  liberal party        age\n##    <chr>  <chr> <lgl>   <chr>      <int>\n##  1 Female White FALSE   Democrat      31\n##  2 Female White FALSE   Republican    34\n##  3 Male   White TRUE    Democrat      63\n##  4 Male   White FALSE   Democrat      45\n##  5 Male   White TRUE    Democrat      55\n##  6 Female White FALSE   Democrat      37\n##  7 Female White FALSE   Republican    53\n##  8 Male   White FALSE   Democrat      36\n##  9 Female White FALSE   Democrat      54\n## 10 Male   White FALSE   Republican    42\n## # … with 105 more rows\ntrains %>% \n  select(treatment, gender, everything())## # A tibble: 115 x 14\n##    treatment gender att_start att_end race  liberal party     age income line   \n##    <fct>     <chr>      <dbl>   <dbl> <chr> <lgl>   <chr>   <int>  <dbl> <chr>  \n##  1 Treated   Female        11      11 White FALSE   Democr…    31 135000 Framin…\n##  2 Treated   Female         9      10 White FALSE   Republ…    34 105000 Framin…\n##  3 Treated   Male           3       5 White TRUE    Democr…    63 135000 Framin…\n##  4 Treated   Male          11      11 White FALSE   Democr…    45 300000 Framin…\n##  5 Control   Male           8       5 White TRUE    Democr…    55 135000 Framin…\n##  6 Treated   Female        13      13 White FALSE   Democr…    37  87500 Framin…\n##  7 Control   Female        13      13 White FALSE   Republ…    53  87500 Framin…\n##  8 Treated   Male          10      11 White FALSE   Democr…    36 135000 Framin…\n##  9 Control   Female        12      12 White FALSE   Democr…    54 105000 Framin…\n## 10 Treated   Male           9      10 White FALSE   Republ…    42 135000 Framin…\n## # … with 105 more rows, and 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains %>% \n  select(starts_with(\"a\"))## # A tibble: 115 x 3\n##    att_start att_end   age\n##        <dbl>   <dbl> <int>\n##  1        11      11    31\n##  2         9      10    34\n##  3         3       5    63\n##  4        11      11    45\n##  5         8       5    55\n##  6        13      13    37\n##  7        13      13    53\n##  8        10      11    36\n##  9        12      12    54\n## 10         9      10    42\n## # … with 105 more rows"},{"path":"visualization.html","id":"slice-and-pull-and","chapter":"1 Visualization","heading":"1.3.5 slice() and pull() and []","text":"slice() pull() additional functions can use pick specific rows columns within data frame.Using slice() gives us specific rows trains tibble:Unlike filter(), slice() relies numeric order data.pull() grabs variable vector, rather leaving within tibble, select() :","code":"\ntrains %>% \n  slice(2:5)## # A tibble: 4 x 14\n##   treatment att_start att_end gender race  liberal party      age income line   \n##   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>    <int>  <dbl> <chr>  \n## 1 Treated           9      10 Female White FALSE   Republi…    34 105000 Framin…\n## 2 Treated           3       5 Male   White TRUE    Democrat    63 135000 Framin…\n## 3 Treated          11      11 Male   White FALSE   Democrat    45 300000 Framin…\n## 4 Control           8       5 Male   White TRUE    Democrat    55 135000 Framin…\n## # … with 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains %>% \n  slice(2:5) %>% \n  pull(age)## [1] 34 63 45 55"},{"path":"visualization.html","id":"arrange","chapter":"1 Visualization","heading":"1.3.6 arrange()","text":"arrange() allows us sort/reorder tibble’s rows according values specific variable. Unlike filter() select(), arrange() remove rows columns tibble. Example:arrange() always returns rows sorted ascending order default. switch ordering descending order instead, use desc() function:first many “pipes” create Primer. First, trains tibble. Second, pipe select() function. Third, pipe results select() arrange() function. step pipe starts tibble , done, produces tibble. tibbles way !","code":"\ntrains %>% \n  select(treatment, gender, age) %>% \n  arrange(age)## # A tibble: 115 x 3\n##    treatment gender   age\n##    <fct>     <chr>  <int>\n##  1 Treated   Female    20\n##  2 Control   Male      20\n##  3 Control   Male      21\n##  4 Control   Female    21\n##  5 Control   Female    21\n##  6 Control   Male      22\n##  7 Control   Female    22\n##  8 Treated   Male      22\n##  9 Treated   Male      23\n## 10 Control   Male      23\n## # … with 105 more rows\ntrains %>% \n  select(treatment, gender, age) %>% \n  arrange(desc(age))## # A tibble: 115 x 3\n##    treatment gender   age\n##    <fct>     <chr>  <int>\n##  1 Control   Female    68\n##  2 Control   Male      68\n##  3 Control   Male      67\n##  4 Treated   Male      63\n##  5 Control   Male      63\n##  6 Control   Male      61\n##  7 Control   Female    60\n##  8 Treated   Male      60\n##  9 Control   Male      60\n## 10 Control   Female    60\n## # … with 105 more rows"},{"path":"visualization.html","id":"mutate","chapter":"1 Visualization","heading":"1.3.7 mutate()","text":"\nFIGURE 1.3: `mutate() adds column tibble.\nmutate() takes existing columns creates new column. Recall income variable trains tibble dollars. Let’s use mutate() create new variable income thousands dollars. (use select() start pipe easier see new old variables time.)Notice newly created column right-hand side tibble named income_in_thousands.creating new variables can also overwrite original tibble:Whenever create new tibble, new variable within tibble, face dilemma: overwrite existing tibble/variable create new one? right answer.example, instead overwriting trains code , created new tibble trains_new. Similarly, instead creating new variable, income_in_thousands, overwritten current value income. Use best judgment careful.","code":"\ntrains %>% \n  select(gender, income) %>% \n  mutate(income_in_thousands = income / 1000)## # A tibble: 115 x 3\n##    gender income income_in_thousands\n##    <chr>   <dbl>               <dbl>\n##  1 Female 135000               135  \n##  2 Female 105000               105  \n##  3 Male   135000               135  \n##  4 Male   300000               300  \n##  5 Male   135000               135  \n##  6 Female  87500                87.5\n##  7 Female  87500                87.5\n##  8 Male   135000               135  \n##  9 Female 105000               105  \n## 10 Male   135000               135  \n## # … with 105 more rows\ntrains <- trains %>% \n  mutate(income_in_thousands = (income) / 1000)"},{"path":"visualization.html","id":"if_else","chapter":"1 Visualization","heading":"1.3.7.1 if_else()","text":"if_else() often used within calls mutate(). three arguments. first argument test logical vector. result contain value second argument, yes, test TRUE, value third argument, , FALSE.Imagine want create new variable old, TRUE age > 50 FALSE otherwise.Another function similar if_else(), dplyr::case_when(). case_when() particularly useful inside mutate want create new variable relies complex combination existing variables. Note different version ifelse() dplyr: if_else(). works exactly standard version somewhat robust.","code":"\ntrains %>% \n  select(age) %>% \n  mutate(old = ifelse(age > 50, TRUE, FALSE))## # A tibble: 115 x 2\n##      age old  \n##    <int> <lgl>\n##  1    31 FALSE\n##  2    34 FALSE\n##  3    63 TRUE \n##  4    45 FALSE\n##  5    55 TRUE \n##  6    37 FALSE\n##  7    53 TRUE \n##  8    36 FALSE\n##  9    54 TRUE \n## 10    42 FALSE\n## # … with 105 more rows"},{"path":"visualization.html","id":"summarize","chapter":"1 Visualization","heading":"1.3.8 summarize()","text":"musical interlude inspired Tidyverse:often need calculate summary statistics, things like mean (also called average) median (middle value). examples summary statistics include sum, minimum, maximum, standard deviation.function summarize() allows us calculate statistics individual columns tibble. Example:mean() sd() summary functions go inside summarize() function. summarize() function takes tibble returns tibbles one row corresponding summary statistics. Remember: Tibbles go tibble come .","code":"\ntrains %>% \n  summarize(mn_age = mean(age), \n            sd_age = sd(age))## # A tibble: 1 x 2\n##   mn_age sd_age\n##    <dbl>  <dbl>\n## 1   42.4   12.2"},{"path":"visualization.html","id":"statistics-for-distributions","chapter":"1 Visualization","heading":"1.3.9 Statistics for distributions","text":"variable tibble column, vector values. sometimes refer vector “distribution.” somewhat sloppy distribution can many things, commonly mathematical formula. , strictly speaking, “frequency distribution” “empirical distribution” list values, usage unreasonable.two distinct concepts: distribution set values drawn distribution. , everyday use, use “distribution” . given distribution (meaning vector numbers), often use geom_histogram() geom_density() graph . , sometimes, don’t want look whole thing. just want summary measures report key aspects distribution. two important attributes distribution center variation around center.use summarize() calculate statistics variable, column, vector values distribution. Note language sloppiness. purposes book, “variable,” “column,” “vector,” “distribution” mean thing. Popular statistical functions include: mean(), median(), min(), max(), n() sum(). Functions may new include three measures “spread” distribution: sd() (standard deviation), mad() (scaled median absolute deviation) quantile(), used calculate interval includes specified proportion values.mean()mean, average, commonly reported measure center distribution. mean sum data elements divided number elements. \\(N\\) data points, mean given :\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_N}{N}\\]median()median another commonly reported measure center distribution, calculated first sorting vector values smallest largest. middle element sorted list median. middle falls two values, median mean two middle values. median mean two common measures center distribution. median stable, less affected outliers. widely accepted symbol median, although \\(\\tilde{x}\\) uncommon.sd()standard deviation (sd) distribution measure variation around mean.\\[\\text{sd} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}}\\]mad()scaled median absolute deviation (mad) measure variation around median. popular standard deviation. formula calculating mad bit mysterious.\\[\\text{mad} = 1.4826 \\times \\text{median}(abs(x - \\tilde{x}))\\]basic idea sd mad need measure variation around center distribution. sd uses mean, \\(\\bar{x}\\), estimate center mad uses median, \\(\\tilde{x}\\). mad uses absolute difference, opposed squared difference, robust outliers. 1.4826 multiplier causes mad sd identical (important) case standard normal distributions, topic cover Chapter 2.quantile()quantile distribution value distribution occupies specific percentile location sorted list values.5th percentile distribution point 5% data falls. 95th percentile , similarly, point 95% data falls. 50th percentile, median, splits data two separate, equal, parts. minimum 0th percentile. maximum 100th percentile.Let’s take look poverty variable kenya tibble primer.data package. poverty percentage residents community incomes poverty line. Let’s first confirm quantile() works comparing output simpler functions.probs argument allows us specify percentile(s) want. Two important percentiles 2.5th 97.5th define 95% interval, central range includes 95% values.interval two percentiles includes 95% values distribution. Depending context, interval sometimes called “confidence interval” “uncertainty interval” “compatibility interval.” Different percentile ranges create intervals different widths.interesting fact , time, 95% confidence interval , roughly, center distribution \\(\\pm\\) two times standard deviation.distributions without major outliers, mean/median sd/mad similar .follows intervals defined similar.intervals similar similar 95% interval calculated . match roughly true, good enough applied work.NA value variable, statistical function like mean() return NA. can fix using na.rm = TRUE statistical function.","code":"\nc(min(kenya$poverty), median(kenya$poverty), max(kenya$poverty))## [1] 0.18 0.43 0.90\nquantile(kenya$poverty, probs = c(0, 0.5, 1))##   0%  50% 100% \n## 0.18 0.43 0.90\nquantile(kenya$poverty, probs = c(0.025, 0.975))##  2.5% 97.5% \n##  0.22  0.66\nkenya %>% \n  summarise(mean = mean(poverty),\n            median = median(poverty),\n            sd = sd(poverty),\n            mad = mad(poverty))## # A tibble: 1 x 4\n##    mean median    sd    mad\n##   <dbl>  <dbl> <dbl>  <dbl>\n## 1 0.426  0.432 0.109 0.0977\nkenya %>% \n  summarise(interval_1_bottom = mean(poverty) - 2 * sd(poverty),\n            interval_1_top = mean(poverty) + 2 * sd(poverty),\n            interval_2_bottom = median(poverty) - 2 * mad(poverty),\n            interval_2_top = median(poverty) + 2 * mad(poverty))## # A tibble: 1 x 4\n##   interval_1_bottom interval_1_top interval_2_bottom interval_2_top\n##               <dbl>          <dbl>             <dbl>          <dbl>\n## 1             0.208          0.645             0.236          0.627"},{"path":"visualization.html","id":"group_by","chapter":"1 Visualization","heading":"1.3.10 group_by()","text":"can use mean() summarize() calculate average age people trains, .want mean age gender? Consider:data , note “Groups” message top. R informing tibble grouped operation perform now done gender.Notice message R sends us. warning means tibble issues forth end pipe “ungrouped.” means group attribute applied group_by() removed. behavior (sensible) default.proper way handle situation, everywhere else use group_by() summarize(), specify .groups argument.code thing first version, issue message, since made affirmative decision drop grouping variables.group_by() function doesn’t change data frames . Rather changes meta-data, data data, specifically grouping structure. apply summarize() function tibble changes.tibble grouped, can remove grouping variable using ungroup().R code behaving weird way, especially “losing” rows, problem often solved using ungroup() pipeline.","code":"\ntrains %>% \n  summarize(mean = mean(age))## # A tibble: 1 x 1\n##    mean\n##   <dbl>\n## 1  42.4\ntrains %>% \n  group_by(gender)## # A tibble: 115 x 15\n## # Groups:   gender [2]\n##    treatment att_start att_end gender race  liberal party     age income line   \n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>   <int>  <dbl> <chr>  \n##  1 Treated          11      11 Female White FALSE   Democr…    31 135000 Framin…\n##  2 Treated           9      10 Female White FALSE   Republ…    34 105000 Framin…\n##  3 Treated           3       5 Male   White TRUE    Democr…    63 135000 Framin…\n##  4 Treated          11      11 Male   White FALSE   Democr…    45 300000 Framin…\n##  5 Control           8       5 Male   White TRUE    Democr…    55 135000 Framin…\n##  6 Treated          13      13 Female White FALSE   Democr…    37  87500 Framin…\n##  7 Control          13      13 Female White FALSE   Republ…    53  87500 Framin…\n##  8 Treated          10      11 Male   White FALSE   Democr…    36 135000 Framin…\n##  9 Control          12      12 Female White FALSE   Democr…    54 105000 Framin…\n## 10 Treated           9      10 Male   White FALSE   Republ…    42 135000 Framin…\n## # … with 105 more rows, and 5 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>, income_in_thousands <dbl>\ntrains %>% \n  group_by(gender) %>% \n  summarize(mean = mean(age))## # A tibble: 2 x 2\n##   gender  mean\n##   <chr>  <dbl>\n## 1 Female  41.0\n## 2 Male    43.5\ntrains %>% \n  group_by(gender) %>% \n  summarize(mean = mean(age),\n            .groups = \"drop\")## # A tibble: 2 x 2\n##   gender  mean\n##   <chr>  <dbl>\n## 1 Female  41.0\n## 2 Male    43.5"},{"path":"visualization.html","id":"advanced-plots","chapter":"1 Visualization","heading":"1.4 Advanced Plots","text":"Good visualizations teach. construct plot, decide message want convey. functions may helpful.","code":""},{"path":"visualization.html","id":"plot-objects","chapter":"1 Visualization","heading":"1.4.1 Plot objects","text":"Plots R objects, just like tibbles. can create , print save . now, just “spat” R code chunk. Nothing wrong ! Indeed, common approach plotting R. Sometimes, however, handy work plot object. Consider:code first example geom_point(). train_plot R object. code print anything . order make plot appear, need print explicitly:Recall typing name object thing using print(). Now object, can display whenever want., sometimes, want permanent copy plot, saved computer. purpose ggsave():ggsave() uses suffix provided filename determine type image save. use “enos_trains.jpg,” file saved JPEG format. used “enos_trains.png,” file saved PNG. can display saved file using knitr::include_graphics(). example:code displays image Rmd, assuming file “enos_trains.jpg” located current working directory. common scenario create image store directory named figures/ use figure one Rmd.","code":"\ntrain_plot <- ggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\ntrain_plot\nggsave(filename = \"enos_trains.jpg\", \n       plot = train_plot)```{r}\nknitr::include_graphics(\"enos_trains.jpg\")\n```"},{"path":"visualization.html","id":"faceting","chapter":"1 Visualization","heading":"1.4.2 Faceting","text":"Faceting splits visualization parts, one value another variable.. create multiple copies type plot matching x y axes, whose contents differ.proceed, let’s create subset tibble gapminder, gapminder, package use. (may need install gapminder package code work.)Let’s plot filtered data using geom_point()difficult compare continents despite colors. much easier “split” scatterplot 4 continents. words, create plots gdpPercap lifeExp continent separately. using function facet_wrap() argument ~ continent.much better! can specify number rows columns grid using nrow argument inside facet_wrap(). Let’s get continents row setting nrow 1. Let’s also add trend line geom_smooth() faceted plot.expected, can see positive correlation economic development life expectancy continents.","code":"\nlibrary(gapminder)\ngapminder_filt <- gapminder %>% \n      filter(year == 2007, continent != \"Oceania\")\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point()\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent)\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x, \n              se = FALSE)"},{"path":"visualization.html","id":"stats","chapter":"1 Visualization","heading":"1.4.3 Stats","text":"Consider following histogram.Recall y-aesthetic histogram — count observations bin — gets computed automatically. can use after_stat() argument within geom_histogram() generate percent values y-aesthetic.","code":"\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp))+ \n  geom_histogram(bins = 20, \n                 color = \"white\")\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp)) + \n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 20) +\n  labs(y = \"Percentage\")"},{"path":"visualization.html","id":"coordinate-systems","chapter":"1 Visualization","heading":"1.4.4 Coordinate Systems","text":"can switch axes plot coord_flip(). Consider faceted scatterplot previously created.Adding layer coord_flip() flips axes.lifeExp now x-axis gdpPercap y-axis. Compared previous plot now easier observe distribution life expectancy respective continents. example, can see many countries Africa 55 years, Americas Asia 75 years, Europe 80 years. However, think makes sense consider lifeExp dependent variable, won’t use coord_flip() subsequent plots.","code":"\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x, \n              se = FALSE)\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x, \n              se = FALSE) +\n  coord_flip()"},{"path":"visualization.html","id":"axis-limits-and-scales","chapter":"1 Visualization","heading":"1.4.5 Axis Limits and Scales","text":"can also manipulate limits axes using xlim() ylim() within call coord_cartesian(). example, assume interested countries GDP per capita 0 30,000. Recall , data first argument mapping second ggplot(), don’t actually name arguments. can just provide , long correct order.can see GDP per capita x-axis now shown 0 30,000.can also change scaling axes. example, might useful display axes logarithmic scale using scale_x_log10() scale_y_log10(). Also, note can (lazily!) provide explicit x y argument names aes() long provide values right order: x comes y.","code":"\nggplot(gapminder_filt, \n       aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  coord_cartesian(xlim = c(0, 30000))\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10()"},{"path":"visualization.html","id":"text","chapter":"1 Visualization","heading":"1.4.6 Text","text":"Recall use labs() add labels titles plots.\ncan also change labels inside plots using geom_text().Let’s breakdown code within geom_text(). included new aesthetic called label. defines character variable used basis labels. set label country point corresponds country represents. set text font setting size 2, set text color using color. Finally, included argument check_overlap = TRUE make sure names countries legible.","code":"\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\") +\n  geom_text(aes(label = country), \n            size = 2, \n            color = \"black\", \n            check_overlap = TRUE)"},{"path":"visualization.html","id":"themes","chapter":"1 Visualization","heading":"1.4.7 Themes","text":"Themes can used change overall appearance plot without much effort. add themes layers plots. can find overview different themes ggplot .Consider following faceted scatterplot.Note use breaks argument scale_x_log10(). specifies location labels x-axis. can also use labels argument want change appearence. tricks work entire family scale_* functions.Let’s now add theme faceted scatterplot. use theme theme_economist(), ggthemes package, make plot look like plots Economist.looks pretty good. However, notice legend top graph. crowds graph takes away important part: data. can use theme() customize non-data parts plots background, gridlines, legends. Let’s de-clutter graph removing legend. can using legend.position argument setting “none.”Great. Now graph easier visualize. addition, note theme() function also offers wide selection functions manually changing individual elements.","code":"\ngapminder %>%\n  filter(continent != \"Oceania\") %>%\n  filter(year == max(year)) %>% \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) \nlibrary(ggthemes)\n\ngapminder %>%\n  filter(continent != \"Oceania\") %>%\n  filter(year == max(year)) %>% \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) +\n  theme_economist()\ngapminder %>%\n  filter(continent != \"Oceania\") %>%\n  filter(year == max(year)) %>% \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000),\n                  labels = scales::dollar_format(accuracy = 1)) + \n    theme_economist() +\n    theme(legend.position = \"none\")"},{"path":"visualization.html","id":"summary-1","chapter":"1 Visualization","heading":"1.5 Summary","text":"Tibbles rectangular stores data. specific type data frame, use terms interchangeably.need practice every day.use pie charts.Shield eyes ugly messages warning.step pipe starts tibble , done, produces tibble. tibbles way !R code behaving weird way, especially “losing” rows, problem often solved using ungroup() pipeline.two important attributes distribution center variation around center.chapter, first looked basic coding terminology concepts deal programming R. learned three basic components make plot: data, mapping, one geoms. ggplot2 package offers wide range geoms can use create different types plots. Next, examined “super package” tidyverse, includes helpful tools visualization. also offers features importing manipulating data, main topic Chapter @(wrangling). Lastly, explored advanced plotting features axis scaling, faceting, themes.main statistical concept discussed “distribution,” collection related numbers. R, numbers usually stored variable inside tibble. “Frequency distribution” “empirical distribution” common phrases describe objects. learned calculate statistics — often referred “summary statistics” — distributions. important involve two concepts: center distribution variability distribution. center often summarized mean (mean()) median (median()). Variability measured standard deviation (sd()) scaled median absolute deviation (mad()) 95% confidence interval (quantile(probs = c(0.025, 0.975))).Recall plot began chapter :now know enough make plots like .important know seen small part R offers. example, can use gganimate package bring slightly modified version gapminder plot life:plotly package makes plot interactive. Click explore!beautiful plot just collection steps, simple enough . taught () steps. Time start walking .","code":"\nlibrary(gganimate)\n\ngap_anim <- gapminder %>%\n  filter(continent != \"Oceania\") %>%\n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(show.legend = FALSE, alpha = 0.7) +\n    facet_wrap(~continent, nrow = 1) +\n    scale_size(range = c(2, 12)) +\n    scale_x_log10() +\n    labs(subtitle = \"Life Expectancy and GDP per Capita (1952-2007)\",\n         x = \"GDP per Capita, USD\",\n         y = \"Life Expectancy, Years\") +\n    theme_linedraw() +\n    transition_time(year) +\n    labs(title = \"Year: {frame_time}\") +\n    shadow_wake(wake_length = 0.1, alpha = FALSE)\n\ngap_anim\nlibrary(plotly)\n\nggplotly(gap_p)"},{"path":"wrangling.html","id":"wrangling","chapter":"2 Wrangling","heading":"2 Wrangling","text":"Data science data cleaning.Start loading packages need chapter.tidyverse package used every chapter. Loading makes 8 packages “Tidyverse” available.primer.data data package created specifically primer.lubridate package working dates times.skimr contains functions useful providing summary statistics, especially skim().nycflights13 includes data associated flights New York City’s three major airports.gapminder annual data countries going back 50 years, used Chapter 1.fivethirtyeight cleans data FiveThirtyEight team.","code":"\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(nycflights13)\nlibrary(gapminder)\nlibrary(fivethirtyeight)"},{"path":"wrangling.html","id":"tibbles","chapter":"2 Wrangling","heading":"2.1 Tibbles","text":"Tibbles kind data frame, useful storing data number observations variable. can use tibble() function create tibbles. Tibbles composed columns, variable, rows, “unit” “observation.” Furthermore, column (.e., variable) can different type: character, integer, factor, double, date .","code":""},{"path":"wrangling.html","id":"tibble","chapter":"2 Wrangling","heading":"2.1.1 tibble()","text":"code, specify variable names (names columns) , , b, c. variable, give different value. variable name, data type specified data within column. tibble can consist one atomic vectors, important types double, character, logical, integer. tibble includes variable type. “L” “9L” tells R want d integer rather default, double. print tibble, variable type shown variable name.Variables begin number (like 54abc) include spaces (like var). insist using variable names , must include backticks around name reference .include backticks, R give us error.sometimes easier use function tribble() create tibbles.tildes — ~ var1 — specify row column names. formatting makes easier, relative specifying raw vectors, see values observation.","code":"\ntibble(a = 2.1, b = \"Hello\", c = TRUE, d = 9L)## # A tibble: 1 x 4\n##       a b     c         d\n##   <dbl> <chr> <lgl> <int>\n## 1   2.1 Hello TRUE      9\ntibble(`54abc` = 1, `my var` = 2, c = 3)## # A tibble: 1 x 3\n##   `54abc` `my var`     c\n##     <dbl>    <dbl> <dbl>\n## 1       1        2     3tibble(54abc = 1, my var = 2, c = 3)## Error: <text>:1:10: unexpected symbol\n## 1: tibble(54abc\n##              ^\ntribble(\n  ~ var1, ~ `var 2`, ~ myvar,\n  1,           3,      5,\n  4,           6,      8,\n)## # A tibble: 2 x 3\n##    var1 `var 2` myvar\n##   <dbl>   <dbl> <dbl>\n## 1     1       3     5\n## 2     4       6     8"},{"path":"wrangling.html","id":"lists","chapter":"2 Wrangling","heading":"2.2 Lists","text":"\nFIGURE 2.1: Subsetting list, visually.\nEarlier, briefly introduced lists. Lists type vector step complexity atomic vectors, lists can contain lists. makes suitable representing hierarchical tree-like structures. create list function list():useful tool working lists str() focuses displaying structure, contents.Unlike atomic vectors, list() can contain mix objects.Lists can even contain lists!","code":"\nx <- list(1, 2, 3)\nx## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 2\n## \n## [[3]]\n## [1] 3\nstr(x)## List of 3\n##  $ : num 1\n##  $ : num 2\n##  $ : num 3\nx_named <- list(a = 1, b = 2, c = 3)\nstr(x_named)## List of 3\n##  $ a: num 1\n##  $ b: num 2\n##  $ c: num 3\ny <- list(\"a\", 1L, 1.5, TRUE)\nstr(y)## List of 4\n##  $ : chr \"a\"\n##  $ : int 1\n##  $ : num 1.5\n##  $ : logi TRUE\nz <- list(list(1, 2), list(3, 4))\nstr(z)## List of 2\n##  $ :List of 2\n##   ..$ : num 1\n##   ..$ : num 2\n##  $ :List of 2\n##   ..$ : num 3\n##   ..$ : num 4"},{"path":"wrangling.html","id":"visualizing-lists","chapter":"2 Wrangling","heading":"2.2.1 Visualizing lists","text":"explain complicated list manipulation functions, ’s helpful visual representation lists. example, take three lists:’ll draw follows:three principles:Lists rounded corners. Atomic vectors square corners.Lists rounded corners. Atomic vectors square corners.Children drawn inside parent, slightly darker background make easier see hierarchy.Children drawn inside parent, slightly darker background make easier see hierarchy.orientation children (.e. rows columns) isn’t important, ’ll pick row column orientation either save space illustrate important property example.orientation children (.e. rows columns) isn’t important, ’ll pick row column orientation either save space illustrate important property example.","code":"\nx1 <- list(c(1, 2), c(3, 4))\nx2 <- list(list(1, 2), list(3, 4))\nx3 <- list(1, list(2, list(3)))"},{"path":"wrangling.html","id":"subsetting","chapter":"2 Wrangling","heading":"2.2.2 Subsetting","text":"three ways subset list, ’ll illustrate list named :[ ] extracts sub-list. result always list.Like vectors, can subset logical, integer, character vector.[[ ]] extracts single component list. removes level hierarchy list.$ shorthand extracting named elements list. works similarly [[ ]] except don’t need use quotes.distinction [ ] [[ ]] really important lists, [[ ]] drills list [ ] returns new, smaller list. Compare code output visual representation.","code":"\na <- list(a = 1:3, b = \"a string\", c = pi, d = list(-1, -5))\nstr(a[1:2])## List of 2\n##  $ a: int [1:3] 1 2 3\n##  $ b: chr \"a string\"\nstr(a[4])## List of 1\n##  $ d:List of 2\n##   ..$ : num -1\n##   ..$ : num -5\nstr(a[[1]])##  int [1:3] 1 2 3\nstr(a[[4]])## List of 2\n##  $ : num -1\n##  $ : num -5\na$a## [1] 1 2 3\na[[\"a\"]]## [1] 1 2 3"},{"path":"wrangling.html","id":"characters","chapter":"2 Wrangling","heading":"2.3 Characters","text":"\nFIGURE 2.2: Real data nasty.\nfar, tibbles clean wholesome, like gapminder trains. Real data nasty. bring data R outside world discover problems. now discuss common remedial tasks cleaning transforming character data, also known strings. string one characters enclosed inside pair matching ‘single’ “double quotes.”use fruit data, vector names different fruits, stringr package, automatically loaded issue library(tidyverse). Although can manipulate character vectors directly, much common, real world situations, work vectors tibble.","code":"\ntbl_fruit <- tibble(fruit = fruit)"},{"path":"wrangling.html","id":"character-vectors","chapter":"2 Wrangling","heading":"2.3.1 Character vectors","text":"Note slice_sample() selects random set rows tibble. use argument n, , get back many rows. Use argue prop return specific percentage rows tibble.str_detect() determines character vector matches pattern. returns logical vector length input. Recall logicals either TRUE FALSE.fruit names actually include letter “c?”str_length() counts characters strings. Note different length() character vector .str_sub() extracts parts string. function takes start end arguments vectorised.str_c() combines character vector length single string. similar normal c() function creating vector.str_replace() replaces pattern within string.","code":"\ntbl_fruit %>% \n  slice_sample(n = 8)## # A tibble: 8 x 1\n##   fruit       \n##   <chr>       \n## 1 pear        \n## 2 raspberry   \n## 3 mandarine   \n## 4 dragonfruit \n## 5 date        \n## 6 pamelo      \n## 7 passionfruit\n## 8 blackberry\ntbl_fruit %>% \n  mutate(fruit_in_name = str_detect(fruit, pattern = \"c\")) ## # A tibble: 80 x 2\n##    fruit        fruit_in_name\n##    <chr>        <lgl>        \n##  1 apple        FALSE        \n##  2 apricot      TRUE         \n##  3 avocado      TRUE         \n##  4 banana       FALSE        \n##  5 bell pepper  FALSE        \n##  6 bilberry     FALSE        \n##  7 blackberry   TRUE         \n##  8 blackcurrant TRUE         \n##  9 blood orange FALSE        \n## 10 blueberry    FALSE        \n## # … with 70 more rows\ntbl_fruit %>% \n  mutate(name_length = str_length(fruit)) ## # A tibble: 80 x 2\n##    fruit        name_length\n##    <chr>              <int>\n##  1 apple                  5\n##  2 apricot                7\n##  3 avocado                7\n##  4 banana                 6\n##  5 bell pepper           11\n##  6 bilberry               8\n##  7 blackberry            10\n##  8 blackcurrant          12\n##  9 blood orange          12\n## 10 blueberry              9\n## # … with 70 more rows\ntbl_fruit %>% \n  mutate(first_three_letters = str_sub(fruit, 1, 3)) ## # A tibble: 80 x 2\n##    fruit        first_three_letters\n##    <chr>        <chr>              \n##  1 apple        app                \n##  2 apricot      apr                \n##  3 avocado      avo                \n##  4 banana       ban                \n##  5 bell pepper  bel                \n##  6 bilberry     bil                \n##  7 blackberry   bla                \n##  8 blackcurrant bla                \n##  9 blood orange blo                \n## 10 blueberry    blu                \n## # … with 70 more rows\ntbl_fruit %>% \n  mutate(name_with_s = str_c(fruit, \"s\")) ## # A tibble: 80 x 2\n##    fruit        name_with_s  \n##    <chr>        <chr>        \n##  1 apple        apples       \n##  2 apricot      apricots     \n##  3 avocado      avocados     \n##  4 banana       bananas      \n##  5 bell pepper  bell peppers \n##  6 bilberry     bilberrys    \n##  7 blackberry   blackberrys  \n##  8 blackcurrant blackcurrants\n##  9 blood orange blood oranges\n## 10 blueberry    blueberrys   \n## # … with 70 more rows\ntbl_fruit %>% \n  mutate(capital_A = str_replace(fruit, \n                                 pattern = \"a\", \n                                 replacement = \"A\")) ## # A tibble: 80 x 2\n##    fruit        capital_A   \n##    <chr>        <chr>       \n##  1 apple        Apple       \n##  2 apricot      Apricot     \n##  3 avocado      Avocado     \n##  4 banana       bAnana      \n##  5 bell pepper  bell pepper \n##  6 bilberry     bilberry    \n##  7 blackberry   blAckberry  \n##  8 blackcurrant blAckcurrant\n##  9 blood orange blood orAnge\n## 10 blueberry    blueberry   \n## # … with 70 more rows"},{"path":"wrangling.html","id":"regular-expressions-with-stringr","chapter":"2 Wrangling","heading":"2.3.2 Regular expressions with stringr","text":"Sometimes, string tasks expressed terms fixed string, can described terms pattern. Regular expressions, also know “regexes,” standard way specify patterns. regexes, specific characters constructs take special meaning order match multiple strings.explore regular expressions, use str_detect() function, reports TRUE string matches pattern, filter() see matches. example, fruits include “w” name.code , first metacharacter period . , stands single character, except newline (, way, represented \\n). regex b.r match fruits “b,” followed single character, followed “r.” Regexes case sensitive.Anchors can included express expression must occur within string. ^ indicates beginning string $ indicates end.","code":"\ntbl_fruit %>%\n  filter(str_detect(fruit, pattern = \"w\"))## # A tibble: 4 x 1\n##   fruit     \n##   <chr>     \n## 1 honeydew  \n## 2 kiwi fruit\n## 3 strawberry\n## 4 watermelon\ntbl_fruit %>%\n  filter(str_detect(fruit, pattern = \"b.r\"))## # A tibble: 15 x 1\n##    fruit      \n##    <chr>      \n##  1 bilberry   \n##  2 blackberry \n##  3 blueberry  \n##  4 boysenberry\n##  5 cloudberry \n##  6 cranberry  \n##  7 cucumber   \n##  8 elderberry \n##  9 goji berry \n## 10 gooseberry \n## 11 huckleberry\n## 12 mulberry   \n## 13 raspberry  \n## 14 salal berry\n## 15 strawberry\ntbl_fruit %>%\n  filter(str_detect(fruit, pattern = \"^w\"))## # A tibble: 1 x 1\n##   fruit     \n##   <chr>     \n## 1 watermelon\ntbl_fruit %>%\n  filter(str_detect(fruit, pattern = \"o$\"))## # A tibble: 5 x 1\n##   fruit    \n##   <chr>    \n## 1 avocado  \n## 2 mango    \n## 3 pamelo   \n## 4 pomelo   \n## 5 tamarillo"},{"path":"wrangling.html","id":"factors","chapter":"2 Wrangling","heading":"2.4 Factors","text":"Factors categorical variables take specified set values. manipulate factors use forcats package, core package Tidyverse.easy make factors either factor(), .factor() parse_factor().three options best depends situation. factor() useful creating factor nothing. .factor() best simple transformations, especially character variables, example. parse_factor() modern powerful three.Let’s use gapminder$continent example. Note str() useful function getting detailed information object.get frequency table tibble, tibble, use count(). get similar result free-range factor, use fct_count().","code":"\ntibble(X = letters[1:3]) %>% \n  mutate(fac_1 = factor(X)) %>% \n  mutate(fac_2 = as.factor(X)) %>% \n  mutate(fac_3 = parse_factor(X))## # A tibble: 3 x 4\n##   X     fac_1 fac_2 fac_3\n##   <chr> <fct> <fct> <fct>\n## 1 a     a     a     a    \n## 2 b     b     b     b    \n## 3 c     c     c     c\nstr(gapminder$continent)##  Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\nlevels(gapminder$continent)## [1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\"\nnlevels(gapminder$continent)## [1] 5\nclass(gapminder$continent)## [1] \"factor\"\ngapminder %>% \n  count(continent)## # A tibble: 5 x 2\n##   continent     n\n##   <fct>     <int>\n## 1 Africa      624\n## 2 Americas    300\n## 3 Asia        396\n## 4 Europe      360\n## 5 Oceania      24\nfct_count(gapminder$continent)## # A tibble: 5 x 2\n##   f            n\n##   <fct>    <int>\n## 1 Africa     624\n## 2 Americas   300\n## 3 Asia       396\n## 4 Europe     360\n## 5 Oceania     24"},{"path":"wrangling.html","id":"dropping-unused-levels","chapter":"2 Wrangling","heading":"2.4.1 Dropping unused levels","text":"Removing rows corresponding specific factor level remove level . unused levels can come back haunt later, e.g., figure legends.Watch happens levels country filter gapminder handful countries.Even though h_gap data handful countries, still schlepping around levels original gapminder tibble.can get rid ? base function droplevels() operates factors data frame single factor. function fct_drop() operates single factor variable.","code":"\nnlevels(gapminder$country)## [1] 142\nh_gap <- gapminder %>%\n  filter(country %in% c(\"Egypt\", \"Haiti\", \n                        \"Romania\", \"Thailand\", \n                        \"Venezuela\"))\nnlevels(h_gap$country)## [1] 142\nh_gap_dropped <- h_gap %>% \n  droplevels()\nnlevels(h_gap_dropped$country)## [1] 5\n# Use fct_drop() on a free-range factor\n\nh_gap$country %>%\n  fct_drop() %>%\n  levels()## [1] \"Egypt\"     \"Haiti\"     \"Romania\"   \"Thailand\"  \"Venezuela\""},{"path":"wrangling.html","id":"change-the-order-of-the-levels","chapter":"2 Wrangling","heading":"2.4.2 Change the order of the levels","text":"default, factor levels ordered alphabetically.can also order factors :Frequency: Make common level first .Another variable: Order factor levels according summary statistic another variable.Let’s order continent frequency using fct_infreq().can also frequency print backwards using fct_rev().two bar charts frequency continent differ order continents. prefer? show code just second one.Let’s now order country another variable, forwards backwards. variable usually quantitative order factor according grouped summary. factor grouping variable default summarizing function median() can specify something else.reorder factor levels? often makes plots much better! plotting factor numeric variable, generally good idea order factors function numeric variable. Alphabetic ordering rarely best.Compare interpretability two plots life expectancy Aericas 2007. difference order country factor. better?","code":"\ngapminder$continent %>%\n  levels()## [1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\"\ngapminder$continent %>% \n  fct_infreq() %>%\n  levels()## [1] \"Africa\"   \"Asia\"     \"Europe\"   \"Americas\" \"Oceania\"\ngapminder$continent %>% \n  fct_infreq() %>%\n  fct_rev() %>% \n  levels()## [1] \"Oceania\"  \"Americas\" \"Europe\"   \"Asia\"     \"Africa\"\ngapminder %>% \n  mutate(continent = fct_infreq(continent)) %>% \n  mutate(continent = fct_rev(continent)) %>% \n  ggplot(aes(x = continent)) +\n    geom_bar() +\n    coord_flip()\n# Order countries by median life expectancy\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp) %>% \n  levels() %>% \n  head()## [1] \"Sierra Leone\"  \"Guinea-Bissau\" \"Afghanistan\"   \"Angola\"       \n## [5] \"Somalia\"       \"Guinea\"\n# Order according to minimum life exp instead of median\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp, min) %>% \n  levels() %>% \n  head()## [1] \"Rwanda\"       \"Afghanistan\"  \"Gambia\"       \"Angola\"       \"Sierra Leone\"\n## [6] \"Cambodia\"\n# Backwards!\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp, \n            .desc = TRUE) %>% \n  levels() %>% \n  head()## [1] \"Iceland\"     \"Japan\"       \"Sweden\"      \"Switzerland\" \"Netherlands\"\n## [6] \"Norway\"\ngapminder %>% \n  filter(year == 2007, \n         continent == \"Americas\") %>% \n  ggplot(aes(x = lifeExp, y = country)) + \n    geom_point()\ngapminder %>% \n  filter(year == 2007, \n         continent == \"Americas\") %>% \n  ggplot(aes(x = lifeExp, \n             y = fct_reorder(country, lifeExp))) + \n    geom_point()"},{"path":"wrangling.html","id":"recode-the-levels","chapter":"2 Wrangling","heading":"2.4.3 Recode the levels","text":"Use fct_recode() change names levels factor.","code":"\ni_gap <- gapminder %>% \n  filter(country %in% c(\"United States\", \"Sweden\", \n                        \"Australia\")) %>% \n  droplevels()\n\ni_gap$country %>% \n  levels()## [1] \"Australia\"     \"Sweden\"        \"United States\"\ni_gap$country %>%\n  fct_recode(\"USA\" = \"United States\", \"Oz\" = \"Australia\") %>% \n  levels()## [1] \"Oz\"     \"Sweden\" \"USA\""},{"path":"wrangling.html","id":"date-times","chapter":"2 Wrangling","heading":"2.5 Date-Times","text":"manipulate date-times using lubridate package lubridate makes easier work dates times R. lubridate part core tidyverse need ’re working dates/times.three types date/time data refer instant time:date. Tibbles print <date>.date. Tibbles print <date>.time within day. Tibbles print <time>.time within day. Tibbles print <time>.date-time date plus time. uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>.date-time date plus time. uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>.always use simplest possible data type works needs. means can use date instead date-time, . Date-times substantially complicated need handle time zones, ’ll come back end chapter.get current date date-time can use today() now():Otherwise, three ways ’re likely create date/time:string.individual date-time components.existing date/time object.work follows:","code":"\ntoday()## [1] \"2021-07-07\"\nnow()## [1] \"2021-07-07 07:46:05 EDT\""},{"path":"wrangling.html","id":"from-strings","chapter":"2 Wrangling","heading":"2.5.1 From strings","text":"Date/time data often comes strings. lubridate functions automatically work format specify order component. First, figure want order year, month, day appear dates, arrange “y,” “m,” “d” accordingly. gives name lubridate function parse date. example:functions also take unquoted numbers. concise way create single date/time object, might need filtering date/time data. ymd() short unambiguous:ymd() friends create dates. create date-time, add underscore one “h,” “m,” “s” name parsing function:can also force creation date-time date supplying timezone:","code":"\nymd(\"2017-01-31\")## [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")## [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")## [1] \"2017-01-31\"\nymd(20170131)## [1] \"2017-01-31\"\nymd_hms(\"2017-01-31 20:11:59\")## [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")## [1] \"2017-01-31 08:01:00 UTC\"\nymd(20170131, tz = \"UTC\")## [1] \"2017-01-31 UTC\""},{"path":"wrangling.html","id":"from-individual-components","chapter":"2 Wrangling","heading":"2.5.2 From individual components","text":"Instead single string, sometimes ’ll individual components date-time spread across multiple columns. flights data:create date/time sort input, use make_date() dates, make_datetime() date-times:","code":"\nflights %>% \n  select(year, month, day, hour, minute)## # A tibble: 336,776 x 5\n##     year month   day  hour minute\n##    <int> <int> <int> <dbl>  <dbl>\n##  1  2013     1     1     5     15\n##  2  2013     1     1     5     29\n##  3  2013     1     1     5     40\n##  4  2013     1     1     5     45\n##  5  2013     1     1     6      0\n##  6  2013     1     1     5     58\n##  7  2013     1     1     6      0\n##  8  2013     1     1     6      0\n##  9  2013     1     1     6      0\n## 10  2013     1     1     6      0\n## # … with 336,766 more rows\nflights %>% \n  select(year, month, day, hour, minute) %>% \n  mutate(departure = make_datetime(year, month, day, hour, minute))## # A tibble: 336,776 x 6\n##     year month   day  hour minute departure          \n##    <int> <int> <int> <dbl>  <dbl> <dttm>             \n##  1  2013     1     1     5     15 2013-01-01 05:15:00\n##  2  2013     1     1     5     29 2013-01-01 05:29:00\n##  3  2013     1     1     5     40 2013-01-01 05:40:00\n##  4  2013     1     1     5     45 2013-01-01 05:45:00\n##  5  2013     1     1     6      0 2013-01-01 06:00:00\n##  6  2013     1     1     5     58 2013-01-01 05:58:00\n##  7  2013     1     1     6      0 2013-01-01 06:00:00\n##  8  2013     1     1     6      0 2013-01-01 06:00:00\n##  9  2013     1     1     6      0 2013-01-01 06:00:00\n## 10  2013     1     1     6      0 2013-01-01 06:00:00\n## # … with 336,766 more rows"},{"path":"wrangling.html","id":"from-other-types","chapter":"2 Wrangling","heading":"2.5.3 From other types","text":"may want switch date-time date. ’s job as_datetime() as_date():Sometimes ’ll get date/times numeric offsets “Unix Epoch,” 1970-01-01. offset seconds, use as_datetime(); ’s days, use as_date().","code":"\nas_datetime(today())## [1] \"2021-07-07 UTC\"\nas_date(now())## [1] \"2021-07-07\"\nas_datetime(60 * 60 * 10)## [1] \"1970-01-01 10:00:00 UTC\"\nas_date(365 * 10 + 2)## [1] \"1980-01-01\""},{"path":"wrangling.html","id":"date-time-components","chapter":"2 Wrangling","heading":"2.5.4 Date-time components","text":"Now know get date-time data R’s date-time data structures, let’s explore can . section focus accessor functions let get set individual components. next section look arithmetic works date-times.can pull individual parts date accessor functions year(), month(), mday() (day month), yday() (day year), wday() (day week), hour(), minute(), second().month() wday() can set label = TRUE return abbreviated name month day week. Set label = TRUE abbr = FALSE return full name.","code":"\ndatetime <- ymd_hms(\"2016-07-08 12:34:56\")\nyear(datetime)## [1] 2016\nmonth(datetime)## [1] 7\nmday(datetime)## [1] 8\nyday(datetime)## [1] 190\nwday(datetime)## [1] 6\nmonth(datetime, label = TRUE)## [1] Jul\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nwday(datetime, label = TRUE, abbr = FALSE)## [1] Friday\n## 7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday"},{"path":"wrangling.html","id":"setting-components","chapter":"2 Wrangling","heading":"2.5.5 Setting components","text":"can create new date-time update().values big, roll-:","code":"\nupdate(datetime, year = 2020, month = 2, mday = 2, hour = 2)## [1] \"2020-02-02 02:34:56 UTC\"\nymd(\"2015-02-01\") %>% \n  update(mday = 30)## [1] \"2015-03-02\"\nymd(\"2015-02-01\") %>% \n  update(hour = 400)## [1] \"2015-02-17 16:00:00 UTC\""},{"path":"wrangling.html","id":"time-zones","chapter":"2 Wrangling","heading":"2.5.6 Time zones","text":"Time zones enormously complicated topic interaction geopolitical entities. Fortunately, don’t need dig details ’re imperative data analysis. can see complete list possible timezones function OlsonNames(). Unless otherwise specified, lubridate always uses UTC (Coordinated Universal Time).R, time zone attribute date-time controls printing. example, three objects represent instant time:","code":"\n(x1 <- ymd_hms(\"2015-06-01 12:00:00\", tz = \"America/New_York\"))## [1] \"2015-06-01 12:00:00 EDT\"\n(x2 <- ymd_hms(\"2015-06-01 18:00:00\", tz = \"Europe/Copenhagen\"))## [1] \"2015-06-01 18:00:00 CEST\"\n(x3 <- ymd_hms(\"2015-06-02 04:00:00\", tz = \"Pacific/Auckland\"))## [1] \"2015-06-02 04:00:00 NZST\""},{"path":"wrangling.html","id":"combining-data","chapter":"2 Wrangling","heading":"2.6 Combining Data","text":"many ways bring data together.\nFIGURE 2.3: Combining data often tricky.\nbind_rows() function used combine rows two tibbles.","code":"\ndata_1 <- tibble(x = 1:2,                \n                 y = c(\"A\", \"B\")) \n\ndata_2 <- tibble(x = 3:4,\n                 y = c(\"C\", \"D\")) \n\n\nbind_rows(data_1, data_2)## # A tibble: 4 x 2\n##       x y    \n##   <int> <chr>\n## 1     1 A    \n## 2     2 B    \n## 3     3 C    \n## 4     4 D"},{"path":"wrangling.html","id":"joins","chapter":"2 Wrangling","heading":"2.6.1 Joins","text":"Consider two tibbles: superheroes publishers.Note easy use tribble() tibble package create tibble fly using text organized easy entry reading. Recall double colon — :: — indicate function comes specific package.","code":"\nsuperheroes <- tibble::tribble(\n       ~name,   ~gender,     ~publisher,\n   \"Magneto\",   \"male\",       \"Marvel\",\n     \"Storm\",   \"female\",     \"Marvel\",\n    \"Batman\",   \"male\",       \"DC\",\n  \"Catwoman\",   \"female\",     \"DC\",\n   \"Hellboy\",   \"male\",       \"Dark Horse Comics\"\n  )\n\npublishers <- tibble::tribble(\n  ~publisher, ~yr_founded,\n        \"DC\",       1934L,\n    \"Marvel\",       1939L,\n     \"Image\",       1992L\n  )"},{"path":"wrangling.html","id":"inner_join","chapter":"2 Wrangling","heading":"2.6.1.1 inner_join()","text":"inner_join(x, y): Returns rows x matching values y, columns x y. multiple matches x y, combination matches returned.\nFIGURE 2.4: Inner join.\nlose Hellboy join , although appears x = superheroes, publisher Dark Horse Comics appear y = publishers. join result variables x = superheroes plus yr_founded, y.Note message ‘Joining, = “publisher”.’ Whenever joining, R checks see variables common two tibbles , , uses join. However, concerned may aware , R tells . messages annoying signal made code robust . Fortunately, can specify precisely variables want join . Always .also takes vector key variables want merge multiple variables.Now compare result using inner_join() two datasets opposite positions.way, illustrate multiple matches, think x = publishers direction. Every publisher match y = superheroes appears multiple times result, match. fact, ’re getting result inner_join(superheroes, publishers), variable order (also never rely analysis).","code":"\ninner_join(superheroes, publishers)## Joining, by = \"publisher\"## # A tibble: 4 x 4\n##   name     gender publisher yr_founded\n##   <chr>    <chr>  <chr>          <int>\n## 1 Magneto  male   Marvel          1939\n## 2 Storm    female Marvel          1939\n## 3 Batman   male   DC              1934\n## 4 Catwoman female DC              1934\ninner_join(superheroes, publishers, by = \"publisher\")\ninner_join(publishers, superheroes, by = \"publisher\")## # A tibble: 4 x 4\n##   publisher yr_founded name     gender\n##   <chr>          <int> <chr>    <chr> \n## 1 DC              1934 Batman   male  \n## 2 DC              1934 Catwoman female\n## 3 Marvel          1939 Magneto  male  \n## 4 Marvel          1939 Storm    female"},{"path":"wrangling.html","id":"full_join","chapter":"2 Wrangling","heading":"2.6.1.2 full_join()","text":"full_join(x, y): Returns rows columns x y. matching values, returns NA one missing.get rows x = superheroes plus new row y = publishers, containing publisher Image. get variables x = superheroes variables y = publishers. row derives solely one table carries NAs variables found table.full_join() returns rows columns x y, result full_join(x = superheroes, y = publishers) match full_join(x = publishers, y = superheroes).","code":"\nfull_join(superheroes, publishers, by = \"publisher\")## # A tibble: 6 x 4\n##   name     gender publisher         yr_founded\n##   <chr>    <chr>  <chr>                  <int>\n## 1 Magneto  male   Marvel                  1939\n## 2 Storm    female Marvel                  1939\n## 3 Batman   male   DC                      1934\n## 4 Catwoman female DC                      1934\n## 5 Hellboy  male   Dark Horse Comics         NA\n## 6 <NA>     <NA>   Image                   1992"},{"path":"wrangling.html","id":"left_join","chapter":"2 Wrangling","heading":"2.6.1.3 left_join()","text":"left_join(x, y): Returns rows x, columns x y. multiple matches x y, combination matches returned.basically get x = superheroes back, additional variable yr_founded, unique y = publishers. Hellboy, whose publisher appear y = publishers, NA yr_founded.Now compare result running left_join(x = publishers, y = superheroes). Unlike inner_join() full_join() order arguments significant effect resulting tibble.get similar result inner_join(), publisher Image survives join, even though superheroes Image appear y = superheroes. result, Image NAs name gender.similar function, right_join(x, y) returns rows y, columns x y.","code":"\nleft_join(superheroes, publishers, by = \"publisher\")## # A tibble: 5 x 4\n##   name     gender publisher         yr_founded\n##   <chr>    <chr>  <chr>                  <int>\n## 1 Magneto  male   Marvel                  1939\n## 2 Storm    female Marvel                  1939\n## 3 Batman   male   DC                      1934\n## 4 Catwoman female DC                      1934\n## 5 Hellboy  male   Dark Horse Comics         NA\nleft_join(publishers, superheroes, by = \"publisher\")## # A tibble: 5 x 4\n##   publisher yr_founded name     gender\n##   <chr>          <int> <chr>    <chr> \n## 1 DC              1934 Batman   male  \n## 2 DC              1934 Catwoman female\n## 3 Marvel          1939 Magneto  male  \n## 4 Marvel          1939 Storm    female\n## 5 Image           1992 <NA>     <NA>"},{"path":"wrangling.html","id":"semi_join","chapter":"2 Wrangling","heading":"2.6.1.4 semi_join()","text":"semi_join(x, y): Returns rows x matching values y, keeping just columns x. semi join differs inner join inner join return one row x matching row y, whereas semi join never duplicate rows x. filtering join.Compare result switching values arguments.Now effects switching x y roles clear. result resembles x = publishers, publisher Image lost, since observations publisher == \"Image\" y = superheroes.","code":"\nsemi_join(superheroes, publishers, by = \"publisher\")## # A tibble: 4 x 3\n##   name     gender publisher\n##   <chr>    <chr>  <chr>    \n## 1 Magneto  male   Marvel   \n## 2 Storm    female Marvel   \n## 3 Batman   male   DC       \n## 4 Catwoman female DC\nsemi_join(x = publishers, y = superheroes, by = \"publisher\")## # A tibble: 2 x 2\n##   publisher yr_founded\n##   <chr>          <int>\n## 1 DC              1934\n## 2 Marvel          1939"},{"path":"wrangling.html","id":"anti_join","chapter":"2 Wrangling","heading":"2.6.1.5 anti_join()","text":"anti_join(x, y): Return rows x matching values y, keeping just columns x.keep Hellboy now.Now switch arguments compare result.keep publisher Image now (variables found x = publishers).","code":"\nanti_join(superheroes, publishers, by = \"publisher\")## # A tibble: 1 x 3\n##   name    gender publisher        \n##   <chr>   <chr>  <chr>            \n## 1 Hellboy male   Dark Horse Comics\nanti_join(publishers, superheroes, by = \"publisher\")## # A tibble: 1 x 2\n##   publisher yr_founded\n##   <chr>          <int>\n## 1 Image           1992"},{"path":"wrangling.html","id":"example","chapter":"2 Wrangling","heading":"2.6.2 Example","text":"Consider relationships among tibbles nycflights13 package:\nFIGURE 2.5: Relationships among nycflights tables\nflights airlines data frames, key variable want join/merge/match rows name: carrier. Let’s use inner_join() join two data frames, rows matched variable carrierThis first example using join function pipe. flights fed first argument inner_join(). pipe . code equivalent :airports data frame contains airport codes airport:However, look airports flights data frames, ’ll find airport codes variables different names. airports airport code faa, whereas flights airport codes origin dest.order join two data frames airport code, inner_join() operation use = c(\"dest\" = \"faa\") thereby allowing us join two data frames key variable different name.Let’s construct chain pipe operators %>% computes number flights NYC destination, also includes information destination airport:\"ORD\" airport code Chicago O’Hare airport \"FLL\" code main airport Fort Lauderdale, Florida, can seen airport_name variable.","code":"\nflights %>% \n  inner_join(airlines, by = \"carrier\")## # A tibble: 336,776 x 20\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # … with 336,766 more rows, and 12 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   name <chr>\ninner_join(flights, airlines, by = \"carrier\")\nairports## # A tibble: 1,458 x 8\n##    faa   name                       lat    lon   alt    tz dst   tzone          \n##    <chr> <chr>                    <dbl>  <dbl> <dbl> <dbl> <chr> <chr>          \n##  1 04G   Lansdowne Airport         41.1  -80.6  1044    -5 A     America/New_Yo…\n##  2 06A   Moton Field Municipal A…  32.5  -85.7   264    -6 A     America/Chicago\n##  3 06C   Schaumburg Regional       42.0  -88.1   801    -6 A     America/Chicago\n##  4 06N   Randall Airport           41.4  -74.4   523    -5 A     America/New_Yo…\n##  5 09J   Jekyll Island Airport     31.1  -81.4    11    -5 A     America/New_Yo…\n##  6 0A9   Elizabethton Municipal …  36.4  -82.2  1593    -5 A     America/New_Yo…\n##  7 0G6   Williams County Airport   41.5  -84.5   730    -5 A     America/New_Yo…\n##  8 0G7   Finger Lakes Regional A…  42.9  -76.8   492    -5 A     America/New_Yo…\n##  9 0P2   Shoestring Aviation Air…  39.8  -76.6  1000    -5 U     America/New_Yo…\n## 10 0S9   Jefferson County Intl     48.1 -123.    108    -8 A     America/Los_An…\n## # … with 1,448 more rows\nflights %>% \n  inner_join(airports, by = c(\"dest\" = \"faa\"))## # A tibble: 329,174 x 26\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      554            600        -6      812            837\n##  5  2013     1     1      554            558        -4      740            728\n##  6  2013     1     1      555            600        -5      913            854\n##  7  2013     1     1      557            600        -3      709            723\n##  8  2013     1     1      557            600        -3      838            846\n##  9  2013     1     1      558            600        -2      753            745\n## 10  2013     1     1      558            600        -2      849            851\n## # … with 329,164 more rows, and 18 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   name <chr>, lat <dbl>, lon <dbl>, alt <dbl>, tz <dbl>, dst <chr>,\n## #   tzone <chr>\nflights %>%\n  group_by(dest) %>%\n  summarize(num_flights = n(),\n            .groups = \"drop\") %>%\n  arrange(desc(num_flights)) %>%\n  inner_join(airports, by = c(\"dest\" = \"faa\")) %>%\n  rename(airport_name = name)## # A tibble: 101 x 9\n##    dest  num_flights airport_name          lat    lon   alt    tz dst   tzone   \n##    <chr>       <int> <chr>               <dbl>  <dbl> <dbl> <dbl> <chr> <chr>   \n##  1 ORD         17283 Chicago Ohare Intl   42.0  -87.9   668    -6 A     America…\n##  2 ATL         17215 Hartsfield Jackson…  33.6  -84.4  1026    -5 A     America…\n##  3 LAX         16174 Los Angeles Intl     33.9 -118.    126    -8 A     America…\n##  4 BOS         15508 General Edward Law…  42.4  -71.0    19    -5 A     America…\n##  5 MCO         14082 Orlando Intl         28.4  -81.3    96    -5 A     America…\n##  6 CLT         14064 Charlotte Douglas …  35.2  -80.9   748    -5 A     America…\n##  7 SFO         13331 San Francisco Intl   37.6 -122.     13    -8 A     America…\n##  8 FLL         12055 Fort Lauderdale Ho…  26.1  -80.2     9    -5 A     America…\n##  9 MIA         11728 Miami Intl           25.8  -80.3     8    -5 A     America…\n## 10 DCA          9705 Ronald Reagan Wash…  38.9  -77.0    15    -5 A     America…\n## # … with 91 more rows"},{"path":"wrangling.html","id":"tidy-data","chapter":"2 Wrangling","heading":"2.7 Tidy data","text":"Consider first five rows drinks data frame fivethirtyeight package:reading help file running ?drinks, ’ll see drinks data frame contains results survey average number servings beer, spirits, wine consumed 193 countries. data originally reported FiveThirtyEight.com Mona Chalabi’s article: “Dear Mona Followup: People Drink Beer, Wine Spirits?”.Let’s apply data wrangling verbs drinks data frame:filter() drinks data frame consider 4 countries: United States, China, Italy, Saudi Arabia, thenselect() columns except total_litres_of_pure_alcohol using - sign, thenrename() variables beer_servings, spirit_servings, wine_servings beer, spirit, wine, respectively.save resulting data frame drinks_smaller.","code":"## # A tibble: 5 x 5\n##   country    beer_servings spirit_servings wine_servings total_litres_of_pure_a…\n##   <chr>              <int>           <int>         <int>                   <dbl>\n## 1 Afghanist…             0               0             0                     0  \n## 2 Albania               89             132            54                     4.9\n## 3 Algeria               25               0            14                     0.7\n## 4 Andorra              245             138           312                    12.4\n## 5 Angola               217              57            45                     5.9\ndrinks_smaller <- drinks %>%\n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) %>%\n  select(-total_litres_of_pure_alcohol) %>%\n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller## # A tibble: 4 x 4\n##   country       beer spirit  wine\n##   <chr>        <int>  <int> <int>\n## 1 China           79    192     8\n## 2 Italy           85     42   237\n## 3 Saudi Arabia     0      5     0\n## 4 USA            249    158    84"},{"path":"wrangling.html","id":"definition-of-tidy-data","chapter":"2 Wrangling","heading":"2.7.1 Definition of “tidy” data","text":"mean data “tidy?” “tidy” clear English meaning “organized,” word “tidy” data science using R means data follows standardized format.“Tidy” data standard way mapping meaning dataset structure. dataset messy tidy depending rows, columns tables matched observations, variables types. tidy data:variable forms column.observation forms row.type observational unit forms table.","code":""},{"path":"wrangling.html","id":"converting-to-tidy-data","chapter":"2 Wrangling","heading":"2.7.2 Converting to “tidy” data","text":"book far, ’ve seen data frames already “tidy” format. Furthermore, rest book, ’ll mostly see data frames already tidy well. always case, however, datasets world. original data frame wide (non-“tidy”) format like use ggplot2 dplyr packages, first convert “tidy” format. , recommend using pivot_longer() function tidyr package (Wickham 2021d).Going back drinks_smaller data frame earlier:tidy using pivot_longer() function tidyr package follows:Let’s dissect arguments pivot_longer().first argumentnames_to corresponds name variable new “tidy”/long data frame contain column names original data. Observe set names_to = \"type\". resulting drinks_smaller_tidy, column type contains three types alcohol beer, spirit, wine. Since type variable name doesn’t appear drinks_smaller, use quotation marks around . ’ll receive error just use names_to = type .first argumentnames_to corresponds name variable new “tidy”/long data frame contain column names original data. Observe set names_to = \"type\". resulting drinks_smaller_tidy, column type contains three types alcohol beer, spirit, wine. Since type variable name doesn’t appear drinks_smaller, use quotation marks around . ’ll receive error just use names_to = type .second argument values_to corresponds name variable new “tidy” data frame contain values original data. Observe set values_to = \"servings\" since numeric value beer, wine, spirit columns drinks_smaller corresponds value servings. resulting drinks_smaller_tidy, column servings contains 4 \\(\\times\\) 3 = 12 numerical values. Note servings doesn’t appear variable drinks_smaller needs quotation marks around values_to argument.second argument values_to corresponds name variable new “tidy” data frame contain values original data. Observe set values_to = \"servings\" since numeric value beer, wine, spirit columns drinks_smaller corresponds value servings. resulting drinks_smaller_tidy, column servings contains 4 \\(\\times\\) 3 = 12 numerical values. Note servings doesn’t appear variable drinks_smaller needs quotation marks around values_to argument.third argument cols columns drinks_smaller data frame either want don’t want “tidy.” Observe set -country indicating don’t want “tidy” country variable drinks_smaller rather beer, spirit, wine. Since country column appears drinks_smaller don’t put quotation marks around .third argument cols columns drinks_smaller data frame either want don’t want “tidy.” Observe set -country indicating don’t want “tidy” country variable drinks_smaller rather beer, spirit, wine. Since country column appears drinks_smaller don’t put quotation marks around .third argument cols little nuanced, let’s consider code written slightly differently produces output:Note third argument now specifies columns want “tidy” c(beer, spirit, wine), instead columns don’t want “tidy” using -country. use c() function create vector columns drinks_smaller ’d like “tidy.” Note since three columns appear one another drinks_smaller data frame, can also following cols argument:Converting “wide” format data “tidy” format often confuses new R users. way get comfortable pivot_longer() function practice, practice, practice using different datasets. example, run ?pivot_longer look examples bottom help file., however, want convert “tidy” data frame “wide” format, need use pivot_wider() function instead. Run ?pivot_wider look examples bottom help file examples.can also view examples pivot_longer() pivot_wider() tidyverse.org webpage. ’s nice example check different functions available data tidying case study using data World Health Organization webpage. Furthermore, week R4DS Online Learning Community posts dataset weekly #TidyTuesday event might serve nice place find data explore transform.","code":"\ndrinks_smaller## # A tibble: 4 x 4\n##   country       beer spirit  wine\n##   <chr>        <int>  <int> <int>\n## 1 China           79    192     8\n## 2 Italy           85     42   237\n## 3 Saudi Arabia     0      5     0\n## 4 USA            249    158    84\ndrinks_smaller_tidy <- drinks_smaller %>% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy## # A tibble: 12 x 3\n##    country      type   servings\n##    <chr>        <chr>     <int>\n##  1 China        beer         79\n##  2 China        spirit      192\n##  3 China        wine          8\n##  4 Italy        beer         85\n##  5 Italy        spirit       42\n##  6 Italy        wine        237\n##  7 Saudi Arabia beer          0\n##  8 Saudi Arabia spirit        5\n##  9 Saudi Arabia wine          0\n## 10 USA          beer        249\n## 11 USA          spirit      158\n## 12 USA          wine         84\ndrinks_smaller %>% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\ndrinks_smaller %>% \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)"},{"path":"wrangling.html","id":"distributions","chapter":"2 Wrangling","heading":"2.8 Distributions","text":"distribution function shows possible values variable often occur.Think distribution variable urn can pull , random, values variable. Drawing thousand values urn, looking histogram, can show values centered values vary. people sloppy, use word distribution refer (imaginary!) urn drawing values list values drawn. better, however, keep three distinct ideas separate:unknown true distribution , reality, generates data see. Outside stylized examples assume distribution follows simple mathematical formula, never access unknown true distribution. can estimate , however imperfectly. unknown true distribution often referred data generating mechanism, DGM. function black box urn produces data. can see data. can’t see urn. Later Primer, learned posterior distributions, often refer Preceptor’s Posterior.unknown true distribution , reality, generates data see. Outside stylized examples assume distribution follows simple mathematical formula, never access unknown true distribution. can estimate , however imperfectly. unknown true distribution often referred data generating mechanism, DGM. function black box urn produces data. can see data. can’t see urn. Later Primer, learned posterior distributions, often refer Preceptor’s Posterior.estimated distribution , think, generates data see. , can never know unknown true distribution. , making assumptions using data , can estimate distribution. estimate may close true distribution. may far away. main task data science create use estimated distributions. Almost always, distributions instantiated computer code.estimated distribution , think, generates data see. , can never know unknown true distribution. , making assumptions using data , can estimate distribution. estimate may close true distribution. may far away. main task data science create use estimated distributions. Almost always, distributions instantiated computer code.vector numbers drawn estimated distribution. true estimated distributions can complex beasties, difficult describe accurately detail. vector numbers drawn distribution easy understand use. , general, work vectors numbers. someone — either colleague piece R code — created distribution want use answer question, don’t really want distribution . just want vectors “draws” distribution. Vectors easy work ! Complex computer code .vector numbers drawn estimated distribution. true estimated distributions can complex beasties, difficult describe accurately detail. vector numbers drawn distribution easy understand use. , general, work vectors numbers. someone — either colleague piece R code — created distribution want use answer question, don’t really want distribution . just want vectors “draws” distribution. Vectors easy work ! Complex computer code ., people (including us!) often sloppy use word, “distribution,” without making clear whether talking true distribution, estimated distribution, vector draws estimated distribution. Try sloppy.","code":""},{"path":"wrangling.html","id":"scaling-a-distribution","chapter":"2 Wrangling","heading":"2.8.1 Scaling a distribution","text":"Consider vector result rolling one die 10 times.ways storing data vector. Instead recording every draw, just record number times value appears.case, 10 values, actually less efficient store data like . happens 10,000 rolls.Instead keeping around vector length 10,000, can just keep 10 values, without losing information.example also highlights fact , graphically, two distributions can identical even different lengths.two vectors — rolls more_rolls — exact shape , even though lengths differ, thing. total count value matter. matters relative proportions.Since shape matters, often “normalize” distributions sum counts equals one, meaning y-axis percentage total. Example:","code":"\nrolls <- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)\ntable(rolls)## rolls\n## 1 2 4 5 6 \n## 2 2 1 4 1\nmore_rolls <- rep(rolls, 1000)\ntable(more_rolls)## more_rolls\n##    1    2    4    5    6 \n## 2000 2000 1000 4000 1000"},{"path":"wrangling.html","id":"sample","chapter":"2 Wrangling","heading":"2.8.2 sample()","text":"common distributions work empirical frequency distributions, values age trains tibble, values poverty kenya tibble, . can also create data making “draws” distribution concocted.Consider distribution possible values rolling fair die. can use sample() function create draws distribution, meaning change (sometimes stay ) every subsequent draw.produces one “draw” distribution possible values one roll fair six-sided die.Now, suppose wanted roll die 10 times. One arguments sample() function replace. must specify TRUE values can appear . Since, rolling die 10 times, expect value like 3 can appear , need set replace = TRUE.words, rolling 1 first roll preclude rolling 1 later roll.die “fair,” meaning sides likely appear others? final argument sample() function prob argument. takes vector (length initial vector x) contains probabilities selecting one elements x. Suppose probability rolling 1 0.5, probability rolling value 0.1. (probabilities sum 1. don’t sample() automatically re-scale .)Remember: real data . actually rolled die. just made assumptions happen roll die. assumptions built urn — data generating mechanism — can draw many values like. Let’s roll unfair die 10,000 times.makes dual nature distributions clear. distribution “thing” see plot. mathematical object several different parts, including set possible values (1 6, case) record number times value appears. distribution also simple vector numbers used create plot.general, travel back--forth distribution thing distribution vector draws thing, depending trying accomplish.sample() just one many functions creating draws — , colloquially, “drawing” — distribution. Three important functions : runif(), rbinom(), rnorm().","code":"\ndie <- c(1, 2, 3, 4, 5, 6)\n\nsample(x = die, size = 1)## [1] 3\nsample(x = die, size = 10, replace = TRUE)##  [1] 5 6 3 3 3 4 3 6 4 5\nsample(x = die, \n       size = 10, \n       replace = TRUE, \n       prob = c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1))##  [1] 1 1 3 1 2 1 1 6 1 1\ntibble(result = sample(x = die, \n                       size = 10000, \n                       replace = TRUE, \n                       prob = c(0.5, rep(0.1, 5)))) %>% \n  ggplot(aes(x = result)) +\n    geom_bar() +\n    labs(title = \"Distribution of Results of an Unfair Die\",\n         x = \"Result of One Roll\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 1:6,\n                       labels = as.character(1:6)) +\n    scale_y_continuous(labels = scales::comma_format())"},{"path":"wrangling.html","id":"runif","chapter":"2 Wrangling","heading":"2.8.3 runif()","text":"Consider “uniform” distribution. case every outcome range possible outcomes chance occurring. function runif() (spoken “r-unif”) enables us draw uniform contribution. runif() three arguments: n, min, max. runif() produce n draws min max, value equal chance occurring.Mathematically, write:\\[y_i \\sim U(4, 6)\\],means value \\(y\\) drawn uniform distribution four six.","code":"\nrunif(n = 10, min = 4, max = 6)##  [1] 5.6 5.0 4.6 5.7 4.9 5.2 4.3 5.6 4.8 5.6"},{"path":"wrangling.html","id":"rbinom","chapter":"2 Wrangling","heading":"2.8.4 rbinom()","text":"Consider binomial distribution, case probability Boolean variable (instance success failure) calculated repeated, independent trials. One common example probability flipping coin landing heads. function rbinom() allows us draw binomial distribution. function takes three arguments, n, size, prob.n number values seek draw.\nsize number trials n.\n*prob probability success trial.Suppose wanted flip fair coin one time, let landing heads represent success.thing 100 times:graph , use function scale_x_continuous() x-axis variable continuous, meaning can take real values. breaks argument scale_x_continuous() converts x-axis two different “tick marks.” fairly even distribution Tails Heads. draws typically result even equal split.Randomness creates (inevitable) tension distribution “thing” distribution vector draws thing. case, vector draws balanced Tails Heads. Yet, “know” since coin , definition, fair. sense, mathematics require even split. Yet, randomness means vector draws rarely match mathematically “true” result. OK! First, randomness intrinsic property real world. Second, can make effect randomness small want increasing number draws.Suppose instead wanted simulate unfair coin, probability landing Heads 0.75 instead 0.25.distribution — imaginary urn — draw results coin flip fair coin different distribution — different imaginary urn — distribution biased coin. fact, infinite number distributions. Yet long can draw values distribution, can work . Mathematics:\\[y_i \\sim B(n, p)\\].value \\(y\\) drawn binomial distribution parameters \\(n\\) number trials \\(p\\) probability success.Instead n consisting single trial, situation , 10,000 times, flipping coin 10 times summing , experiment, number heads. case:","code":"\nrbinom(n = 1 , size = 1, prob = 0.5)## [1] 0\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.5)) %>% \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.75)) %>% \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\nset.seed(9)\ntibble(heads = rbinom(n = 10000, size = 10, prob = 0.5)) %>% \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 10 Times\",\n         subtitle = \"Extreme results are possible with enough experiments\",\n         x = \"Total Number of Heads in Ten Flips\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 0:10)"},{"path":"wrangling.html","id":"normal","chapter":"2 Wrangling","heading":"2.8.5 rnorm()","text":"important distribution normal distribution. Mathematics:\\[y_i \\sim N(\\mu, \\sigma^2)\\].value \\(y_i\\) drawn normal distribution parameters \\(\\mu\\) mean \\(\\sigma\\) standard deviation.bell-shaped distribution defined two parameters: (1) mean \\(\\mu\\) (spoken “mu”) locates center distribution (2) standard deviation \\(\\sigma\\) (spoken “sigma”) determines variation values around center. figure , plot three normal distributions :solid normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).dotted normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).dashed normal curve mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\nFIGURE 2.6: Three normal distributions.\nNotice solid dotted line normal curves center due common mean \\(\\mu\\) = 5. However, dotted line normal curve wider due larger standard deviation \\(\\sigma = 5\\). hand, solid dashed line normal curves variation due common standard deviation \\(\\sigma = 2\\). However, centered different locations.mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\), normal distribution special name. ’s called standard normal distribution \\(z\\)-curve.Furthermore, variable follows normal curve, three rules thumb can use:68% values lie within \\(\\pm\\) 1 standard deviation mean.95% values lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations mean.99.7% values lie within \\(\\pm\\) 3 standard deviations mean.Let’s illustrate standard normal curve. dashed lines -3, -1.96, -1, 0, 1, 1.96, 3. 7 lines cut x-axis 8 segments. areas normal curve 8 segments marked add 100%. example:middle two segments represent interval -1 1. shaded area interval represents 34% + 34% = 68% area curve. words, 68% values.middle four segments represent interval -1.96 1.96. shaded area interval represents 13.5% + 34% + 34% + 13.5% = 95% area curve. words, 95% values.middle six segments represent interval -3 3. shaded area interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% area curve. words, 99.7% values.\nFIGURE 2.7: Rules thumb areas normal curves.\nfunction rnorm() (spoken “r-norm”) returns draws normal distribution. rnorm() three arguments: n, mean, sd. n corresponds number draws, mean sd \\(\\mu\\) \\(\\sigma\\) distribution want draw. , imagine urn filled beads. bead number written . distribution standard normal, can draw 10 beads urn running:10 draws come distribution default mean 0 default sd 1. create histogram values?can see, symmetrical one displayed . surprising! just draw 10 beads urn, can possibly good sense numbers beads urn look like. draw 100 values? 100,000?Now ’s looking lot similar “truth,” although still imperfect.Now, let’s compare normal distributions varying means standard deviations, can set using mean sd arguments included function.","code":"\nrnorm(10)##  [1] -0.84  1.38 -1.26  0.07  1.71 -0.60 -0.47 -0.64 -0.29  0.14\ntibble(value = rnorm(10)) %>% \n  ggplot(aes(x = value)) + \n    geom_histogram(bins = 10)\ntibble(value = rnorm(100)) %>% \n  ggplot(aes(x = value)) +\n    geom_histogram(bins = 10)\ntibble(value = rnorm(100000)) %>% \n  ggplot(aes(x = value)) +\n    geom_histogram(bins = 1000)\ntibble(rnorm_5_1 = rnorm(n = 1000, mean = 5, sd = 1), \n       rnorm_0_3 = rnorm(n = 1000, mean = 0, sd = 3),\n       rnorm_0_1 = rnorm(n = 1000, mean = 0, sd = 1)) %>%\n  pivot_longer(cols = everything(), \n               names_to = \"distribution\", \n               values_to = \"value\") %>% \n  ggplot(aes(x = value, fill = distribution)) +\n    geom_density(alpha = 0.5) +\n    labs(title = \"Comparison of Normal Distributions with Differing Mean and Standard Deviation Values\", \n         fill = \"Distribution\",\n         x = \"Value\",\n         y = \"Density\")"},{"path":"wrangling.html","id":"working-with-draws","chapter":"2 Wrangling","heading":"2.8.6 Working with draws","text":"vector draws, can examine various aspects distribution. Examples:case distinction true distribution estimated distribution. know, assumption, truth .Even though know, wrote code, draws come normal distribution mean 2 standard deviation 1, calculated results match values exactly draws random.Note likely use median mad summarize distribution. case, similar mean standard deviation.practice, know exact distribution generates data. (know, estimation necessary.) inherent randomness world means calculated statistics match underlying truth perfectly. data collect, closer match .addition mean standard deviation draws, often interested various quantiles distribution, commonly want create intervals cover specified portion draws. Examples:Note draws come distribution centered around 2 rather 0. nothing intrinsically special ranges. mere convention, especially 95% interval.Note cavalier sometimes using word “distribution” sometimes word “draws.” two different things! distribution underlying reality, know certain create , example. draws vector numbers , assume, “drawn” underlying distribution , general, know.assumption, can analyze draws make inferences distribution.Although distributions (draws therefrom) complex, can often treat way treat simple numbers. example, can add two distributions together.Drawing distribution also allows us answer questions via simulation. example, imagine B flipping fair coins. flips coin 3 times. B flips coin 6 times. probability flips heads B?obvious B win game often . also obvious win time. order estimate chances winning, can simply simulate playing game 1,000 times.9% chance winning game.data science, important kind distribution probability distribution, concept introduce Chapter 5.","code":"\ndraws <- rnorm(100, mean = 2, sd = 1)\nmean(draws)## [1] 1.9\nsd(draws)## [1] 1\nmedian(draws)## [1] 1.9\nmad(draws)## [1] 1.1\nquantile(draws, probs = c(0.25, 0.75))## 25% 75% \n## 1.2 2.7\nquantile(draws, probs = c(0.05, 0.95))##  5% 95% \n## 0.4 3.5\nquantile(draws, probs = c(0.025, 0.975))##  2.5% 97.5% \n##  0.15  3.80\nn <- 100000\ntibble(Normal = rnorm(n, mean = 1),\n       Uniform = runif(n, min = 2, max = 3),\n       Combined = Normal + Uniform) %>% \n  pivot_longer(cols = everything(),\n               names_to = \"Distribution\",\n               values_to = \"draw\") %>% \n  ggplot(aes(x = draw, fill = Distribution)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Two Distributions and Their Sum\",\n         subtitle = \"You can sum distributions just like you sum numbers\",\n         x = \"Value\",\n         y = \"Probability\")\nset.seed(56)\ngames <- 1000 \n\ntibble(A_heads = rbinom(n = games, size = 3, prob = 0.5),\n       B_heads = rbinom(n = games, size = 6, prob = 0.5)) %>% \n  mutate(A_wins = ifelse(A_heads > B_heads, 1, 0)) %>% \n  summarize(A_chances = mean(A_wins))## # A tibble: 1 x 1\n##   A_chances\n##       <dbl>\n## 1     0.091"},{"path":"wrangling.html","id":"other-commands","chapter":"2 Wrangling","heading":"2.9 Other Commands","text":"topics prove important later Primer.","code":""},{"path":"wrangling.html","id":"matrices","chapter":"2 Wrangling","heading":"2.9.1 Matrices","text":"Recall “matrix” R rectangular array data, shaped like data frame tibble, containing one type data, e.g., numeric. Large matrices also print ugly. (differences, none care .) Example:easiest way pull information matrix use [ ], subset operator. grab second third columns m:Note, however, matrices just one dimension “collapse” single vectors:Tibbles, hand, always maintain rectangular shapes, even one column row.can turn matrices tibbles as_tibble().m column names, as_tibble() creates won variables names, using “V” variable.","code":"\nm <- matrix(c(3, 4, 8, 9, 12, 13, 0, 15, -1), ncol = 3)\nm##      [,1] [,2] [,3]\n## [1,]    3    9    0\n## [2,]    4   12   15\n## [3,]    8   13   -1\nm[, 2:3]##      [,1] [,2]\n## [1,]    9    0\n## [2,]   12   15\n## [3,]   13   -1\nm[, 2]## [1]  9 12 13\nm %>% \n  as_tibble()## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\n## Using compatibility `.name_repair`.## # A tibble: 3 x 3\n##      V1    V2    V3\n##   <dbl> <dbl> <dbl>\n## 1     3     9     0\n## 2     4    12    15\n## 3     8    13    -1"},{"path":"wrangling.html","id":"missing-values","chapter":"2 Wrangling","heading":"2.9.2 Missing Values","text":"observations tibble blank. called missing values, often marked NA. can create tibble follows:presence NA values can problematic.Fortunately, R functions take argument, na.rm, , set TRUE, removes NA values calculations.Another approach use drop_na().careful, however, use drop_na() without specific variable provided. case, remove rows missing value variable tibble.final approach use .na() explicitly determine value missing.","code":"\ntbl <- tribble(\n  ~ a, ~ b, ~ c,\n    2,   3,   5,\n    4,  NA,   8,\n   NA,   7,   9,\n)\n\ntbl## # A tibble: 3 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\n## 2     4    NA     8\n## 3    NA     7     9\ntbl %>% \n  summarize(avg_a = mean(a))## # A tibble: 1 x 1\n##   avg_a\n##   <dbl>\n## 1    NA\ntbl %>% \n  summarize(avg_a = mean(a, na.rm = TRUE))## # A tibble: 1 x 1\n##   avg_a\n##   <dbl>\n## 1     3\ntbl %>% \n  drop_na(a)## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\n## 2     4    NA     8\ntbl %>% \n  drop_na()## # A tibble: 1 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\ntbl %>% \n  mutate(a_missing = is.na(a))## # A tibble: 3 x 4\n##       a     b     c a_missing\n##   <dbl> <dbl> <dbl> <lgl>    \n## 1     2     3     5 FALSE    \n## 2     4    NA     8 FALSE    \n## 3    NA     7     9 TRUE"},{"path":"wrangling.html","id":"working-by-rows","chapter":"2 Wrangling","heading":"2.9.3 Working by rows","text":"Tibbles main Tidyverse functions designed work columns. something values variable . , sometimes, want work across tibble, comparing value first row value b first row, . , need two tricks. First, use rowwise() inform R next set commands executed across rows.Note “# Rowwise:” printed . set pipe work across rows, need pass c_across() whichever function using, generally specifying variables want use. don’t provide arguments c_across(), use columns tibble.","code":"\ntbl %>% \n  rowwise()## # A tibble: 3 x 3\n## # Rowwise: \n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\n## 2     4    NA     8\n## 3    NA     7     9\ntbl %>% \n  rowwise() %>% \n  mutate(sum_a_c = sum(c_across(c(a, c)))) %>% \n  mutate(largest = max(c_across())) %>% \n  mutate(largest_na = max(c_across(), na.rm = TRUE))## # A tibble: 3 x 6\n## # Rowwise: \n##       a     b     c sum_a_c largest largest_na\n##   <dbl> <dbl> <dbl>   <dbl>   <dbl>      <dbl>\n## 1     2     3     5       7       7          7\n## 2     4    NA     8      12      NA         12\n## 3    NA     7     9      NA      NA          9"},{"path":"wrangling.html","id":"using-skim","chapter":"2 Wrangling","heading":"2.9.4 Using skim()","text":"skimr package offers useful function know skim() function, allows get valuable information data set one glance. similar glimpse() function ’s little detailed offers preliminary analysis topic.Let’s try skimming nhanes dataset.TABLE 2.1: Data summaryVariable type: characterVariable type: factorVariable type: numericThe skim() function provides information mean, number unique counts, even bar graphs distribution data. information extremely useful allows us easily find things data give us starting point. example, 7000 missing values pregnancies variable. means ’re going run drop_na() can ignore 7000 missing values.running skim() can create starting point, analysis creation graph.","code":"\nskim(nhanes)"},{"path":"wrangling.html","id":"summary-2","chapter":"2 Wrangling","heading":"2.10 Summary","text":"Data science data cleaning.Real data nasty.chapter covered many, many commands. memorized now.! ridiculous. don’t memorized. ? point chapter give tour can R . information, base try solve problems encounter future.key data science concept chapter , , idea “distribution.” word distribution used two different ways. First, distribution invisible object can never use touch. imaginary urn can take draws. special cases ever able “know” distribution , mainly case physical process, like roulette wheel, can inspect case assumed mathematical formula. , almost real world data science problems, “distribution” mental creation whose reality can never confirm.second way word distribution used refer vector values, variable R tibble. 115 ages trains distribution 1,000 draws rnorm().Whether “distribution” means imaginary object vector numbers drawn imaginary object depends context.","code":""},{"path":"data.html","id":"data","chapter":"3 Data","heading":"3 Data","text":"can never look data much. – Mark Engerman","code":""},{"path":"data.html","id":"introduction","chapter":"3 Data","heading":"3.1 Introduction","text":"Getting data R major part real world data science project. multiple data formats use transport data, positives negatives.Start loading packages need chapter.can find files use chapter . can also access files saving URL R using can download access files located internet.","code":"\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(dbplyr)\nlibrary(janitor)\ngithub_url <- \"https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/www/\""},{"path":"data.html","id":"reading-and-writing-files","chapter":"3 Data","heading":"3.2 Reading and writing files","text":"\nFIGURE 3.1: Choose file formats wisely.\nfirst method can use import data using file. ’ve likely downloaded files , whether ’s game’s EXE file, image’s JPG file, essay’s PDF file. core, files just data. JPG file bunch data colors image PDF file bunch data text. can use files store data experiments surveys can analyze data later share data people.section, ’ll going common file formats can pull data files R create new files can share people.","code":""},{"path":"data.html","id":"text-files","chapter":"3 Data","heading":"3.2.1 Text files","text":"common type data text file “CSV,” stands comma separated value. words, CSV files files whose values separated commas. comma csv file corresponds column, column names , default, taken first line file.CSV files (counterparts) easily transferable computers programming languages extremely simple ’re effectively just text files special format. Additionally, ’re easy parse small amounts data ’re easily readable humans computers.\nHowever, due simple nature, CSV files able move basic data text values. also poor support special characters like commas, can make dataset harder organize understand adding new column specific entries. make CSV files good sharing data computers languages, efficient transporting saving large amounts data.","code":""},{"path":"data.html","id":"reading-and-writing-from-a-csv-file","chapter":"3 Data","heading":"3.2.1.1 Reading and writing from a CSV file","text":"’s example CSV file looks like. Use read_csv() readr package — one main packages within tidyverse collection packages — load data R. file argument file path CSV file.Use write_csv() save tibble csv file. write_csv() two main arguments: x file. x argument data set want save. file argument file path want save file. end file argument name want use file.read csv file , data shows . useful saving information share projects.want remove file system, use file.remove().Sometimes, CSV files want. Maybe wrong column names, information top file, comments interspersed within file.","code":"\n# We can access files either by using their URL or their file path. In this\n# case, we're using the GitHub URL to access the database. This is done by\n# pasting the file name into our URL using the paste0() command This does the\n# same thing as creating the URL below.\n# https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/www/test_1.csv\n\nfile_1 <- paste0(github_url, \"test_1.csv\")\n\nread_csv(file = file_1)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   a = col_double(),\n##   b = col_double(),\n##   c = col_double()\n## )## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\ncsv_data <- tribble(\n  ~ `a`, ~ `b`, ~ `c`,\n      1,     2,     3,\n      4,     5,     6)\n\nwrite_csv(x = csv_data, file = \"my_csv.csv\")\nread_csv(\"my_csv.csv\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   a = col_double(),\n##   b = col_double(),\n##   c = col_double()\n## )## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\nfile.remove(\"my_csv.csv\")## [1] TRUE"},{"path":"data.html","id":"skip","chapter":"3 Data","heading":"3.2.1.2 skip","text":"Consider following csv file: test_2.csv. ’s looks like text file:can see, text top file. Often times information data collected, relevant information, included top data file. However, read_csv() can’t differentiate text data want read, causing fail output gibberish.can use skip argument skip first 2 text lines allow read_csv() work.Now ’ve gotten rid warnings, let’s look message R sends: column specification message.","code":"## [1] \"Top two rows consist of junk which\"        \n## [2] \"we don't care about. Data starts on row 3.\"\n## [3] \"a,b,c\"                                     \n## [4] \"9,8,7\"                                     \n## [5] \"4,5,6\"\n# You can also get a csv file by using the URL of the file. This won't work for\n# all file types though.\n\nfile_2 <- paste0(github_url, \"test_2.csv\")\n\nread_csv(file_2)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   `Top two rows consist of junk which` = col_character()\n## )## Warning: 3 parsing failures.\n## row col  expected    actual                                                                                  file\n##   2  -- 1 columns 3 columns 'https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/www/test_2.csv'\n##   3  -- 1 columns 3 columns 'https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/www/test_2.csv'\n##   4  -- 1 columns 3 columns 'https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/www/test_2.csv'## # A tibble: 4 x 1\n##   `Top two rows consist of junk which`      \n##   <chr>                                     \n## 1 we don't care about. Data starts on row 3.\n## 2 a                                         \n## 3 9                                         \n## 4 4\nread_csv(file = file_2,\n         skip = 2)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   a = col_double(),\n##   b = col_double(),\n##   c = col_double()\n## )## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     9     8     7\n## 2     4     5     6"},{"path":"data.html","id":"col_types","chapter":"3 Data","heading":"3.2.1.3 col_types","text":"column specification message message R sends tell data types using column.Data types types discussed Chapter 2 Wrangling, characters, factors, integers, dates.\nuse tibble, column specific type data. example, columns numbers . characters column, columns going character data type.get rid column specification message, use col_types() argument specify data types. can just copying column specification message putting col_types() argument.can also change column arguments get data type want.\nTake test_7.csv.Let’s try parsing student column factor grade column integer.clever columns, can make lives easier line graph data.can also manipulate arguments CSV files.","code":"\nread_csv(file = file_2,\n         skip = 2,\n         col_types = cols(a = col_double(),\n                          b = col_double(),\n                          c = col_double()))## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     9     8     7\n## 2     4     5     6\ntest_7 <- paste0(github_url, \"test_7.csv\")\n\nread_csv(test_7)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   grade = col_double(),\n##   student = col_character()\n## )## # A tibble: 2 x 2\n##   grade student\n##   <dbl> <chr>  \n## 1     1 Sam    \n## 2     5 Becca\nread_csv(test_7,\n         col_types = cols(grade = col_integer(),\n                          student = col_factor()))## # A tibble: 2 x 2\n##   grade student\n##   <int> <fct>  \n## 1     1 Sam    \n## 2     5 Becca"},{"path":"data.html","id":"col_names-and-clean_names","chapter":"3 Data","heading":"3.2.1.4 col_names and clean_names()","text":"Let’s try changing column names test_3.csv file.can see , file doesn’t column names, resulting first row considered names rest file.can fix changing col_names argument.can also create names automatically setting col_names FALSEChanging names columns allows call columns later data actually tibble. Setting column names something can understand makes much easier understand code later .good column names, just aren’t formatted correctly? Let’s look test_4.csv example.can see, function compile names aren’t easy access. ’s possible access column using ` tickmark like :can still cause problems line ’s just annoying use backticks every time want column name. can use clean_names() function janitor package. essentially formats column names follow [underscore separated naming convention](https://en.wikipedia.org/wiki/Snake_case#:~:text=Snake%20case%20(stylized%20as%20snake_case,subroutine%20names%2C%20and%20for%20filenames.) unique.cleans column names, saving time file large amount columns need type lot column names. issues common ’re pulling data internet ’s normally dirty data can lot columns formatted weird ways.","code":"\nfile_3 <- paste0(github_url, \"test_3.csv\")\n\nread_csv(file_3)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   `11` = col_double(),\n##   `21` = col_double(),\n##   `33` = col_double()\n## )## # A tibble: 1 x 3\n##    `11`  `21`  `33`\n##   <dbl> <dbl> <dbl>\n## 1     4     5     6\nread_csv(file_3, col_names = c(\"a\", \"b\", \"c\"))## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   a = col_double(),\n##   b = col_double(),\n##   c = col_double()\n## )## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    11    21    33\n## 2     4     5     6\nread_csv(file_3, col_names = FALSE)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   X1 = col_double(),\n##   X2 = col_double(),\n##   X3 = col_double()\n## )## # A tibble: 2 x 3\n##      X1    X2    X3\n##   <dbl> <dbl> <dbl>\n## 1    11    21    33\n## 2     4     5     6\nfile_4 <- paste0(github_url, \"test_4.csv\")\n\nfile_4_tibble <- read_csv(file_4)## Warning: Duplicated column names deduplicated: 'Two_Powers' =>\n## 'Two_Powers_1' [4]## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   `one powers` = col_double(),\n##   Two_Powers = col_double(),\n##   `3_Powers` = col_double(),\n##   Two_Powers_1 = col_double()\n## )\nfile_4_tibble## # A tibble: 3 x 4\n##   `one powers` Two_Powers `3_Powers` Two_Powers_1\n##          <dbl>      <dbl>      <dbl>        <dbl>\n## 1            1          2          3            2\n## 2            1          4         81            5\n## 3            1          8         27            8\nfile_4_tibble$`one powers`## [1] 1 1 1\nfile_4_tibble %>% \n  clean_names()## # A tibble: 3 x 4\n##   one_powers two_powers x3_powers two_powers_1\n##        <dbl>      <dbl>     <dbl>        <dbl>\n## 1          1          2         3            2\n## 2          1          4        81            5\n## 3          1          8        27            8"},{"path":"data.html","id":"na","chapter":"3 Data","heading":"3.2.1.5 na","text":"Another feature read_csv() na argument.test_5.csv missing value, uses . substitute. makes computer think period actual value data point, obviously true (want numbers instead). default, read_csv() treats white space like spaces tabs missing value, can set argument directly well.","code":"\nfile_5 <- paste0(github_url, \"test_5.csv\")\n\nread_csv(file_5)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   a = col_double(),\n##   b = col_character(),\n##   c = col_double()\n## )## # A tibble: 2 x 3\n##       a b         c\n##   <dbl> <chr> <dbl>\n## 1     1 .         3\n## 2     4 5         6\nread_csv(file_5,\n         na = \".\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   a = col_double(),\n##   b = col_double(),\n##   c = col_double()\n## )## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1    NA     3\n## 2     4     5     6"},{"path":"data.html","id":"comment","chapter":"3 Data","heading":"3.2.1.6 comment","text":"can also tell code ignore comment lines, may common csv file written human comments . test_6.csv perfect example . ’s looks like:setting comment argument, ’re able skip lines certain starting point.\ncase, comment # sign, just need include .ever want skip certain lines, just use comment argument order designate something skipped. best used cases read_csv() command compile without .","code":"## [1] \"a,b,c\"                          \"# This is a comment line\"      \n## [3] \"98,99,100\"                      \"# Here is another comment line\"\n## [5] \"4,5,6\"\nread_csv(file_6, comment = \"#\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   a = col_double(),\n##   b = col_double(),\n##   c = col_double()\n## )## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    98    99   100\n## 2     4     5     6"},{"path":"data.html","id":"read_delim","chapter":"3 Data","heading":"3.2.1.7 read_delim()","text":"far, ’ve covered can organize CSV data, ’s core ’re working comma separated values. happens want read data something doesn’t use commas separate values?tabular data comes different format, can use read_delim() function instead. example, different version test_6.csv exist column names uses pipes (|) delimiter instead commas.’s another file named delim_1, uses | separate lines instead comma like normal CSV file.read_delim(), specify first argument path file, done read_csv(). provide values delim argument code use | separator instead comma.can often find CSV files websites like kaggle well exporting Excel spreadsheet. Keep mind data imported internet often messy, try use functions listed clean . full list arguments -depth documentation read_csv() function, please visit website.","code":"## [1] \"population|town\"   \"150|Cambridge, MA\" \"92|Newton, MA\"\n# Because delim_1 uses pipes to separate values, we can just use that as our\n# delim value. However, for more complex symbols like tab, we use something\n# different like \"\\\\t\". This varies for every symbol, but you can find most\n# delim values on the internet.\n\ndelim_1 <- paste0(github_url, \"delim_1.txt\")\n\nread_delim(delim_1, delim = \"|\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   population = col_double(),\n##   town = col_character()\n## )## # A tibble: 2 x 2\n##   population town         \n##        <dbl> <chr>        \n## 1        150 Cambridge, MA\n## 2         92 Newton, MA"},{"path":"data.html","id":"excel-files","chapter":"3 Data","heading":"3.2.2 Excel files","text":"Excel spreadsheet program use tables analyze, store, manipulate data. tables composed cells include text, numbers, formulas. Excel files filename extensions .xls .xlsx, ’re capable storing additional things store .csv file fonts, text formatting, graphics, etc.order write excel files install complex packages, hard create. Writing excel files beyond scope Primer.makes Excel files valuable ’re commonly accepted usable (Microsoft Excel common program), ’re also hard use can’t write new data . makes Excel files common data originally Excel, like accounting data spreadsheet applications.Reading Excel files easy. , use read_excel() function readxl package..xlsx file multiple sheets, use sheet argument specify sheet number name.\nread_excel() function also arguments similar read_csv() function col_names, col_types, na.","code":"\nlibrary(readxl)\n\n# Unfortunately, it is not possible to read Excel files directly from the web.\n# So we download the file by hand and then read it in from the current working\n# directory. Note that the \"proper\" way of handling this would be to create a\n# temp directory with tempdir(), download the file into that directory, read it,\n# and then delete the temp directory. That way, you would not have random\n# downloaded files hanging around.\n\n# The mode = \"wb\" is a necessary addition for Windows users because Windows is\n# weird. It's not necessary on MacOS and may cause an error as well.\n\ndownload.file(url = paste0(github_url, \"excel_1.xlsx\"), \n              destfile = \"example_excel.xlsx\", mode = \"wb\")\n\n\nread_excel(path = \"example_excel.xlsx\")## # A tibble: 2 x 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6"},{"path":"data.html","id":"rds-files","chapter":"3 Data","heading":"3.2.3 RDS files","text":"One important aspect R saving objects RDS files, store single R object file.\nfiles allow us save R objects plots tibbles R reload object contains later without re-running code made .\nespecially useful ’re dealing bulk data want save plot comes later don’t data wrangling plotting .\nRDS file, save entire object, allowing things later without go code.\nHowever, RDS files limited R projects , ’re incomprehensible human eyes programming languages. makes RDS files ideal saving objects temporarily project sharing objects R users.Take following R object, graph iris data set.save RDS file, use function write_rds(). Just like write_csv(), function two main arguments: x file. x argument object want save. file argument file path want save file. determines name use file.read_rds() reads file back R. Just like read_csv() read_rds() one main argument, path file wanting read R.can use R object operations, adding trend line.saving iris_p plot RDS file, eliminate time needed calculate generate plot can use saved information. can use object reading file back r using like normal plot, adding new layers new operations.also .Rdata files can store multiple objects, RDS files can accomplish similar task. makes much easier use RDS files anything want keep R.","code":"\niris_p <- iris %>% \n  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_jitter() +\n  labs(title = \"Sepal Dimensions of Various Species of Iris\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\")\niris_p\nwrite_rds(x = iris_p, file = \"iris_p.rds\")\nread_rds(file = \"iris_p.rds\")\nrds_p <- read_rds(file = \"iris_p.rds\")\nrds_p + \n  geom_smooth(method = \"loess\",\n              formula = y ~ x,\n              se = FALSE)"},{"path":"data.html","id":"json","chapter":"3 Data","heading":"3.2.4 JSON","text":"increasingly common format sharing data JavaScript Object Notation JSON. format general, nothing like spreadsheet. Note JSON files often made available via internet. Several organizations provide JSON API web service can connect directly can obtain data.JSON files minimal readable format structures data, ’re commonly used transmit data server web application like website. people familiar Javascript coding language, ’ll likely see similarities JSON file format Javascript syntax. makes JSON files ideal internet transport, don’t see much use within project like RDS files .functions fromJSON() toJSON() allow convert R objects JSON. functions come jsonlite package.function toJSON() converts tibble JSON format. Consider example_1 tibble:function fromJSON() converts JSON format tibble.Make sure follow JSON format exactly ’re writing JSON files, format makes special allows work.","code":"\nlibrary(jsonlite)\nexample_1 <- tibble(name= c(\"Miguel\", \"Sofia\", \"Aya\", \"Cheng\"), \n                    student_id = 1:4, exam_1 = c(85, 94, 87, 90), \n                    exam_2 = c(86, 93, 88, 91))\n\nexample_1## # A tibble: 4 x 4\n##   name   student_id exam_1 exam_2\n##   <chr>       <int>  <dbl>  <dbl>\n## 1 Miguel          1     85     86\n## 2 Sofia           2     94     93\n## 3 Aya             3     87     88\n## 4 Cheng           4     90     91\n# The pretty argument adds indentation and whitespace when TRUE.\n\ntoJSON(example_1, pretty = TRUE) ## [\n##   {\n##     \"name\": \"Miguel\",\n##     \"student_id\": 1,\n##     \"exam_1\": 85,\n##     \"exam_2\": 86\n##   },\n##   {\n##     \"name\": \"Sofia\",\n##     \"student_id\": 2,\n##     \"exam_1\": 94,\n##     \"exam_2\": 93\n##   },\n##   {\n##     \"name\": \"Aya\",\n##     \"student_id\": 3,\n##     \"exam_1\": 87,\n##     \"exam_2\": 88\n##   },\n##   {\n##     \"name\": \"Cheng\",\n##     \"student_id\": 4,\n##     \"exam_1\": 90,\n##     \"exam_2\": 91\n##   }\n## ]\njson_format_ex <-\n'[\n  {\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"}, \n  {\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n  {},\n  {\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}\n]'\n\nfromJSON(json_format_ex) ##     Name Age Occupation\n## 1  Mario  32    Plumber\n## 2  Peach  21   Princess\n## 3   <NA>  NA       <NA>\n## 4 Bowser  NA      Koopa"},{"path":"data.html","id":"databases","chapter":"3 Data","heading":"3.3 Databases","text":"\nFIGURE 3.2: DROP TABLE particularly infamous SQL command\nDatabases one common methods storing data, capable storing large amounts data also allowing multiple people access change time. Think like giant book bunch tables hold data. useful , advent computers now able use relational databases.relational database database tables interact one another based common data, allowing create custom tables existing set records. example, relational database may hold multiple tables use ID keep track different information, like one table ID movie name another ID rating. can combine two code, creating table ID, name, rating. allows person made database easily put new data well allow pull data time without loading entire database.’s common use interact databases real world due businesses using relational databases keep track data update leisure. ’s uncommon databases hold thousands even millions rows due need keep track data.section, ’ll going pull data databases interact data without using entire database.","code":""},{"path":"data.html","id":"reading-data-from-a-sqlite-database","chapter":"3 Data","heading":"3.3.1 Reading data from a SQLite database","text":"SQLite probably simplest relational database one can use combination R. SQLite databases self-contained usually stored accessed locally one computer, instead internet cloud. Data usually stored file .db extension. Similar Excel files, plain text files read plain text editor.first thing need read data R database connect database. using dbConnect() function DBI (database interface) package. read data, simply tells R database opens communication channel.Often relational databases many tables, power comes useful ways can joined. Thus anytime want access data relational database, need know table names. can get names tables database using dbListTables().get one table name returned, tells us one table database. reference table database things like select columns filter rows, use tbl() function dbplyr package. package dbplyr allows us work data stored databases local data frames, useful can lot big datasets without actually bring vast amounts data computer!Although looks like just got data frame database, didn’t! ’s reference, showing us data still SQLite database (note first two lines output). databases often efficient selecting, filtering joining large data sets R. typically, database even stored computer, rather powerful machine somewhere web. R lazy waits bring data memory explicitly tell . , use collect() function.filter rows Aboriginal languages category according 2016 Canada Census, use collect() finally bring data R data frame.bother use collect() function? data looks pretty similar outputs shown . dbplyr provides lots functions similar filter() can use directly feed database reference (.e. tbl() gives ) downstream analysis functions (e.g., ggplot2 data visualization lm linear regression modeling). However, work every case; look happens try use nrow count rows data frame:tail preview last 6 rows data frame:functions work use version used collect() :order delete stop using SQLite server, need first disconnect file connection using dbDisconnect() passing connection object argument. can safely delete database file computer using file.remove().Additionally, operations work extract columns single values reference given tbl function. Thus, finished data wrangling tbl() database reference object, advisable bring local machine’s memory using collect() data frame.Warning: Usually, databases big! Reading object local machine may give error take lot time run careful plan !","code":"\nlibrary(DBI)\nlibrary(RSQLite)\n\n# This example uses a different github URL, so you can't use the paste0() trick\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/can_lang.db?raw=true\",\n              destfile = \"example_db.db\", mode = \"wb\")\n\ncon_lang_data <- dbConnect(RSQLite::SQLite(), \"example_db.db\")\ntables <- dbListTables(con_lang_data)\ntables## [1] \"lang\"\nlang_db <- tbl(con_lang_data, \"lang\")\nlang_db## # Source:   table<lang> [?? x 6]\n## # Database: sqlite 3.35.5\n## #   [/Users/davidkane/Desktop/projects/primer/example_db.db]\n##    category       language    mother_tongue most_at_home most_at_work lang_known\n##    <chr>          <chr>               <dbl>        <dbl>        <dbl>      <dbl>\n##  1 Aboriginal la… Aboriginal…           590          235           30        665\n##  2 Non-Official … Afrikaans           10260         4785           85      23415\n##  3 Non-Official … Afro-Asiat…          1150          445           10       2775\n##  4 Non-Official … Akan (Twi)          13460         5985           25      22150\n##  5 Non-Official … Albanian            26895        13135          345      31930\n##  6 Aboriginal la… Algonquian…            45           10            0        120\n##  7 Aboriginal la… Algonquin            1260          370           40       2480\n##  8 Non-Official … American S…          2685         3020         1145      21930\n##  9 Non-Official … Amharic             22465        12785          200      33670\n## 10 Non-Official … Arabic             419890       223535         5585     629055\n## # … with more rows\naboriginal_lang_db <- filter(lang_db, category == \"Aboriginal languages\")\naboriginal_lang_db## # Source:   lazy query [?? x 6]\n## # Database: sqlite 3.35.5\n## #   [/Users/davidkane/Desktop/projects/primer/example_db.db]\n##    category     language      mother_tongue most_at_home most_at_work lang_known\n##    <chr>        <chr>                 <dbl>        <dbl>        <dbl>      <dbl>\n##  1 Aboriginal … Aboriginal l…           590          235           30        665\n##  2 Aboriginal … Algonquian l…            45           10            0        120\n##  3 Aboriginal … Algonquin              1260          370           40       2480\n##  4 Aboriginal … Athabaskan l…            50           10            0         85\n##  5 Aboriginal … Atikamekw              6150         5465         1100       6645\n##  6 Aboriginal … Babine (Wets…           110           20           10        210\n##  7 Aboriginal … Beaver                  190           50            0        340\n##  8 Aboriginal … Blackfoot              2815         1110           85       5645\n##  9 Aboriginal … Carrier                1025          250           15       2100\n## 10 Aboriginal … Cayuga                   45           10           10        125\n## # … with more rows\naboriginal_lang_data <- collect(aboriginal_lang_db)\naboriginal_lang_data## # A tibble: 67 x 6\n##    category     language      mother_tongue most_at_home most_at_work lang_known\n##    <chr>        <chr>                 <dbl>        <dbl>        <dbl>      <dbl>\n##  1 Aboriginal … Aboriginal l…           590          235           30        665\n##  2 Aboriginal … Algonquian l…            45           10            0        120\n##  3 Aboriginal … Algonquin              1260          370           40       2480\n##  4 Aboriginal … Athabaskan l…            50           10            0         85\n##  5 Aboriginal … Atikamekw              6150         5465         1100       6645\n##  6 Aboriginal … Babine (Wets…           110           20           10        210\n##  7 Aboriginal … Beaver                  190           50            0        340\n##  8 Aboriginal … Blackfoot              2815         1110           85       5645\n##  9 Aboriginal … Carrier                1025          250           15       2100\n## 10 Aboriginal … Cayuga                   45           10           10        125\n## # … with 57 more rows\nnrow(aboriginal_lang_db)## [1] NAtail(aboriginal_lang_db)## Error: tail() is not supported by sql sources\nnrow(aboriginal_lang_data)## [1] 67\ntail(aboriginal_lang_data)## # A tibble: 6 x 6\n##   category     language       mother_tongue most_at_home most_at_work lang_known\n##   <chr>        <chr>                  <dbl>        <dbl>        <dbl>      <dbl>\n## 1 Aboriginal … Tahltan                   95            5            0        265\n## 2 Aboriginal … Thompson (Ntl…           335           20            0        450\n## 3 Aboriginal … Tlingit                   95            0           10        260\n## 4 Aboriginal … Tsimshian                200           30           10        410\n## 5 Aboriginal … Wakashan lang…            10            0            0         25\n## 6 Aboriginal … Woods Cree              1840          800           75       2665\ndbDisconnect(con_lang_data)\nfile.remove(\"example_db.db\")"},{"path":"data.html","id":"interacting-with-sqlite-databases","chapter":"3 Data","heading":"3.3.2 Interacting with SQLite databases","text":"Now ’ve figured get data database, let’s look wrangle data within database.databases normally contain large amounts data, ’s advisable wrangling use collect() transform database table tibble. stops pulling large amounts data onto computer just ignoring .let’s try pulling database see can manipulate .First , look data database holds.13 tables database. Let’s access first couple tables, just can get good look see ’re structured.can see, tables common column: ArtistId column. However, different information linked ID, “albums” albums artist produced “artists” name artist.full relationship diagram database ’re using.Let’s go operations can SQLite database.","code":"\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Again, use mode = \"wb\" if you're using a Windows operating system.\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/chinook.db?raw=true\",\n             dest = \"chinook.db\", mode = \"wb\")\n\ncon <- dbConnect(RSQLite::SQLite(), \"chinook.db\")\ndbListTables(con)##  [1] \"albums\"          \"artists\"         \"customers\"       \"employees\"      \n##  [5] \"genres\"          \"invoice_items\"   \"invoices\"        \"media_types\"    \n##  [9] \"playlist_track\"  \"playlists\"       \"sqlite_sequence\" \"sqlite_stat1\"   \n## [13] \"tracks\"\nalbums <- tbl(con, \"albums\")\nalbums## # Source:   table<albums> [?? x 3]\n## # Database: sqlite 3.35.5 [/Users/davidkane/Desktop/projects/primer/chinook.db]\n##    AlbumId Title                                 ArtistId\n##      <int> <chr>                                    <int>\n##  1       1 For Those About To Rock We Salute You        1\n##  2       2 Balls to the Wall                            2\n##  3       3 Restless and Wild                            2\n##  4       4 Let There Be Rock                            1\n##  5       5 Big Ones                                     3\n##  6       6 Jagged Little Pill                           4\n##  7       7 Facelift                                     5\n##  8       8 Warner 25 Anos                               6\n##  9       9 Plays Metallica By Four Cellos               7\n## 10      10 Audioslave                                   8\n## # … with more rows\nartists <- tbl(con, \"artists\")\nartists## # Source:   table<artists> [?? x 2]\n## # Database: sqlite 3.35.5 [/Users/davidkane/Desktop/projects/primer/chinook.db]\n##    ArtistId Name                \n##       <int> <chr>               \n##  1        1 AC/DC               \n##  2        2 Accept              \n##  3        3 Aerosmith           \n##  4        4 Alanis Morissette   \n##  5        5 Alice In Chains     \n##  6        6 Antônio Carlos Jobim\n##  7        7 Apocalyptica        \n##  8        8 Audioslave          \n##  9        9 BackBeat            \n## 10       10 Billy Cobham        \n## # … with more rows"},{"path":"data.html","id":"using-dbplyr","chapter":"3 Data","heading":"3.3.2.1 Using dbplyr","text":"’ve already seen can use dbpylr package get table database, let’s look can also use operations databases well.dbpylr package allows use many functions tidyverse like select(), filter(), mutate() without issues, making easiest operations .’s example using dbpylr package get number albums artist created.functions ’re already familiar using, work just well database tables well.However, cases use SQL code accomplish certain tasks.","code":"\n# You can keep a column during the summarize() if you just put the name equal to\n# itself. This code is deliberately long in order to show the common functions\n# that we use.\n\nband_albums <- tbl(con, \"albums\") %>%\n                 inner_join(tbl(con, \"artists\"), by = \"ArtistId\") %>%\n                 select(\"AlbumId\", \"ArtistId\", \"Title\", \"Name\") %>%\n                 group_by(ArtistId) %>%\n                 summarize(Name = Name, num_albums = n()) %>%\n                 mutate(artist_name = Name) %>%\n                 mutate(artist_id = ArtistId) %>%\n                 filter(num_albums > 3) %>%\n                 arrange(desc(num_albums)) %>%\n                 select(artist_id, artist_name, num_albums)\nband_albums## # Source:     lazy query [?? x 3]\n## # Database:   sqlite 3.35.5\n## #   [/Users/davidkane/Desktop/projects/primer/chinook.db]\n## # Ordered by: desc(num_albums)\n##    artist_id artist_name     num_albums\n##        <int> <chr>                <int>\n##  1        90 Iron Maiden             21\n##  2        22 Led Zeppelin            14\n##  3        58 Deep Purple             11\n##  4        50 Metallica               10\n##  5       150 U2                      10\n##  6       114 Ozzy Osbourne            6\n##  7       118 Pearl Jam                5\n##  8        21 Various Artists          4\n##  9        82 Faith No More            4\n## 10        84 Foo Fighters             4\n## # … with more rows"},{"path":"data.html","id":"using-sql-code","chapter":"3 Data","heading":"3.3.2.2 Using SQL code","text":"First , let’s try understand SQLite SQL really .SQL, Structured Query Language, coding language works build relational databases ’ve using far. SQLite special type database can download onto computer uses SQL basic structure basic language. Essentially, SQLite uses SQL code things .shouldn’t need write SQL code primer, ’s common read SQL database order access data can graph .SQL runs something known query, line code pulls information database. effectively asking database specific set data. ’s known SELECT command, “selects” data database brings computer use.TABLE 3.1: Displaying records 1 - 10You can modify queries operations like joins.TABLE 3.2: Displaying records 1 - 10If ’re interested SQL statements mean write SQL code, visit tutorial. ’re interested create \ndatabase SQL, visit .fact, dbplyr commands actually use SQL code create effect.\ncan run show_query() see SQL code, query dbplyr produces.However, can’t everything SQL dbplyr package. example, can’t use slice() command.possible SQL database. couple ways run SQL code R, easiest way using dbGetQuery() function DBI library.method allows use R code within project, making easier.can also use SQL code directly project changing header Rmarkdown file. code Rmarkdown file, create chunks look like :can code SQL changing chunk look like :allows write SQL code genuine SQL syntax save output variable.can use variable R code.Coding SQL allows things dbplyr package doesn’t, using certain rows. add LIMIT 10; end query (first SELECT statement), can limit query first 10 rows.SQL versatile dbplyr, ’s also lot confusing ’s straightforward. creates one golden rule:Use dbplyr functions data analysis SQL code accessing dataThis SQL code weird hard understand, allows easily pull data. SQL coding language designed query ask databases data. Meanwhile, dbplyr meant analyzing data modifying .Now, worked SQLite database within tutorial due simplicity. However, databases can accessed different ways. example, PostgresSQL server located internet allows pull data internet instead downloading computer. can learn types SQL databases differences .bother databases ?Opening database stored .db file involved lot effort just opening .csv, .tsv, plain text Excel formats. bit pain use database setting since use dbplyr translate tidyverse-like commands (filter, select, head, etc.) SQL commands database understands. tidyverse commands can currently translated SQLite databases, like showed slice(). might wondering, use databases ?Databases beneficial large-scale setting:enable storing large data sets across multiple computers automatic redundancy backupsthey allow multiple users access simultaneously remotely without conflicts errorsthey provide mechanisms ensuring data integrity validating inputthey provide security keep data safe\nexample, billions Google searches conducted daily.makes databases extremely useful business ’ll employees thousands millions operations daily. makes knowing SQL databases extremely vital data science general usage perspective.","code":"-- This code is written in SQL syntax, so it won't work if you just put it into R. \n-- We'll be going over how to write SQL in R later in this section.\n\nSELECT AlbumId,\n       Title,\n       ArtistId\nFROM albums;-- This does the same thing as the inner join from Chapter 2 Wrangling\n-- Here we're joining the albums and the artists table by the ArtistId column\n\nSELECT artistid,\n       NAME,\n       title\nFROM\n  (SELECT title,\n          albums.artistid AS \"ArtistId\",\n          \"name\"\n   FROM albums\n   INNER JOIN artists ON albums.artistid = artists.artistid);\nshow_query(band_albums)## <SQL>\n## SELECT `artist_id`, `artist_name`, `num_albums`\n## FROM (SELECT `ArtistId`, `Name`, `num_albums`, `artist_name`, `ArtistId` AS `artist_id`\n## FROM (SELECT `ArtistId`, `Name`, `num_albums`, `Name` AS `artist_name`\n## FROM (SELECT `ArtistId`, `Name`, COUNT(*) AS `num_albums`\n## FROM (SELECT `AlbumId`, `ArtistId`, `Title`, `Name`\n## FROM (SELECT `AlbumId`, `Title`, `LHS`.`ArtistId` AS `ArtistId`, `Name`\n## FROM `albums` AS `LHS`\n## INNER JOIN `artists` AS `RHS`\n## ON (`LHS`.`ArtistId` = `RHS`.`ArtistId`)\n## ))\n## GROUP BY `ArtistId`)))\n## WHERE (`num_albums` > 3.0)\n## ORDER BY `num_albums` DESCslice(band_albums, 1:10)## Error: slice() is not supported on database backends\n# This code is just copy-pasted and reformatted from the earlier dbplyr code so\n# you can best understand it.\n\ndbGetQuery(con, \"\nSELECT `artist_id`,\n       `artist_name`,\n       `num_albums`\nFROM\n  (SELECT `artistid`,\n          `name`,\n          `num_albums`,\n          `artist_name`,\n          `artistid` AS `artist_id`\n   FROM\n     (SELECT `artistid`,\n             `name`,\n             `num_albums`,\n             `name` AS `artist_name`\n      FROM\n        (SELECT `artistid`,\n                `name`,\n                Count(*) AS `num_albums`\n         FROM\n           (SELECT `albumid`,\n                   `artistid`,\n                   `title`,\n                   `name`\n            FROM\n              (SELECT `albumid`,\n                      `title`,\n                      `LHS`.`artistid` AS `ArtistId`,\n                      `name`\n               FROM `albums` AS `LHS`\n               INNER JOIN `artists` AS `RHS` ON (`LHS`.`artistid` = `RHS`.`artistid`))) GROUP  BY `artistid`)))\nWHERE (`num_albums` > 3.0)\n  ORDER  BY `num_albums` DESC\n\")##    artist_id     artist_name num_albums\n## 1         90     Iron Maiden         21\n## 2         22    Led Zeppelin         14\n## 3         58     Deep Purple         11\n## 4         50       Metallica         10\n## 5        150              U2         10\n## 6        114   Ozzy Osbourne          6\n## 7        118       Pearl Jam          5\n## 8         21 Various Artists          4\n## 9         82   Faith No More          4\n## 10        84    Foo Fighters          4\n## 11       149            Lost          4\n## 12       152       Van Halen          4## ```{r}\n##\n## ```## ```{sql, connection = con, output.var = \"your_variable_name\"}\n## \n## ```## ```{sql, connection = con, output.var = \"your_variable_name\"}\n## SELECT * FROM albums\n## ```\nhead(your_variable_name)##   AlbumId                                 Title ArtistId\n## 1       1 For Those About To Rock We Salute You        1\n## 2       2                     Balls to the Wall        2\n## 3       3                     Restless and Wild        2\n## 4       4                     Let There Be Rock        1\n## 5       5                              Big Ones        3\n## 6       6                    Jagged Little Pill        4-- We're writing this code using the sql code chunk method. \n-- This is because it allows you to see what's highlighted and what's not.\n-- SQL code is also inconvenient because you can't just add a new line,\n-- Instead, you need to treat all of these lines of code as a single statement\n-- Then add in the command where you think that it would fit.\nSELECT `artist_id`,\n       `artist_name`,\n       `num_albums`\nFROM\n  (SELECT `artistid`,\n          `name`,\n          `num_albums`,\n          `artist_name`,\n          `artistid` AS `artist_id`\n   FROM\n     (SELECT `artistid`,\n             `name`,\n             `num_albums`,\n             `name` AS `artist_name`\n      FROM\n        (SELECT `artistid`,\n                `name`,\n                Count(*) AS `num_albums`\n         FROM\n           (SELECT `albumid`,\n                   `artistid`,\n                   `title`,\n                   `name`\n            FROM\n              (SELECT `albumid`,\n                      `title`,\n                      `LHS`.`artistid` AS `ArtistId`,\n                      `name`\n               FROM `albums` AS `LHS`\n               INNER JOIN `artists` AS `RHS` ON (`LHS`.`artistid` = `RHS`.`artistid`))) GROUP  BY `artistid`)))\nWHERE (`num_albums` > 3.0)\n  ORDER  BY `num_albums` DESC\nLIMIT 10;\n# You can't see this, but output.var = \"band_albums_sql\" in the chunk above\n# this. So we're outputting our data into band_albums_sql\nband_albums_sql##    artist_id     artist_name num_albums\n## 1         90     Iron Maiden         21\n## 2         22    Led Zeppelin         14\n## 3         58     Deep Purple         11\n## 4         50       Metallica         10\n## 5        150              U2         10\n## 6        114   Ozzy Osbourne          6\n## 7        118       Pearl Jam          5\n## 8         21 Various Artists          4\n## 9         82   Faith No More          4\n## 10        84    Foo Fighters          4"},{"path":"data.html","id":"webscraping","chapter":"3 Data","heading":"3.4 Webscraping","text":"first part chapter, learned read data plain text files usually “rectangular” shape using tidyverse read_* functions. Sadly, data comes simple format, can happily use many tools read messy/wild data formats. formal name gathering non-rectangular data web transforming useful format data analysis web scraping.can web scraping r using rvest library, library allows us look HTML CSS selectors, pick apart get data want. Let’s look means.","code":""},{"path":"data.html","id":"html-and-css-selectors","chapter":"3 Data","heading":"3.4.1 HTML and CSS selectors","text":"jump scraping, let’s learn little bit “source code” website looks like.","code":""},{"path":"data.html","id":"website-structure","chapter":"3 Data","heading":"3.4.1.1 Website structure","text":"\nFIGURE 3.3: little bit HTML editing can bring long way.\nWebsites coded language called HTML, HyperText Markup Language. code puts information website, numbers text. files created “knit” .Rmd files ’s makes put information screen. ’re webscraping, can look information website looking HTML code. can try ! Just go website right-click, press “Inspect.” ’ll able see HTML code behind website even edit liking!\nFIGURE 3.4: can see HTML code website even edit temporarily inspecting HTML code \nHTML uses many nested “tags” organize structure. example, HTML file like :looks like result:\nHello World\ntag defines element, part website. example, used <p> tag define paragraph element HTML. close paragraph using <\/p> tag, ending paragraph. can find full list HTML tags elements .Now, ’re data science. let’s say find really cool website internet lot really useful data, ’s download file (’s legal scrape website). means pull information HTML code.can using rvest package like talked earlier. essentially package allows scrape information HTML code, allowing scrape information complete website.Let’s try scraping Hello World earlier HTML code.First , need use minimal_html() function can get R object can work . temporary use ’re trying Think like changing information HTML code something R can understand.need filter elements HTML file contains. case, ’re looking paragraph tag, <p>. means can just use html_element() get first paragraph element.returns HTML node, specific element chose. paragraphs HTML code, return HTML nodes. Now, interesting thing nodes contain information can access using html_text2(). lets us parse code without problems.websites, especially ones useful data, don’t just exist state “Hello World.” ’re much complex . layering different elements, can create website contains lot information.example, HTML:Creates table paragraph looks like :\nuseless, shouldn’t even reading .\nNow, let’s say want get information table don’t want get useless description end.can looking table (<table> tag) getting cells (<td> tag). ’ll also save variable later.Notice actually outputs list elements text. means can use output like list order get information want. Just use [[]] syntax.method ’d use just wanted get information inside specific table without filtering data . pulling directly HTML makes lot sense scenario fiddling around junk. Just pull data go, fancy things.However, want important information table? ’s CSS selectors .","code":"\nknitr::include_graphics(\"03-data/images/html_inspect.gif\")<!-- This is using the HTML language. You don't actually put this in a chunk if \nyou want to put this in an Rmarkdown file, you can just type it straight in.-->\n<html>\n  <p>\n    Hello World\n  <\/p>\n<\/html>\nlibrary(rvest)\nraw_html_1 <- \"<html>\n                 <p>\n                   Hello World\n                 <\/p>\n               <\/html>\"\nraw_html_1 %>%\n  minimal_html()## {html_document}\n## <html>\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body>\\n<p>\\n                   Hello World\\n                 <\/p>\\n      ...\n# Only use the letter part of the tag, not the <, >, or /\n\nraw_html_1 %>%\n  minimal_html() %>%\n  html_element(\"p\")## {html_node}\n## <p>\n# html_text2() is different from the normal html_text() because it returns an\n# actual string. This is normally what we want.\n\nraw_html_1 %>%\n  minimal_html() %>%\n  html_element(\"p\") %>%\n  html_text2()## [1] \"Hello World\"<html>\n  <table>\n    <tr>\n      <td>\n        This is some important info.\n      <\/td>\n      <td>\n        This is some unimportant info.\n      <\/td>\n    <\/tr>\n    <tr>\n      <td>\n        This is really important info.\n      <\/td>\n      <td>\n        This is some other unimportant information.\n      <\/td>\n    <\/tr>\n  <\/table>\n  <p>\n    This is so useless, you shouldn't even be reading it.\n  <\/p>\n<\/html>\nraw_html_2 <- \"<html>\n                 <table>\n                   <tr>\n                     <td>\n                       This is some important info.\n                     <\/td>\n                     <td>\n                       This is some unimportant info.\n                     <\/td>\n                   <\/tr>\n                   <tr>\n                     <td>\n                       This is really important info.\n                     <\/td>\n                     <td>\n                       This is some other unimportant information.\n                     <\/td>\n                   <\/tr>\n                 <\/table>\n                 <p>\n                   This is so useless, you shouldn't even be reading it.\n                 <\/p>\n               <\/html>\"\nraw_html_2## [1] \"<html>\\n                 <table>\\n                   <tr>\\n                     <td>\\n                       This is some important info.\\n                     <\/td>\\n                     <td>\\n                       This is some unimportant info.\\n                     <\/td>\\n                   <\/tr>\\n                   <tr>\\n                     <td>\\n                       This is really important info.\\n                     <\/td>\\n                     <td>\\n                       This is some other unimportant information.\\n                     <\/td>\\n                   <\/tr>\\n                 <\/table>\\n                 <p>\\n                   This is so useless, you shouldn't even be reading it.\\n                 <\/p>\\n               <\/html>\"\n# We use html_elements() to get all of the elements in the HTML file.\n\ntd_tags <- raw_html_2 %>%\n             minimal_html() %>%\n             html_element(\"table\") %>%\n             html_elements(\"td\")\ntd_tags## {xml_nodeset (4)}\n## [1] <td>\\n                       This is some important info.\\n               ...\n## [2] <td>\\n                       This is some unimportant info.\\n             ...\n## [3] <td>\\n                       This is really important info.\\n             ...\n## [4] <td>\\n                       This is some other unimportant information.\\ ...\ntd_tags[[2]] %>%\n  html_text2()## [1] \"This is some unimportant info.\""},{"path":"data.html","id":"css-selectors","chapter":"3 Data","heading":"3.4.1.2 CSS selectors","text":"CSS, Cascading Style Sheets, coding language defines style webpages. may noticed earlier HTML code output earlier just black white. websites aren’t black white, colors cool things. using CSS add special rules specific elements.Now, number ways . first one using class tell webpage want elements style. CSS classes way multiple elements style, rather limited unique ID. can see example , set elements class good-info green class amazing-info pale red background.can use class HTML code.\nuseless, shouldn’t even reading .\nNow, ’s really clever trick can use . Let’s say want table cells important, don’t know exactly (know class). Well cases, data want different color, like see data . Think like making important information italic font make stand . CSS makes information italic assigning specific class, can just look everything class pull get important information. can using html_elements(). Just plug class element want . front tell R ’s class.can sort multiple classes just chaining together.Webpages also use something called ID change color specific element. thing time “#” beginning signify ’s symbol.\nred!\n\nred.\nfar, ’ve covered 3 ways find parts website ’s HTML code: element (<p>), ID (#red-id), class (.red-class). However, can also mix--match tags. Take following HTML element:can try accessing normal methods, like :can mix match make even narrow search chaining together.can also get link node refers rather text. case, link stored href attribute, use html_attr() access .accesses href attribute element without accessing link, allowing find links outside websites.chains known CSS Selectors ’re important part data science allow us find data information website without download every file copying everything . Instead, can just use website’s inbuilt code get information.","code":"/*This uses the `css` language instead of the R language. You can use this\n  code in an Rmarkdown file by substituting the \"r\" for \"css\" in the top \n  brackets*/\n  \n.good-info {\n  color: green\n}\n  \n.amazing-info {\n  background-color: lightpink\n}<html>\n  <table>\n    <tr>\n      <td class = 'good-info'>\n        This is some important info.\n      <\/td>\n      <td>\n        This is some unimportant info.\n      <\/td>\n    <\/tr>\n    <tr>\n      <td class = 'good-info amazing-info'>\n        This is really important info.\n      <\/td>\n      <td>\n        This is some other unimportant information.\n      <\/td>\n    <\/tr>\n  <\/table>\n  <p>\n    This is so useless, you shouldn't even be reading it.\n  <\/p>\n<\/html>\nraw_html_3 <- \"<html>\n                 <table>\n                   <tr>\n                     <td class = 'good-info'>\n                       This is some important info.\n                     <\/td>\n                     <td>\n                       This is some unimportant info.\n                     <\/td>\n                   <\/tr>\n                   <tr>\n                     <td class = 'good-info amazing-info'>\n                       This is really important info.\n                     <\/td>\n                     <td>\n                       This is some other unimportant information.\n                     <\/td>\n                   <\/tr>\n                 <\/table>\n                 <p>\n                   This is so useless, you shouldn't even be reading it.\n                 <\/p>\n               <\/html>\"\n\nraw_html_3 %>%\n  minimal_html() %>%\n  html_elements(\".good-info\") %>%\n  html_text2()## [1] \"This is some important info.\"   \"This is really important info.\"\nraw_html_3 %>%\n  minimal_html() %>%\n  html_elements(\".good-info.amazing-info\") %>%\n  html_text2()## [1] \"This is really important info.\".red-id {\n  color: red\n}<html>\n<p id = \"red-id\"> This is red! <\/p>\n<p> This is not red. <\/p>\n<\/html><html>\n<a href=\"https://ppbds.github.io/primer/index.html\" id=\"abcd1234\" class=\"hello hi\">\nlink to primer\n<\/a>\n<\/html>\nraw_html_5 <- '<a href = \"https://ppbds.github.io/primer/index.htm\" id=\"abcd1234\" class=\"hello hi\">link to primer<\/a>'\n\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\"a\") %>%\n  html_text2()## [1] \"link to primer\"\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\"#abcd1234\") %>%\n  html_text2()## [1] \"link to primer\"\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\".hello\") %>%\n  html_text2()## [1] \"link to primer\"\n# Keep in mind that IDs (the # part) are usually unique to that element only, so\n# there's not really a point in filtering it even more when it's already unique.\n\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\"a#abcd1234\") %>%\n  html_text2()## [1] \"link to primer\"\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\"a.hello\") %>%\n  html_text2()## [1] \"link to primer\"\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\"a.hello#abcd1234\") %>%\n  html_text2()## [1] \"link to primer\"\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\".hello#abcd1234\") %>%\n  html_text2()## [1] \"link to primer\"\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\"a.hello.hi#abcd1234\") %>%\n  html_text2()## [1] \"link to primer\"\nraw_html_5 %>%\n  minimal_html() %>%\n  html_element(\"a.hello.hi#abcd1234\") %>%\n  html_attr(\"href\")## [1] \"https://ppbds.github.io/primer/index.htm\""},{"path":"data.html","id":"application","chapter":"3 Data","heading":"3.4.2 Application","text":"Now let’s look real-world context. Say interested knowing average rental price (per square footage) recently available one-bedroom apartments Vancouver according . visit Vancouver Craigslist website search one-bedroom apartments, shown:page, ’s pretty easy us find apartment price square footage (Craigslist utterly incomprehensible otherwise). computer can’t deal human eyes computer eyes different. can’t understand anything screen. instead looking screen, can just parse HTML find correct information. ’s part HTML code Vancouver’s Craigslist:genuinely sucks read. ’s links, random numbers, elements 4 classes time, stupidly long point eyes fall . can pick CSS selectors need, (“result-price” “housing”), hassle pain ’s better solution.using SelectorGadget tool order find CSS selectors. ’s open source tool simplifies generating finding CSS selectors. recommend use Chrome web browser use tool, install selector gadget tool Chrome Web Store. short video install use SelectorGadget tool get CSS selector use web scraping:installing using selectorgadget shown video , get two CSS selectors .housing .result-price can use scrape information square footage rental price, respectively. selector gadget returns us comma separated list (.housing , .result-price), exactly format need provide R.However, ’re scraping actual websites need take just size code consideration. also need take legal aspects account,","code":"        <span class=\"result-meta\">\n                <span class=\"result-price\">$800<\/span>\n\n                <span class=\"housing\">\n                    1br -\n                <\/span>\n\n                <span class=\"result-hood\"> (13768 108th Avenue)<\/span>\n\n                <span class=\"result-tags\">\n                    <span class=\"maptag\" data-pid=\"6786042973\">map<\/span>\n                <\/span>\n\n                <span class=\"banish icon icon-trash\" role=\"button\">\n                    <span class=\"screen-reader-text\">hide this posting<\/span>\n                <\/span>\n\n            <span class=\"unbanish icon icon-trash red\" role=\"button\" aria-hidden=\"true\"><\/span>\n            <a href=\"#\" class=\"restore-link\">\n                <span class=\"restore-narrow-text\">restore<\/span>\n                <span class=\"restore-wide-text\">restore this posting<\/span>\n            <\/a>\n\n        <\/span>\n    <\/p>\n<\/li>\n         <li class=\"result-row\" data-pid=\"6788463837\">\n\n        <a href=\"https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html\" class=\"result-image gallery\" data-ids=\"1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV\">\n                <span class=\"result-price\">$2285<\/span>\n        <\/a>\n\n    <p class=\"result-info\">\n        <span class=\"icon icon-star\" role=\"button\">\n            <span class=\"screen-reader-text\">favorite this post<\/span>\n        <\/span>\n\n            <time class=\"result-date\" datetime=\"2019-01-06 12:06\" title=\"Sun 06 Jan 12:06:01 PM\">Jan  6<\/time>\n\n\n        <a href=\"https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html\" data-id=\"6788463837\" class=\"result-title hdrlnk\">Luxury 1 Bedroom CentreView with View - Lonsdale<\/a>\n\n"},{"path":"data.html","id":"legal-to-scrape","chapter":"3 Data","heading":"3.4.2.1 Are you allowed to scrape that website?","text":"scraping data web, always check whether ALLOWED scrape ! two documents important : robots.txt file (found adding /robots.txt end URL like ) reading website’s Terms Service document. website’s Terms Service document far away important two ’s actually legally binding, look first. happens look Craigslist’s Terms Service document? Well read :“agree copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, automated manual equivalent (e.g., hand).”source: https://www.craigslist.org//terms..useWant learn legalities web scraping crawling? Read interesting blog post titled “Web Scraping Crawling Perfectly Legal, Right?” Benoit Bernard (optional, required reading).now? Well, can’t scrape Craigslist, find something else allows scrape. Let’s use database Open Secrets foreign-connected PACs within country.","code":""},{"path":"data.html","id":"scraping-from-actual-websites","chapter":"3 Data","heading":"3.4.2.2 Scraping from actual websites","text":"Now, earlier scraped pre-written HTML code, can websites well. can just plugging URL website using read_html() instead minimal_html().case, can use Selector Gadget find correct CSS selector “table.DataTable-Partial.” just proceed normal, need use html_table() ’re trying bring table use tibble.allows us scrape tibbles websites. can use tibbles create new graphs.Let’s try using Wikipedia page gun violence United States.First, need save URL read HTML code R.can use SelectorGadget find correct CSS selector. can’t easily click table, can also find selector reading HTML code . can highlighting table, right clicking, pressing “Inspect.” ’ll dig HTML order find correct selector, perfectly viable way find selectors.can save table provided variable known raw_data_wiki.Past point, can work data just like normal data. ’s example graph states denser population experience consistent gun deaths states less dense population.Webscraping powerful tool gain data directly internet, make sure ’re following proper protocols don’t break law actually get information need.","code":"\nweb_url <- \"https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020\"\n\nweb_url %>%\n  read_html()## {html_document}\n## <html class=\"no-js\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body>\\n\\n    <!-- Google Adsense Script -->\\n    <script async src=\"//pa ...\nweb_url %>%\n  read_html() %>%\n  html_element(\"table.DataTable-Partial\") %>%\n  html_table()## # A tibble: 225 x 5\n##    `PAC Name (Affiliate)`     `Country of Origin/Parent Co… Total  Dems   Repubs\n##    <chr>                      <chr>                         <chr>  <chr>  <chr> \n##  1 7-Eleven                   Japan/Seven & I Holdings      $20,0… $1,000 $19,0…\n##  2 ABB Group (ABB Group)      Switzerland/Asea Brown Boveri $16,9… $6,800 $10,1…\n##  3 Accenture (Accenture)      Ireland/Accenture plc         $83,5… $50,5… $33,0…\n##  4 Air Liquide America        France/L'Air Liquide SA       $37,8… $15,8… $22,0…\n##  5 Airbus Group               Netherlands/Airbus Group      $182,… $79,0… $103,…\n##  6 Alkermes Inc               Ireland/Alkermes Plc          $94,7… $30,7… $64,0…\n##  7 Allianz of America (Allia… Germany/Allianz AG Holding    $71,3… $36,1… $35,1…\n##  8 AMG Vanadium               Netherlands/AMG Advanced Met… $2,000 $0     $2,000\n##  9 Anheuser-Busch (Anheuser-… Belgium/Anheuser-Busch InBev  $336,… $174,… $162,…\n## 10 AON Corp (AON plc)         UK/AON PLC                    $80,5… $44,0… $36,5…\n## # … with 215 more rows\nwiki_url <- \"https://en.wikipedia.org/w/index.php?title=%22,%22Gun_violence_in_the_United_States_by_state%22,%22&direction=prev&oldid=810166167\"\n\nwiki_url %>%\n  read_html()## {html_document}\n## <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\nraw_data_wiki <- wiki_url %>%\n                   read_html() %>%\n                   html_element(\"table.wikitable.sortable\") %>%\n                   html_table()\nraw_data_wiki## # A tibble: 51 x 4\n##    State    `Population (total … `Murders and Nonnegli… `Murder and Nonnegligen…\n##    <chr>    <chr>                <chr>                                     <dbl>\n##  1 Alabama  4,853,875            348                                         7.2\n##  2 Alaska   737,709              59                                          8  \n##  3 Arizona  6,817,565            309                                         4.5\n##  4 Arkansas 2,977,853            181                                         6.1\n##  5 Califor… 38,993,940           1,861                                       4.8\n##  6 Colorado 5,448,819            176                                         3.2\n##  7 Connect… 3,584,730            117                                         3.3\n##  8 Delaware 944,076              63                                          6.7\n##  9 Distric… 670,377              162                                        24.2\n## 10 Florida  20,244,914           1,041                                       5.1\n## # … with 41 more rows\nclean_data <- raw_data_wiki %>%\n                rename(\"population\" = \"Population (total inhabitants) (2015) [1]\",\n                       \"death_rate\" = \"Murder and Nonnegligent\\nManslaughter Rate(per 100,000 inhabitants) (2015)\",\n                       \"total_deaths\" = \"Murders and Nonnegligent\\nManslaughter(total deaths) (2015) [2]\") %>%\n                        select(population, death_rate) %>%\n                        mutate(population = parse_number(population)) %>%\n                        mutate(pop_tile = ntile(population, 20)) %>%\n                        group_by(pop_tile) %>%\n                        summarize(num_states = n(), sum_rate = sum(death_rate)) %>%\n                        mutate(avg_rate = sum_rate/num_states) %>%\n                        mutate(percent_rate = avg_rate / sum(avg_rate))\n\nggplot(clean_data, aes(x = pop_tile, y = percent_rate)) + \n  geom_col() +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", se = FALSE) +\n  scale_x_continuous(breaks = 1:20) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  theme_classic() +\n  labs(title = \"Distribution of US Death Rate by Guns in Quantile of Population in 2015\",\n       subtitle = \"Death rate of less populated states fluctuate more than states with a denser population\",\n       x = \"Quantile by Population (least to most)\",\n       y = \"Percent of Average Death Rate\",\n       caption = \"Wikipedia: Gun Violence in the United States (2017)\")"},{"path":"data.html","id":"working-with-apis","chapter":"3 Data","heading":"3.5 Working with APIs","text":"“API” stands Application Program Interface. allow us access open data government agencies, companies, organizations. API provides rules software applications interact one another. Open data APIs provide rules need know write R code request pull data organization’s web server R. Usually, computational burden querying subsetting data taken source’s server, create subset requested data pass computer. practice, means can often pull subset data want large available dataset without download full dataset load locally R session.overview, basic steps accessing using data web API working R :Figure API rules HTTP requestsWrite R code create request proper formatSend request using GET POST HTTP methodsOnce get back data request, parse easier--use format necessaryTo get data API, first read organization’s API documentation. organization post details data available API(s), well set HTTP requests get data. request data API, typically need send organization’s web server HTTP request using GET POST method. API documentation details typically show example GET POST request API, including base URL use possible query parameters can used customize dataset request.example:National Aeronautics Space Administration (NASA) API pulling Astronomy Picture Day. API documentation, specify base URL API request https://api.nasa.gov/planetary/apod can include parameters specify date daily picture want, whether pull high-resolution version picture, NOAA API key requested NOAA.Many organizations require get API key use key API requests. key allows organization control API access, including enforcing rate limits per user. API rate limits restrict often can request data (hourly limit 1,000 requests per user NASA APIs).API keys kept private, writing code includes API key, careful include actual key code public (even code public GitHub repositories). ensure privacy, save value key file named .Renviron home directory. file plain text file must end blank line. ’ve saved API key global variable file (e.g., line added .Renviron file like NOAA_API_KEY = “abdafjsiopnab038”), can assign key value R object R session using Sys.getenv function (e.g., noaa_api_key <- Sys.getenv(“NOAA_API_KEY”)), use object noaa_api_key anywhere otherwise used character string API key.find R packages accessing exploring open data, check Open Data CRAN task view. can also browse ROpenSci packages, GitHub repositories can explore package works! ROpenSci organization mission create open software tools science. create package access data relevant scientific research API, consider submitting peer-review ROpenSci.riem package, developed Maelle Salmon ROpenSci package, excellent straightforward example can use R pull open data web API. package allows pull weather data airports around world directly Iowa Environmental Mesonet. show pull data R API, section walk code riem package code based closely code package.get certain set weather data Iowa Environmental Mesonet, can send HTTP request specifying base URL, https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/, well parameters describing subset dataset want (e.g., date ranges, weather variables, output format). know rules names possible values parameters (), can submit HTTP GET request using functionGET() httr package.making HTTP request using GET() POST() functions httr package, can include key-value pairs query parameters list object query argument function. example, suppose want get wind speed miles per hour (data = “sped”) Denver, CO, (station = “DEN”) month June 2016 (year1 = “2016,” month1 = “6,” etc.) Denver’s local time zone (tz = “America/Denver”) comma-separated file (format = “comma”). get weather dataset, can run:content() call extracts content response HTTP request sent GET() function. Iowa Environmental Mesonet API offers option return requested data comma-separated file (format = “comma” GET request), content read_csv() used extract read csv file. Usually, data returned JSON format instead.tricky part process figuring available parameter names (e.g., station) possible values (e.g., “DEN” Denver). Currently, details can send HTTP request Iowa Environmental Mesonet’s API include:four-character weather station identifier (station)weather variables (e.g., temperature, wind speed) include (data)Starting ending dates describing range ’d like pull data (year1, month1, day1, year2, month2, day2)time zone use date-times weather observations (tz)Different formatting options (e.g., delimiter use resulting data file [format], whether include longitude latitude)Typically, parameter names possible values explained API documentation. cases, however, documentation limited. case, may able figure possible values, especially API specifies GET rather POST method, playing around website’s point--click interface looking url resulting data pages. example, look Iowa Environmental Mesonet’s page accessing data, ’ll notice point--click web interface allows options list , click access dataset using interface, web address data page includes parameter names values.riem package implements ideas three clean straightforward functions. can explore code behind package see ideas can incorporated small R package, /R directory package’s GitHub page.R packages already exist many open data APIs. R package already exists API, can use functions package directly, rather writing code using API protocols httr functions. examples existing R packages interact open data APIs include:twitteR: Twitterrnoaa: National Oceanic Atmospheric AdministrationQuandl: Quandl (financial data)RGoogleAnalytics: Google Analyticscensusr, acs: United States CensusWDI, wbstats: World BankGuardianR, rdian: Guardian Media GroupblsAPI: Bureau Labor Statisticsrtimes: New York TimesdataRetrieval, waterData: United States Geological Survey\nR package doesn’t exist open API ’d like write package, find writing API packages vignette httr package. document includes advice error handling within R code accesses data open API.Information section API’s taken Mastering Software Development R textbook, authored Roger D. Peng, Sean Kross, Brooke Anderson.","code":"\nlibrary(httr)\nlibrary(httr)\nmeso_url <- \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/\"\ndenver <- GET(url = meso_url,\n                    query = list(station = \"DEN\",\n                                 data = \"sped\",\n                                 year1 = \"2016\",\n                                 month1 = \"6\",\n                                 day1 = \"1\",\n                                 year2 = \"2016\",\n                                 month2 = \"6\",\n                                 day2 = \"30\",\n                                 tz = \"America/Denver\",\n                                 format = \"comma\")) %>%\n  content() %>% \n  read_csv(skip = 5, na = \"M\")\n\n# There are 9,106 rows of data to look at! Let's just look at subset for our\n# purposes.\n\ndenver %>% \n  slice(1:3)## # A tibble: 3 x 3\n##   station valid                sped\n##   <chr>   <dttm>              <dbl>\n## 1 DEN     2016-06-01 00:00:00   9.2\n## 2 DEN     2016-06-01 00:05:00   9.2\n## 3 DEN     2016-06-01 00:10:00   6.9"},{"path":"data.html","id":"distill","chapter":"3 Data","heading":"3.6 Distill","text":"Now, may noticed whenever knit .Rmd files, produce HTML file. just little bit earlier webscraping section, talked websites using HTML make basic structure. logic, can’t make websites using HTML files created knit .Rmd?’s exactly distill package . Essentially, distill allows make websites using HTML files output knit . also organizes pages nicely make website navigable look professional.’s example distill website. can see, clear, easy website pages easily distinguishable information.cover use distill package associated primer tutorials, ’ll walk creating distill page developing citations.","code":""},{"path":"data.html","id":"common-errors-and-functions","chapter":"3 Data","heading":"3.6.1 Common Errors and Functions","text":"forge onwards world distill, likely encounter variety errors ’ll make want throw computer away. , ’ll go common functions errors encountered making distill page.","code":""},{"path":"data.html","id":"create_website","chapter":"3 Data","heading":"3.6.1.1 create_website()","text":"first step creating website using create_website() function. creates entire distill website first function run whenever want make one. Just use following syntax:essentially creates distill website current R project, titles “title---website” formats can publish Github Pages (Github’s website publishing service). run , see index.Rmd .Rmd file project, well miscellaneous files. “Home” “” pages can use website.","code":"\nlibrary(distill)\ncreate_website(dir = \".\", title = \"title-of-your-website\", gh_pages = TRUE)"},{"path":"data.html","id":"create_article","chapter":"3 Data","heading":"3.6.1.2 create_article()","text":"want pages ? restricted Home page pretty bad, especially want bibliography. use create_article() create new page within website. Just run function title file ’ll automatically create .Rmd file code ., need put website. Just go _site.yml file add page using text: \"Sources\" \"href: sources.html\". _site.yml file look like add page._site.yml file keeps everything line organizes distill website. However, ’s specific syntax, take care add specific things. Additionally, keep mind always use create_article() function whenever need create new page .Rmd file. add .Rmd file manually. lead errors can break project.","code":"\ncreate_article(\"sources.Rmd\")name: \".\"\ntitle: \"title-of-your-website\"\ndescription: |\n  Welcome to the website. I hope you enjoy it!\noutput_dir: \"docs\"\nnavbar:\n  right:\n    # These two pages were automatically put in.\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    # This page was added by us.\n    - text: \"Sources\"\n      href: sources.html\noutput: distill::distill_article"},{"path":"data.html","id":"formatting-the-article","chapter":"3 Data","heading":"3.6.1.3 Formatting the article","text":"can format article just like ’s still RMarkdown file. However, can also put advanced features like block quotes . Just make sure edit headers top file without knowing exactly ’re order prevent errors.","code":""},{"path":"data.html","id":"error-operator-is-invalid-for-atomic-vectors","chapter":"3 Data","heading":"3.6.1.4 Error: $ operator is invalid for atomic vectors","text":"error caused creating .Rmd file without using _site.yml using create_article() make . Essentially, distill doesn’t know deal extra .Rmd file crashes . can normally solved just deleting extra .Rmd, may necessary just nuke project orbit restart., make sure never create extra .Rmd, ’s just going work end start encountering error. want write R code graph can put website, just use normal R script file going File -> New File -> R Script RStudio.","code":""},{"path":"data.html","id":"yaml-errors","chapter":"3 Data","heading":"3.6.1.5 YAML Errors","text":"errors created problems _site.yml file headers (— part) top file. parts essentially tell distill title description page , well information. However, missing parts can cause pretty significant problems distill just doesn’t information needs. example, deleting site: part header allow website work including description: | stop description showing .’s YAML header top .Rmd file look like. can make edits add information, make sure keep basic structure order prevent errors.Keep mind can modify need make new page. However, whenever error beginning Error yaml::yaml.load(..., eval.expr = TRUE) :, normally means YAML error need fix part code.","code":"---\ntitle: \"Home\"\ndescription: |\n  Description of the page\nsite: distill::distill_website\n---"},{"path":"data.html","id":"summary-3","chapter":"3 Data","heading":"3.7 Summary","text":"Pulling data R key factor data science process.Use files like CSV, Excel, RDS, JSON, SQL files/databases organize share data.Use SelectorGadget pull data websites, make sure ’s legal .Use API get data government agencies companies.Publish results website using Distill.chapter pulling data R. ’s graphing data, cleaning , wrangling , anything like , ’s just pulling data without losing mind.chapter, looked common file formats used readr package read pull data can use data plots. allowed us download data websites like kaggle.com use R session.also looked databases can pull select pieces data aren’t overloading computers thousands millions data rows. went can write SQL queries can acheive special effects without causing errors detonating computers.finally, looked can pull data websites webscraping APIs, letting us pull data internet quickly easily. lets us find data projects load R session without create download file internet.end, chapter getting data people using inside projects.\nFIGURE 3.5: Make sure use date data.\n","code":""},{"path":"rubin-causal-model.html","id":"rubin-causal-model","chapter":"4 Rubin Causal Model","heading":"4 Rubin Causal Model","text":"ever wondered world like without ?George Bailey, movie “’s Wonderful Life,” believes life served purpose. movie follows George explores world never born. clear profound impact lives many people community. actions mattered, ever realized.showing world like without George, get idea causal effect life town people live . chapter covers causal effects using framework potential outcomes Rubin Causal Model (RCM).","code":""},{"path":"rubin-causal-model.html","id":"preceptor-tables","chapter":"4 Rubin Causal Model","heading":"4.1 Preceptor Tables","text":"Preceptor Table table rows columns data data (reasonably) like , , none data missing, thing want know trivial calculate. Preceptor Tables vary number rows columns, well amount missing data. use question marks indicate missing data Preceptor Table.Assume five adult brothers given four heights. average height five brothers? given , can use statistics estimate unknown value. Consider Preceptor Table problem: case, row brother column height. always ID column Preceptor Tables can identify different units. always furthest left. addition ID column, call column brothers’ heights outcome column.estimate average height, need estimate Andy’s height. One guess just average four brothers: \\[\\frac{(178 + 165 + 172 + 173)}{4} = 172\\] realistic? consider things like know four brothers’ heights. sampled randomly? know heights tallest family? case make sense use average estimate Andy’s height? Probably !Keep mind truth , state world independent knowledge . Andy specific height. complete Preceptor Table, missing values, calculate average height brothers exactly. fancy statistics needed. Just arithmetic.Consider complex problem. heights 100 Harvard students, want know average height students school. , 100 students randomly sampled? estimate 90th percentile height student population? questions complicated, might less confident best guess. Now let’s say given characteristics height 100 sampled students, .e., sex age. ’ll notice Preceptor Table new type column: covariates. better able forecast student’s height given age sex?far, asked predictive questions. chapter, however, primarily focuses causal inference.","code":""},{"path":"rubin-causal-model.html","id":"causal-effect","chapter":"4 Rubin Causal Model","heading":"4.2 Causal effect","text":"\nFIGURE 4.1: study conducted Ryan Enos.\nRubin Causal Model (RCM) based idea potential outcomes. example, Enos (2014) measured attitudes toward immigration among Boston commuters. Individuals exposed one two possible conditions, attitudes towards immigrants recorded. One condition train platform near individuals speaking Spanish. train platform without Spanish-speakers. calculate causal effect Spanish-speakers nearby, need compare outcome individual one possible state world (Spanish-speakers) another (without Spanish-speakers). impossible observe potential outcomes . One potential outcomes always missing. dilemma Fundamental Problem Causal Inference.circumstances, interested comparing two experimental manipulations, one generally termed “treatment” “control.” difference potential outcome treatment potential outcome control “causal effect” “treatment effect.” scenario didn’t actually happen, thus didn’t observe, “counterfactual.” According RCM, causal effect platform Spanish-speakers difference attitude “treatment” (Spanish-speakers) “control” (Spanish-speakers).commuter survey consisted three questions, measuring agreement 1 5 integer scale, 1 liberal 5 conservative. person, three answers summed, generating overall measure attitude toward immigration ranged 3 15. attitude towards immigrants 13 Spanish-speakers 9 without Spanish-speakers, causal effect platform Spanish-speakers 4-point increase score.use symbol \\(Y\\) represent potential outcomes, variable interested understanding modeling. \\(Y\\) called response outcome variable. variable want “explain.” case attitude score. trying understand causal effect, need two symbols: control treated values can represented separately. use symbols \\(Y_t\\) \\(Y_c\\).","code":""},{"path":"rubin-causal-model.html","id":"potential-outcomes","chapter":"4 Rubin Causal Model","heading":"4.2.1 Potential outcomes","text":"Suppose Yao one commuters surveyed experiment. omniscient, know outcomes Yao treatment (Spanish-speakers) control (Spanish-speakers). can show using ideal Preceptor Table. Preceptor Table considered ideal missing data, calculating number interested trivial. table know causal effect Yao. Everyone else study might lower attitude score (liberal) treated. Regardless causal effect subjects, causal effect Yao train platform Spanish-speakers shift towards conservative attitude.\nUsing response variable — actual symbol rather written description — makes concise Preceptor Table.Yao139 Recall “causal effect” difference Yao’s potential outcomes treatment control.Yao139+4 Remember actual Preceptor Table bunch missing data! can use simple arithmetic calculate causal effect Yao’s attitude toward immigration. Instead, required estimate . estimand variable real world trying measure. case, \\(Y_{t}-Y_{c}\\), \\(+4\\). estimand value calculated, rather unknown variable want estimate.\nFIGURE 4.2: Don Rubin professor Statistics Harvard.\n","code":""},{"path":"rubin-causal-model.html","id":"multiple-units","chapter":"4 Rubin Causal Model","heading":"4.2.2 Multiple units","text":"Generally study many individuals (, broadly, “units”) potential outcomes. notation needed allow us differentiate different units.words, needs distinction \\(Y_t\\) Yao, \\(Y_t\\) Emma. use variable \\(u\\) (\\(u\\) “unit”) indicate outcome control outcome treatment can differ individual unit (person).Instead \\(Y_t\\), use \\(Y_t(u)\\) represent “Attitude Treated.” want talk Emma, say “Emma’s Attitude Treated” “\\(Y_t(u = Emma)\\)” “\\(Y_t(u)\\) Emma,” just \\(Y_t\\). notation ambiguous one subject.Let’s look ideal Preceptor Table subjects using new notation:Yao139+4Emma1411+3Cassidy116+5Tahmid912-3Diego34-1 ideal Preceptor Table, many possible estimands might interested . Consider examples, along true values:potential outcome one person, e.g., Yao’s potential outcome treatment: \\(13\\).causal effect one person, Emma. difference potential outcomes: \\(14 - 11 = +3\\).positive causal effect: \\(+5\\), Cassidy.negative causal effect: \\(-3\\), Tahmid.median causal effect: \\(+3\\).median percentage change: \\(+27.2\\%\\). see , calculate percentage change person. ’ll get 5 percentages: \\(+44.4\\%\\), \\(+27.2\\%\\), \\(+83.3\\%\\), \\(-25.0\\%\\), \\(-25.0\\%\\).. lot things one might care !variables calculated examples estimands might interested . One estimand important enough name: average treatment effect, often abbreviated ATE. average treatment effect mean individual causal effects. , mean \\(+1.6\\).Remember actual Preceptor Table riddled, always , question marks, looks like:Yao13??Emma14??Cassidy?6?Tahmid?12?Diego3?? Calculating values table longer simple math problem. See discussion Harvard Professor Matt Blackwell:","code":""},{"path":"rubin-causal-model.html","id":"simple-models","chapter":"4 Rubin Causal Model","heading":"4.3 Simple models","text":"can fill question marks? Fundamental Problem Causal Inference, can never know missing values. can never know missing values, must make assumptions. “Assumption” just means need “model,” models parameters.","code":""},{"path":"rubin-causal-model.html","id":"a-single-value-for-tau","chapter":"4 Rubin Causal Model","heading":"4.3.1 A single value for tau","text":"One model might causal effect everyone. single parameter, \\(\\tau\\), estimate. (\\(\\tau\\) Greek letter, written “tau” rhyming “cow.”) estimate, can fill Preceptor Table , knowing , can estimate unobserved potential outcome person. use assumption \\(\\tau\\) estimate counterfactual outcome unit.Remember Preceptor Table looks like missing data:Yao13??Emma14??Cassidy?6?Tahmid?12?Diego3?? assume \\(\\tau\\) treatment effect everyone, fill table? using \\(\\tau\\) estimate causal effect. definition: \\(Y_t(u) - Y_c(u) = \\tau\\). Using simple algebra, clear \\(Y_t(u) = Y_c(u) + \\tau\\) \\(Y_c(u) = Y_t(u) - \\tau\\). words, add observed value every observation control group (subtract observed value every observation treatment group), thus fill missing values.Assuming constant treatment effect, \\(\\tau\\), everyone, filling missing values look like :Yao13$$13 - \\tau$$$$\\tau$$Emma14$$14 - \\tau$$$$\\tau$$Cassidy$$6 + \\tau$$6$$\\tau$$Tahmid$$12 + \\tau$$12$$\\tau$$Diego3$$3 - \\tau$$$$\\tau$$ Now need find estimate \\(\\tau\\) order fill missing values. One approach subtract average observed control values average observed treated values. \\[((13 + 14 + 3) / 3) - ((6 + 12) /  2)\\] \\[10 - 9 = +1\\]gives us estimate \\(+1\\) \\(\\tau\\). Let’s fill missing values adding \\(\\tau\\) observed values control subtracting \\(\\tau\\) observed value treatment like :Yao13$$13 - (+1)$$+1Emma14$$14 - (+1)$$+1Cassidy$$6 + (+1)$$6+1Tahmid$$12 + (+1)$$12+1Diego3$$3 - (+1)$$+1 gives us:Yao1312+1Emma1413+1Cassidy76+1Tahmid1312+1Diego32+1 ","code":""},{"path":"rubin-causal-model.html","id":"two-values-for-tau","chapter":"4 Rubin Causal Model","heading":"4.3.2 Two values for tau","text":"second model might assume causal effect different levels category within levels. example, perhaps \\(\\tau_F\\) females \\(\\tau_M\\) males \\(\\tau_F != \\tau_M\\). making assumption give us different model can fill missing values Preceptor Table. key concept can’t make progress unless make assumptions. inescapable result Fundamental Problem Causal Inference.Consider model causal effects differ based sex:Yao13$$13 - \\tau_M$$$$\\tau_M$$Emma14$$14 - \\tau_F$$$$\\tau_F$$Cassidy$$6 + \\tau_F$$6$$\\tau_F$$Tahmid$$12 + \\tau_M$$12$$\\tau_M$$Diego3$$3 - \\tau_M$$$$\\tau_M$$ two different estimates \\(\\tau\\).\\(\\tau_M\\) \\[(13+3)/2 - 12 = -4\\]\n\\(\\tau_F\\) \\[(14-6 = +8)\\]Using values, fill new table like :Yao13$$13 - (-4)$$-4Emma14$$14 - (+8)$$+8Cassidy$$6 + (+8)$$6+8Tahmid$$12 + (-4)$$12-4Diego3$$3 - (-4)$$-4 gives us:Yao1317-4Emma146+8Cassidy146+8Tahmid812-4Diego37-4 now two different estimates Emma (everyone else table). estimate \\(Y_c(Emma)\\) using assumption constant treatment effect (single value \\(\\tau\\)), get \\(Y_c(Emma) = 13\\). estimate assuming treatment effect constant sex, calculate \\(Y_c(Emma) = 8\\). difference estimates Emma highlights difficulties inference. Models drive inference. Different models produce different inferences.","code":""},{"path":"rubin-causal-model.html","id":"heterogenous-treatment-effects","chapter":"4 Rubin Causal Model","heading":"4.3.3 Heterogenous treatment effects","text":"assumption constant treatment effect, \\(\\tau\\), usually true? ! never true. People vary. effect pill always different effect pill friend, least measure outcomes accurately enough. Treatment effects always heterogeneous, meaning vary across individuals.Reality looks like :Yao13$$13 - \\tau_{yao}$$$$\\tau_{yao}$$Emma14$$14 - \\tau_{emma}$$$$\\tau_{emma}$$Cassidy$$6 + \\tau_{cassidy}$$6$$\\tau_{cassidy}$$Tahmid$$12 + \\tau_{tahmid}$$12$$\\tau_{tahmid}$$Diego3$$3 - \\tau_{diego}$$$$\\tau_{diego}$$ Can solve \\(\\tau_{yao}\\)? ! Fundamental Problem Causal Inference. can make progress unwilling assume least structure causal effect across different individuals? Instead worrying causal effect specific individuals, might, instead, focus causal effect entire population.","code":""},{"path":"rubin-causal-model.html","id":"average-treatment-effect","chapter":"4 Rubin Causal Model","heading":"4.3.4 Average treatment effect","text":"average treatment effect average difference potential outcomes treated group control groups. averaging linear operator, average difference difference averages. distinction estimand estimands like \\(\\tau\\), \\(\\tau_M\\) \\(\\tau_F\\), , case, care using average treatment effect fill missing values row. average treatment effect useful don’t assume anything individuals’ \\(\\tau\\), like \\(\\tau_{yao}\\), can still understand something average causal effect across whole population., simplest way estimate ATE take mean treated group (\\(10\\)) mean control group (\\(9\\)) take difference means (\\(1\\)). ’ll call estimate average treatment effect, \\(\\widehat{ATE}\\), pronounced “ATE-hat.”already exact calculation , talking ? Remember unwilling assume treatment effect constant study population, solve \\(\\tau\\) \\(\\tau\\) different different individuals. \\(\\widehat{ATE}\\) helpful.estimands may require filling question marks Preceptor Table. can get good estimate average treatment effect without filling every question mark — average treatment effect just single number. Rarely study care happens individuals. case, don’t care specifically happen Cassidy’s attitude treated. Instead, care generally experiment impacts people’s attitudes towards immigrants. average estimate, like \\(\\widehat{ATE}\\) can helpful.noted , popular estimand. ?’s obvious estimator estimand: difference observed outcomes treated group control group: \\(Y_t(u) - Y_c(u)\\).’s obvious estimator estimand: difference observed outcomes treated group control group: \\(Y_t(u) - Y_c(u)\\).treatment randomly assigned, estimator unbiased: can fairly confident estimate large enough treatment control group.treatment randomly assigned, estimator unbiased: can fairly confident estimate large enough treatment control group.earlier, willing assume causal effect everyone (big assumption!), can use estimate ATE, \\(\\widehat{ATE}\\), fill missing individual values Preceptor Table.earlier, willing assume causal effect everyone (big assumption!), can use estimate ATE, \\(\\widehat{ATE}\\), fill missing individual values Preceptor Table.Just ATE often useful estimand doesn’t mean always .Consider point #3. example, let’s say treatment effect vary dependent sex. males strong negative effect (-3.5), females smaller positive effect (+1). However, average treatment effect whole sample, even estimate correctly, single negative number (-1) – since negative effect males larger positive effect females.Estimating average treatment effect, calculating \\(\\widehat{ATE}\\), easy. \\(\\widehat{ATE}\\) good estimate actual ATE? , knew missing values Preceptor Table, calculate ATE perfectly. missing values may wildly different observed values. Consider unobservable ideal Preceptor Table:Yao1310+3Emma1411+3Cassidy96+3Tahmid1512+3Diego30+3 example, indeed constant treatment effect everyone: \\(+3\\). Note observed values , unobserved values estimated ATE, \\(-1\\), pretty far actual ATE, \\(+3\\). think reasonable estimate ATE, using value constant \\(\\tau\\) might best guess. discussion, see Matt Blackwell:","code":""},{"path":"rubin-causal-model.html","id":"complications","chapter":"4 Rubin Causal Model","heading":"4.4 Complications","text":"","code":""},{"path":"rubin-causal-model.html","id":"data-problems","chapter":"4 Rubin Causal Model","heading":"4.4.1 Data problems","text":"Assume run train experiment, want know average attitude towards immigrants United States adults. first, seems like easy problem — ’s nothing causal ! knew true values, build data set like :  , answer simply average values. table? ! actually : reality, don’t know attitude towards immigrants United States adults. , lot missing data.maybe survey 1,000 people attitudes towards immigrants, get table looks like :  surveying 1,000 people attitudes towards immigrants now values work . , however, solve missing data problem. likely interested attitudes toward immigration entire population, just 1,000 person sample. ’ll need think whether sample representative full population. vast majority US adults still value. second two common sources missing data:units sample, see one potential outcome.units outside sample, see potential outcomes.fact many potential sources missing data, explore discussion infinite Preceptor Table. missing data problem creates need statistical inferences. data missing, inference needed.\nFIGURE 4.3: study conducted Barfort, Klemmensen Larsen.\nLet’s consider new example experiment highlight another type data problem might encounter. Say want know causal effect elected governor longevity. states minimum age requirement elected governor 30. People age 30 chance elected governor. means people less 30 one possible potential outcome. actual Preceptor Table, means rows 2 columns (people old enough elected), rows 1 column (people young elected).actual Preceptor Table problem might look something like : Yao--?Dean Khurana??Cassidy--?Preceptor??Tahmid--? Yao, Cassidy Tahmid question mark treatment column chance elected governor. Often real world actual Preceptor Table might look like . rows two () potential outcomes, fewer. go ?consider actually interested knowing. case, don’t really care causal effect people can’t possibly elected governor. words, don’t care causal effect whole population, rather subset population. Just like case infinite Preceptor Table need throw rows specific problem. Instead saying want know causal effect elected governor, might specify want know causal effect American population 30.defined problem reasonable actual Preceptor Table, can begin deal question marks.","code":""},{"path":"rubin-causal-model.html","id":"causal-and-predictive-models","chapter":"4 Rubin Causal Model","heading":"4.4.2 Causal and predictive models","text":"Causal inference often compared prediction. prediction, want know outcome, \\(Y(u)\\). causal inference, want know function potential outcomes, treatment effect: \\(Y_t(u) - Y_c(u)\\).missing data problems. Prediction involves getting estimate outcome variable don’t , thus missing, whether future data unable collect. Thus, prediction term using statistical inference fill missing data individual outcomes, situations concept potential outcomes apply.Causal inference, however, term filling missing data potential outcomes. unlike prediction, one potential outcome can ever observed, even principle.causal inference prediction, process data missing observed crucial. think missing data similar observed data, can make inferences easily. , think dissimilarities consider model .Key point: predictive model, one \\(Y(u)\\) value unit. different RCM (least) two potential outcomes (treated control). one outcome column predictive model, whereas two causal model.predictive model infer happen outcome \\(Y(u)\\) changed \\(X\\) given unit. can compare two units, one one value \\(X\\) another different value \\(X\\).sense, models predictive. data stable distribution, make predictive forecast someone’s attitude. subset models causal, meaning , given individual, can change value \\(X\\) observe change outcome, \\(Y(u)\\), calculate causal effect.","code":""},{"path":"rubin-causal-model.html","id":"the-assignment-mechanism","chapter":"4 Rubin Causal Model","heading":"4.4.3 The assignment mechanism","text":"“assignment mechanism” process units received treatment units got control.sidestepped following question : difference sample means treated units control units, \\(\\widehat{ATE}\\), good estimate \\(ATE\\)? depends entirely method units assigned treatment, called assignment mechanism. mechanism whereby potential outcomes missing potential outcomes observed.already comes non-causal context considering sampling. trying estimate average attitude towards immigrants US, usually taking sample. process people enter sample called sampling mechanism. process people enter sample related attitude, even indirectly, estimates sample good estimates population.Whenever assignment mechanism correlated potential outcomes, say confounding. Confounding problem, since means simple estimate \\(ATE\\) , potentially, biased.Assignment mechanisms can also intentionally biased order manufacture desired outcomes. Let’s consider scenario entire platform either treated control. case assignment mechanism choice Spanish-speakers; allowed choose platform want stand . Let’s also say can perfectly predict attitude people platform. Spanish-speakers know platform liberal attitudes towards immigrants friendly, therefore always choose stand platforms. case, assignment mechanism platforms random. Spanish-speakers know values platforms experiment: Based knowledge Spanish-speakers experiment choose following treatment assignments: assignment mechanism used distorts averages \\(Y_t(u)\\) \\(Y_c(u)\\), turn distorts difference means. average treated group shifted lower (liberal), average control group shifted higher (conservative). gives illusion average treatment effect \\(\\widehat{ATE}\\) negative. true positive causal effect masked non-random assignment mechanism.Therefore, difference means longer good estimate ATE. fact, case wrong sign! merely consequence small sample: even million platforms experiment, get good estimate ATE.extreme example problem called selection bias. Selection bias person assigning treatment chooses basis potential outcomes. Spanish-speakers choosing platforms stand randomly. Rather, making treatment decisions based directly potential outcomes platform. Remember, whenever assignment mechanism correlated potential outcomes confounding, problem means estimate biased. examples confounding caused selection bias, selection bias always confounding.Much like best way avoid making poor inferences sample population take random sample population, best assignment mechanism avoiding confounding randomization. platform flip coin determine treatment control group.Random assignment guarantees , average, correlation treatment assignment anything else, neither covariates potential outcomes.many circumstances, however, randomized trials possible due ethical practical concerns. scenarios necessity non-random assignment mechanism.example, let’s say train platforms experiment loud Spanish-speakers might heard anyone nearby. Therefore necessity, quieter platforms can assigned treatment group. non-random assignment may introduce confounding. Say systematic difference people quieter platforms compared people louder platforms. case, assignment mechanism correlated potential outcomes, confounding.Many statistical methods developed causal inference non-random assignment mechanism. methods, however, beyond scope book.","code":""},{"path":"rubin-causal-model.html","id":"the-infinite-preceptor-table","chapter":"4 Rubin Causal Model","heading":"4.4.4 The infinite Preceptor Table","text":"discussed ideal Preceptor Tables (missing data), actual Preceptor Tables (question marks representing values don’t know), one type Preceptor Table. real world, Preceptor Table infinite number rows, therefore infinite amount missing data. call type Preceptor Table infinite Preceptor Table. reality unworkable, make assumptions reduce true problem something manageable.Let’s start looking kinds missing data make infinite Preceptor Table. example, say care causal effect experiment Yao. care attitude right experiment? ! also care Yao’s potential outcomes one year now, two years now, .full Preceptor Table includes people know (Yao) people don’t (example, Eliot), now future: fact, time continuous, row Yao now, Yao one second now, Yao one day now . Preceptor Table extends downward forever. Thus, order estimate causal effect, need assumptions, aren’t dealing infinite table.obvious way eliminate rows table assume causal effect Yao now ones Yao future. plausible? Sort . Yao now Yao one second pretty similar! Yao now Yao 30 years less . Unfortunately, ’s magic way get good estimate every missing value infinite Preceptor Table! assumptions, can reduce true problem problem dealing .can extend Preceptor Table adding people sample rows, can also add additional treatments columns. Let’s go back original five people sample. also wanted test causal effect another language spoken platform? ’ll call original treatment \\(t\\) new treatment \\(t'\\).Yao13??Emma11??Cassidy??10Tahmid??12Diego6?? Note Yao, now three causal effects can estimate: difference original treatment new treatment, difference original treatment control, difference new treatment control.Even just one language testing, still multiple treatments. example, amount time commuter platform Spanish-speakers might vary across commuters. case, might receive different treatment.Yao13?????Emma11?????Cassidy?????10Tahmid?????12Diego6????? Instead considering treatment terms duration, also consider different volume levels, measured decibels (dB), Spanish spoken.Yao13????Emma11????Cassidy????10Tahmid????12Diego6???? Indeed, infinite number possible treatments. Preceptor Table extends right forever. , assumptions come rescue. rather, just throw hands try estimate things. crucial define one’s estimand precisely: interested difference potential outcomes Spanish spoken 10 minutes 60 dB versus control, can ignore possible columns infinite Preceptor Table.Thus, whenever considering causal question, best way think start infinite Preceptor Table. First throw rows think duplicates (observations Yao one second now, two seconds now, etc.) outside scope interested now (maybe don’t care outcomes 30 years future study). Second, throw columns don’t care , possible treatments aren’t considering. Finally, define precisely—terms potential outcomes—estimand. may something simple, average treatment effect, something complex. done steps, can start thinking fill question marks. remember infinite Preceptor Table always , conscious rows columns throwing !","code":""},{"path":"rubin-causal-model.html","id":"no-causation-without-manipulation","chapter":"4 Rubin Causal Model","heading":"4.4.5 No causation without manipulation","text":"order potential outcome make sense, must possible, least priori. example, way Yao, circumstance, ever train study, \\(Y_{t}(u)\\) impossible . can never happen. \\(Y_{t}(u)\\) can never observed, even theory, causal effect treatment Yao’s attitude undefined.causal effect train study well defined simple difference two potential outcomes, might happen. case, (something else) can manipulate world, least conceptually, possible one thing different thing might happen.definition causal effects becomes much problematic way one potential outcomes happen, ever. example, causal effect Yao’s height weight? might seem just need compare two potential outcomes: Yao’s weight treatment (treatment defined 3 inches taller) Yao’s weight control (control defined current height).moment’s reflection highlights problem: can’t increase Yao’s height. way observe, even conceptually, Yao’s weight taller way make taller. can’t manipulate Yao’s height, makes sense investigate causal effect height weight. Hence slogan: causation without manipulation.raises question can manipulated. something manipulated, consider causal. can race ever considered causal? sex? genetic condition like color-blindness? Can manipulate characteristics? modern world questions simple.Take color-blindness example. Say interested color-blindness impacts ability complete jig-saw puzzle. color-blindness genetic might argue manipulated. advances technology like gene-therapy might allow us actually change someone’s genes. claim ability manipulate color-blindness? yes, measure causal effect color-blindness ability complete jig-saw puzzles.slogan “causation without manipulation” may first seem straight forward, clearly simple. Questions race, sex, gender genetics complex considered care.","code":""},{"path":"rubin-causal-model.html","id":"internal-and-external-validity","chapter":"4 Rubin Causal Model","heading":"4.4.6 Internal and external validity","text":"Recall two main sources missing data:units sample, see one potential outcomeFor units outside sample, see potential outcomesIf randomized assignment large sample, can confident good estimate average treatment effect sample. say experiment high internal validity: inferences making likely reflect truth sample.However, may interested population beyond particular sample, second main source missing data. example, let’s look broader context train experiment. likely exclusively concerned attitudes people ride trains, rather attitudes larger population. Train platforms, however, convenient setting run experiment. Let’s say ran randomized experiment 10,000 people Boston, found \\(\\widehat{ATE} = -1\\). assume estimate accurate larger general population?answer question depends part external validity study. 10,000 people study similar people want generalize findings ? Perhaps want generalize train commuters cities. Let’s say 10,000 people Boston choose ride trains environmental reasons. ’s another form selection bias. sample randomly selected population interested. problem? people differ systematically people way may affect response experiment. example, preference public transportation environmental reasons may correlated political beliefs.Note concern can expressed terms assignment mechanism. People don’t ride train 0% chance receiving treatment. Thus, study can’t directly speak treatment impact attitudes towards immigrants. way can make claims making additional assumptions, train-riders reflect makeup political beliefs people don’t ride trains.external validity study often directly related representativeness sample. Representativeness well sample represents larger population interested generalizing . train experiment allow us calculate causal effect people commute cars? Can calculate causal effect people New York City? generalize broader populations consider experimental estimates applicable beyond experiment. Maybe think commuters Boston New York similar enough, \\(\\widehat{ATE}\\) also good estimate causal effect treatment NYC. also conclude people commute car fundamentally different people commute train. true, say estimate true commuters sample accurately represent broader group want generalize .circumstances experiment may also affect external validity study. Perhaps train study conducted middle summer platforms uncomfortably hot. , variation one aspect treatment (whether Spanish-speakers nearby), don’t variation another (temperature train platform). may attitude towards immigrants impacted train platform uncomfortably hot, treatment impact attitude otherwise.dealing human subjects, particular concern regarding external validity: Hawthorne effect. human subjects know part experiment, may change behavior.example, Hawthorne effect can impact attitudes expressed surveys. Maybe respondents extreme attitude either direction (liberal conservative) survey opportunity express opinion.border concept validity consider well. , data valid, accurately captures concepts care . want use data connection problem face data .","code":""},{"path":"rubin-causal-model.html","id":"correlation-with-potential-outcomes","chapter":"4 Rubin Causal Model","heading":"4.4.7 Correlation with potential outcomes","text":"considering relationship treatment outcome, one important assumptions lack correlation treatment assignment potential outcomes. Consider version train experiment. Assume , Republican platform Spanish-speakers, attitude value 9. platform hear Spanish, attitude 11. Democrat attitude 9 regardless whether platform Spanish-speakers. words, causal effect treatment group +2 Republicans, 0 Democrats. run experiment random assignment, discover average causal effect somewhere 0 2, depending relative proportion Republicans Democrats.1119+2R2119+2R3990D4119+2R5990D Unfortunately (?), people choose ride certain trains randomly. might assume/hope correlation train ride potential outcomes. true, still able estimate causal effect. Yet rarely true general. , instead, Republicans control group, Democrats treated? case, everyone attitude 9! appears presence Spanish-speakers platform “matter.” correlation treatment potential outcomes invalidates naive estimate average treatment effect.R?9?R?9?D9??R?9?D9?? Keep mind problem arises correlation treatment assignment potential outcome, simply correlation treatment assignment outcome. case, correlation treatment assignment outcome zero! Just looking outcomes observe enough. must make assumptions outcomes don’t observe, happened.","code":""},{"path":"rubin-causal-model.html","id":"summary-4","chapter":"4 Rubin Causal Model","heading":"4.5 Summary","text":"fundamental components every problem causal inference units, treatments outcomes. units rows tibble. treatments columns. outcomes values. Whenever confront problem causal inference, start identifying units, treatments outcomes.causal effect difference one potential outcome another. different life missed train?Preceptor Table includes data actually , data like , solve problem. ideal Preceptor Table involves missing data. know outcome unit \\(\\) treatment control. ideal Preceptor Table easy calculate, using algebra, quantity interest. actual Preceptor Table littered missing data, represented question mark. know value outcome unit \\(\\) treatment , definition, can know outcome unit \\(\\) control.tibble store data Preceptor Table one key difference structure. tibble one column outcome variable. Preceptor Table one column potential outcome, meaning one column values treatment variable.causal effect treatment single unit point time difference value outcome variable treatment without treatment. call “potential outcomes” , , can observe one . Fundamental Problem Causal Inference impossible observe causal effect single unit. must make assumptions — .e, must make models — order estimate causal effects.Random assignment treatments units best way estimate causal effects. assignment mechanisms subject confounding. treatment assigned correlated potential outcomes, hard estimate true treatment effect. (always, use terms “causal effects” “treatment effects” interchangeably. random assignment, can, mostly safely, estimate average treatment effect (ATE) looking difference average outcomes treated control units.wary claims made situations without random assignment: dragons!","code":""},{"path":"probability.html","id":"probability","chapter":"5 Probability","heading":"5 Probability","text":"chapter edited.usual touchstone whether someone asserts mere persuasion least subjective conviction, .e., firm belief, betting. Often someone pronounces propositions confident inflexible defiance seems entirely laid aside concern error. bet disconcerts . Sometimes reveals persuaded enough one ducat ten. happily bet one, ten suddenly becomes aware previously noticed, namely quite possible erred. -— Immanuel Kant, Critique Pure ReasonThe central tension, opportunity, data science interplay data science, empirical observations models use understand . Probability language use explore interplay; connects models data, data models.package need chapter tidyverse.","code":"\nlibrary(tidyverse)"},{"path":"probability.html","id":"list-columns-and-map-functions","chapter":"5 Probability","heading":"5.1 List-columns and map functions","text":"learning probability, need expand collection R tricks understanding list-columns map_* functions. Recall list different atomic vector. atomic vectors, element vector one value. Lists, however, can contain vectors, even complex objects, elements.x list two elements. first element numeric vector length 3. second element character vector length 2. use [[]] extract specific elements.number built-R functions output lists. example, ggplot objects making store plot information lists. function returns multiple values can used create list output wrapping returned object list().Notice 1--1 tibble one observation, list one element. Voila! just created list-column.function returns multiple values vector, like range() , must use list() wrapper want create list-column.list column column data list rather atomic vector. stand-alone list objects, can pipe str() examine column.can use map_* functions create list-column , much importantly, work list-column afterwards.map_* functions, like map_dbl() example, take two key arguments, .x (data acted ) .f (function act data). , .x data col_1, list-column. .f function sum(). However, can simply write map_dbl(col_1, sum). Instead, use map_* functions requires use tilde — ~ — indicate start function use dot — . — specify data goes function.map_* functions family functions, suffix specifying type object returned. map() returns list. map_dbl() returns double. map_int() returns integer. map_chr() returns character, .summarise map function map_* functions convert data (vector list) specific denoted functions formula set, always results list called “list Column.” two arguments map() function well map_*() function .x .f, .x .f placed map() functions map_*()functions like map(.x,.f), .x either list vector, .f either direct function like .f=mean, formula like .f= ~mean(.x), remember put ~ using .f formula, difference map map_* function know want outcome data specific data vector like (double, logical,character,integer) rather general list map(), can use map_* instead map organized list column.Consider detailed example:flexibility possible via use list-columns map_* functions. workflow extremely common. start empty tibble, using ID specify number rows. skeleton, step pipe adds new column, working column already exists.","code":"\nx <- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx## [[1]]\n## [1]  4 16  9\n## \n## [[2]]\n## [1] \"A\" \"Z\"\nx[[1]]## [1]  4 16  9\nx <- rnorm(10)\n\n# range() returns the min and max of the argument \n\ntibble(col_1 = list(range(x))) ## # A tibble: 1 x 1\n##   col_1    \n##   <list>   \n## 1 <dbl [2]>\ntibble(col_1 = list(range(x))) %>%\n  str()## tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -1.161 0.178\ntibble(col_1 = list(range(x))) %>%\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) %>% \n  str()## tibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -1.161 0.178\n##  $ col_2: num -0.983\ntibble(ID = 1) %>% \n  mutate(col_1 = map(ID, ~range(rnorm(10)))) %>%\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) %>% \n  mutate(col_3 = map_int(col_1, ~ length(.))) %>% \n  mutate(col_4 = map_chr(col_1, ~ sum(.))) %>% \n  str()## tibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n##  $ ID   : num 1\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -1.95 0.993\n##  $ col_2: num -0.956\n##  $ col_3: int 2\n##  $ col_4: chr \"-0.956291\"\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) %>% \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) %>% \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) %>% \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))## # A tibble: 3 x 4\n##      ID draws        max min_max  \n##   <int> <list>     <dbl> <list>   \n## 1     1 <dbl [10]> 1.90  <dbl [2]>\n## 2     2 <dbl [10]> 0.744 <dbl [2]>\n## 3     3 <dbl [10]> 1.10  <dbl [2]>"},{"path":"probability.html","id":"probability-distributions","chapter":"5 Probability","heading":"5.2 Probability distributions","text":"\nFIGURE 5.1: Dice Probability.\nmean Trump 30% chance winning re-election fall 2020? 90% probability rain today? dice casino unfair?Probability quantifying uncertainty. can think probability proportion. probability event occurring number 0 1, 0 means event impossible 1 means event 100% certain.Let’s begin simplest events: coin flips dice rolls. dice coins fair, can operate assumption outcomes equally likely.allows us make following statements:probability rolling 1 2 2/6, 1/3.probability rolling 1, 2, 3, 4, 5, 6 1.probability flipping coin getting tails 1/2.purposes Primer, probability distribution mathematical object covers set outcomes, distinct outcome chance occurring 0 1 inclusive. chances must sum 1. set possible outcomes — heads tails coin, 1 6 single die, 2 12 pair dice — can either discrete continuous. Remember, discrete data information can take certain value. hand, continuous data data can take values, like height weight sense “continuous.” set outcomes domain probability distribution. three types probability distributions: mathematical, empirical, posterior.key difference distribution, explored Section 2.8, probability distribution requirement sum probabilities individual outcomes must exactly 1. requirement distribution. distribution can turned probability distribution “normalizing” , explore. context, often refer distribution (yet) probability distribution “unnormalized” distribution.Pay attention notation. Whenever talking specific probability (represented single value), use \\(\\rho\\) (Greek letter “rho” spoken aloud “p” us) subscript specifies exact outcome probability. instance, \\(\\rho_h = 0.5\\) denotes probability getting heads coin toss coin fair. \\(\\rho_t\\) — spoken “PT” “P sub T” “P tails” — denotes probability getting tails coin toss. However, referring entire probability distribution set outcomes, use \\(\\text{Prob}()\\). example, probability distribution coin toss \\(\\text{Prob}(\\text{coin})\\). , \\(\\text{Prob}(\\text{coin})\\) composed two specific probabilities (50% 50%) mapped two values domain (Heads Tails). Similarly, \\(\\text{Prob}(\\text{sum two dice})\\) probability distribution set 11 outcomes (2 12) possible take sum two dice. \\(\\text{Prob}(\\text{sum two dice})\\) made 11 numbers — \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_{12}\\) — representing unknown probability sum equal value. , \\(\\rho_2\\) probability rolling 2.","code":""},{"path":"probability.html","id":"flipping-a-coin","chapter":"5 Probability","heading":"5.2.1 Flipping a coin","text":"data science problems start question. Example: result next flip coin? questions answered help probability distributions.mathematical distribution based mathematical formula. Assuming coin perfectly fair, , average, get heads often get tails.empirical distribution based data. can think probability distribution created running simulation. theory, increase number coins flip simulation, empirical distribution look similar mathematical distribution. mathematical distribution Platonic form. empirical distribution often look like mathematical probability distribution, rarely exactly .simulation, 56 heads 44 tails. outcome vary every time run simulation, proportion heads tails different coin fair.posterior distribution based beliefs expectations. displays belief things can’t see right now. may posterior distributions outcomes past, present, future.case coin toss, posterior distribution changes depending beliefs. instance, let’s say friend brought coin school asked bet . result heads, pay $5.makes suspicious, world longer trust “population” made previous two examples, results probability based flipping fair coin, ’s way left (don’t jsut simply walk away) posterior distribution. posterior distribution reflects beliefs based assumption, time population longer fair dice define \\(\\rho_h\\)=0.5, population “crooked” dice might believe \\(\\rho_h\\) 0.95 \\(\\rho_t\\) 0.05.full terminology mathematical (empirical posterior) probability distribution. often shorten just mathematical (empirical posterior) distribution. word “probability” understood, even present.","code":"\n# We are flipping one fair coin a hundreds times. We need to get the same result\n# each time we create this graphic because we want the results to match the\n# description in the text. Using set.seed() guarantees that the random results\n# are the same each time. We define 0 as tails and 1 as heads.\n\nset.seed(3)\n\ntibble(results = sample(c(0, 1), 100, replace = TRUE)) %>% \n  ggplot(aes(x = results)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Flipping one coin a hundred times\",\n         x = \"Outcome\\nResult of Coin Flip\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = c(0, 1), \n                       labels = c(\"Heads\", \"Tails\")) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"probability.html","id":"rolling-two-dice","chapter":"5 Probability","heading":"5.2.2 Rolling two dice","text":"mathematical distribution tells us , fair dice, probability getting 1, 2, 3, 4, 5, 6 equal: 1/6 chance . roll two dice time sum numbers, values closest middle common values edge combinations numbers add middle values.\\(p(2)= 1/6*1/6=\\frac{1}{36} = 2.8 \\%\\)\n\\(p(3)= 1/6*1/6*2=\\frac{1}{18} = 5.6 \\%\\)\n\\(p(4)= 1/6*1/6*3=\\frac{1}{12} = 8.3 \\%\\)\n\\(p(5)= 1/6*1/6*4=\\frac{1}{9} = 11.1 \\%\\)\n\\(p(6)= 1/6*1/6*5=\\frac{5}{36} = 13.9 \\%\\)\n\\(p(7)= 1/6*1/6*6=\\frac{1}{6} = 16.7 \\%\\)\n\\(p(8)= 1/6*1/6*5=\\frac{5}{36} = 13.9 \\%\\)\n\\(p(9)= 1/6*1/6*4=\\frac{1}{9} = 11.1 \\%\\)\n\\(p(10)= 1/6*1/6*3=\\frac{1}{12} = 8.3 \\%\\)\n\\(p(11)= 1/6*1/6*2=\\frac{1}{18} = 5.6 \\%\\)\n\\(p(12)= 1/6*1/6=\\frac{1}{36} = 2.8 \\%\\)get empirical distribution rolling two dice hundred times, either hand computer simulation. result identical mathematical distribution inherent randomness real world /simulation.might consider labeling y-axis plots empirical distributions “Proportion” rather “Probability” since actual proportion, calculated real (simulated) data. keep “Probability” since want emphasize parallels mathematical, empirical posterior probability distributions.posterior distribution rolling two dice hundred times depends beliefs. take dice Monopoly set, reason believe assumptions underlying mathematical distribution true. However, walk crooked casino host asks play craps, might suspicious, just “flipping coin example” word “suspicious” means longer trust “population” mathematical empircal distribution drawn data . example, craps, come-roll 7 11 “natural,” resulting win “shooter” loss casino. might expect numbers occur less often fair dice. Meanwhile, come-roll 2, 3 12 loss shooter. might also expect values like 2, 3 12 occur frequently. posterior distribution might look like :Someone less suspicious casino posterior distribution looks like mathematical distribution.","code":"\n# In the coin example, we create the vector ahead of time, and then assigned\n# that vector to a tibble. There was nothing wrong with that approach. And we\n# could do the same thing here. But the use of map_* functions is more powerful.\n\nset.seed(1)\n\nemp_dist_dice <- tibble(ID = 1:100) %>% \n  mutate(die_1 = map_dbl(ID, ~ sample(c(1:6), size = 1))) %>% \n  mutate(die_2 = map_dbl(ID, ~ sample(c(1:6), size = 1))) %>% \n  mutate(sum = die_1 + die_2) %>% \n  ggplot(aes(x = sum)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 1, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Sum from rolling two dice, replicated one hundred times\",\n         x = \"Outcome\\nSum of Two Die\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = seq(2, 12, 1), labels = 2:12) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\nemp_dist_dice"},{"path":"probability.html","id":"presidential-elections","chapter":"5 Probability","heading":"5.2.3 Presidential elections","text":"Now let’s say building probability distributions political events, like presidential election. want know probability Democratic candidate wins X electoral votes, X comes range possible outcomes: 0 538. (total number electoral votes US elections since 1964 538.)can start mathematical distribution X assumes chances Democratic candidate winning given state’s electoral votes 0.5 results state independent.know campaign platforms, donations, charisma, many factors contribute candidate’s success. Elections complicated coin tosses. also know many presidential elections history resulted much bigger victories defeats distribution seems allow .empirical distribution case involve looking past elections United States counting number electoral votes Democrats won . empirical distribution, create tibble electoral vote results past elections. Looking elections since 1964, can observe number electoral votes Democrats received one different. Given 15 entries, difficult draw conclusions make predictions based empirical distribution.However, model enough suggest assumptions mathematical probability distribution work electoral votes. model assumes Democrats 50% chance receiving 538 votes. Just looking mathematical probability distribution, can observe receiving 13 17 486 votes 538 extreme almost impossible mathematical model. However, empirical distribution tells us real election results.using past elections data mathematical empirical distribution think performance past elections influence election 2020.posterior distribution electoral votes popular topic, area strong disagreement, among data scientists. Consider posterior FiveThirtyEight.posterior FiveThirtyEight website August 13, 2020. created using data distribution, simply displayed differently. electoral result, height bar represents probability given event occur. However, lablels y-axis telling us specific probability outcome . OK! specific values useful. removed labels y-axes, matter?posterior Economist, also August 13, 2020. looks confusing first chose merge axes Republican Democratic electoral votes. can tell Economist less optimistic, relative FiveThirtyEight, Trump’s chances election.two models, built smart people using similar data sources, reached fairly different conclusions. Data science difficult! one “right” answer. Real life problem set.\nFIGURE 5.2: Watch makers two models throw shade Twitter! Eliot Morris one primary authors Economist model. Nate Silver charge 538. don’t seem impressed ’s work! smack talk .\nmany political science questions explore posterior distributions. can relate past, present, future.Past: many electoral votes Hilary Clinton won picked different VP?Present: total campaign donations Harvard faculty?Future: many electoral votes Democratic candidate president win 2024?","code":""},{"path":"probability.html","id":"height","chapter":"5 Probability","heading":"5.2.4 Height","text":"Question: height next adult male meet?three examples discrete probability distributions, meaning outcome variable can take limited set values. coin flip two outcomes. sum pair dice 11 outcomes. total electoral votes Democratic candidate 539 possible outcomes. limit, can also create continuous probability distributions infinite number possible outcomes. example, average height American male real number 0 inches 100 inches. (course, average value anywhere near 0 100 absurd. point average 68.564, 68.5643, 68.56432 68.564327, real number.)characteristics discrete probability distributions reviewed apply just much continuous probability distributions. example, can create mathematical, empirical posterior probability distributions continuous outcomes just discrete outcomes.Mathematical distribution complete based mathematical formula assumptions like Flipping coin session assume coin perfectly fair coin probability landing heads tails equal. case, assume average hight men 175 cm, well standard deviation height around 9 cm. two values, average also called mean, standard deviation (sd), can create normal distribution using rnorm() function. normal distribution good approximation generalization height scenario.Mathematical Distribution:\\(mean=175\\)\n\\(sd=9\\)Now create normal distribution, histogram graph ’s high bar near 178, set creating tibble simulated data. anybody can question legitimacy model, completely throw trash,one way telling someone make graph bad empirical distribution.empirical distribution involves using data National Health Nutrition Examination Survey (NHANES). instead making model ourself using mathematical formula, use actual data, can get data either simulated like “flipping coin” “Rolling two dice” scenario, used data someone else, like presidential election scenario.posterior distribution heights depends context. considering adult men America? case, posterior probably look lot like empirical distribution using NHANES data. asked distribution heights among players NBA, posterior might look like:general think difference three distributions, can think mathematical completely theoretical, imagine paper pencil, ask create model, model basically mathematical distributions. Empirical distribution hand different, empirical distribution completely based data, thing writing formula equation coming distribution based , come empirical distribution analyzing data, can think empirical distribution someone flip coin 1000 times, record data creating graph based . Last least, Posterior distribution graph represents one’s belief certain situation, example think flipping coin, mathematical empirical results likely look similar, assumption posterior also looks similar, casino, someone tells want bet 100 bucks head bet 100 bucks tail, wins get’s money. Now maybe suspicious, won’t likely trust person convincing bet, now posterior distribution help visualize quantified beliefs extremely helpful making decision.short:\n* Mathematical distribution based mathematical formula basic assumptions\n* Empirical distribution based data, data eigther done somebody else.\n* Posterior distribution based belief, belief usually shaped added information scenario.Comments:\nFIGURE 5.3: truth \ntruth . asked 300+ million Americans whether approve President Biden, know \\(p\\) exactly. Alas, can’t . use posterior probability distribution summarize beliefs true value \\(p\\), truth can never confirm.truth . asked 300+ million Americans whether approve President Biden, know \\(p\\) exactly. Alas, can’t . use posterior probability distribution summarize beliefs true value \\(p\\), truth can never confirm.Continuous variables myth. Nothing can represented computer truly continuous. Even something appears continuous, like \\(p\\), actually can take (large) set discrete variables. case, approximately 300 million possible true values \\(p\\), one total number people approve President Biden.Continuous variables myth. Nothing can represented computer truly continuous. Even something appears continuous, like \\(p\\), actually can take (large) set discrete variables. case, approximately 300 million possible true values \\(p\\), one total number people approve President Biden.math continuous probability distributions can tricky. Read book mathematical probability messy details. Little matters applied work.math continuous probability distributions can tricky. Read book mathematical probability messy details. Little matters applied work.important difference , discrete distributions, makes sense estimate probability specific outcome. probability rolling 9? continuous distributions, makes sense infinite number possible outcomes. continuous variables, estimate intervals.important difference , discrete distributions, makes sense estimate probability specific outcome. probability rolling 9? continuous distributions, makes sense infinite number possible outcomes. continuous variables, estimate intervals.Don’t worry distinctions discrete continuous outcomes, discrete continuous probability distributions use summarize beliefs outcomes. basic intuition cases.","code":"\ntibble(height = rnorm(10000, mean = 175, sd = 9)) %>% \n  ggplot(aes(x = height)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                 binwidth = 1, \n                 color = \"white\")+\n  labs(title = \"Mathematical Probability Distribution\",\n       subtitle = \"Expectations for an random male's height\",\n       x = \"Height\",\n       y = \"Probability\") +\n  scale_y_continuous(labels =\n                      scales::percent_format(accuracy = 1)) +\n  theme_classic()\nnhanes %>%\n  filter(gender == \"Male\", age >= 18) %>%\n  select(height)%>%\n  drop_na() %>%\n  ggplot(aes(x = height)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                 binwidth = 1, \n                 color = \"white\")+\n  labs(title = \"Empiracal Probability Distribution\",\n       subtitle = \"Height for male by NHANES\",\n       x = \"Height\",\n       y = \"Probability\",caption = \"Source:NHANES\") +\n  scale_y_continuous(labels =\n                      scales::percent_format(accuracy = 1)) +\n  theme_classic()\ntibble(height = c(rep(165, 2), rep(167, 3), rep(170, 8), \n                  rep(172, 9), rep(176, 9), rep(177, 8),\n                  rep(179, 10), rep(182, 18), rep(183, 23), \n                  rep(189, 20),rep(193, 21), rep(200, 15),rep(205, 15),rep(212, 8),rep(220, 4)))%>% \n    ggplot(aes(x = height)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 5, \n                   color = \"white\")+\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"One's belief about NBA baskectball players\",\n         x = \"Height\",\n         y = \"Probability\") +\n    scale_y_continuous(labels =\n                           scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"probability.html","id":"unnormalized-distributions","chapter":"5 Probability","heading":"5.2.5 Unnormalized distributions","text":"Remember probability distributions mathematical objects cover set outcomes, outcome domain mapped probability value 0 1 inclusive sum mappings 1. Sometimes, may see distributions similar probability distributions, y-axis displays raw counts instead proportions. Unnormalized distributions probability distributions, easy convert two. simply divide outcome counts y-axis sum outcome counts “normalize” unnormalized distribution. Unnormalized distributions often intermediary step; sometimes handy work counts end.instance, can generate following unnormalized distribution sum rolling two dice. (uses code , without normalization step.)Notice shape distribution empirical probability distribution generated earlier, except y-axis labeled differently.two plots — unnormalized normalized — exact shape. many ways, object. Yet normalization required want work probability distribution.","code":""},{"path":"probability.html","id":"joint-distributions","chapter":"5 Probability","heading":"5.2.6 Joint distributions","text":"Recall \\(\\text{Prob}(\\text{coin})\\) probability distribution result coin toss. includes two parts, probability heads (\\(\\rho_h\\)) probability tails (\\(\\rho_t\\)). univariate distribution one outcome, can heads tails. one outcome, joint distribution.Joint distributions also mathematical objects cover set outcomes, distinct outcome chance occurring 0 1 sum chances must equal 1. key joint distribution measures chance events B occur. notation \\(\\text{Prob}(, B)\\).Let’s say rolling two six-sided dice simultaneously. Die 1 weighted 50% chance rolling 6 10% chance values. Die 2 weighted 50% chance rolling 5 10% chance rolling values. Let’s roll dice 1,000 times. previous examples involving two dice, cared sum results outcomes first versus second die simulation. joint distributions, order matters; instead 11 possible outcomes x-axis distribution plot (ranging 2 12), 36. Furthermore, 2D probability distribution sufficient represent variables involved, joint distribution example displayed using 3D plot.","code":""},{"path":"probability.html","id":"working-with-probability-distributions","chapter":"5 Probability","heading":"5.2.7 Working with probability distributions","text":"\nFIGURE 5.4: Bruno de Finetti, Italian statistician wrote famous treatise theory probability began statement “PROBABILITY EXIST.” probability exists subjectively minds.\nprobability distribution always easy work . complex object. , many contexts, don’t really care complexity. , instead providing full probability distribution, often just use summary measure, number two three captures aspects entire distribution relevant matter hand. Let’s explore issues using 538 posterior probability distribution, August 13, 2020, number electoral votes won Joe Biden. tibble 1,000,000 draws distribution:distribution sample draws distribution different things. , squint, sort thing, least purposes. example, want know mean distribution, mean draws fairly good estimate, especially number draws large enough.Recall Chapter 2 can draw randomly specified probability distributions:elements vectors “draws” specified probability distributions. applied situations, tools produce draws rather summary objects. Fortunately, vector draws easy work . Start summary statistics:Calculate 95% interval directly:Approximate 95% interval two ways:case, using mean standard deviation produces 95% interval closer true interval. cases, median scaled median absolute deviation better. Either approximation generally “good enough” work. , need know exact 95% interval, must use quantile().","code":"\ndraws## # A tibble: 1,000,000 x 2\n##       ID electoral_votes\n##    <int>           <int>\n##  1     1             268\n##  2     2             390\n##  3     3             434\n##  4     4             249\n##  5     5             423\n##  6     6             350\n##  7     7             353\n##  8     8             240\n##  9     9             176\n## 10    10             229\n## # … with 999,990 more rows\nrnorm(10)##  [1]  1.12  0.77  0.78 -0.11 -2.42  2.66  0.14 -0.69  0.28 -0.25\nrunif(10)##  [1] 0.3883 0.9125 0.5160 0.2306 0.4914 0.4101 0.2344 0.0019 0.7475 0.4324\nkey_stats <- draws %>% \n  summarize(mn = mean(electoral_votes),\n            md = median(electoral_votes),\n            sd = sd(electoral_votes),\n            mad = mad(electoral_votes))\n\nkey_stats## # A tibble: 1 x 4\n##      mn    md    sd   mad\n##   <dbl> <dbl> <dbl> <dbl>\n## 1  325.   326  86.9  101.\nquantile(draws$electoral_votes, probs = c(0.025, 0.975))##  2.5% 97.5% \n##   172   483\nc(key_stats$mn - 2 * key_stats$sd, \n  key_stats$mn + 2 * key_stats$sd)## [1] 152 499\nc(key_stats$md - 2 * key_stats$mad, \n  key_stats$md + 2 * key_stats$mad)## [1] 124 528"},{"path":"probability.html","id":"tree-diagrams","chapter":"5 Probability","heading":"5.3 Tree diagrams","text":"","code":""},{"path":"probability.html","id":"independence","chapter":"5 Probability","heading":"5.3.1 Independence","text":"far, learned explore \\(\\text{Prob}()\\), fancy, statistical way saying probability distribution event . Keep mind distinction individual outcome set \\(\\), like probability heads (\\(\\rho_h\\)) entire distribution \\(\\text{Prob}(\\text{coin})\\), includes probability possible outcomes. \\(\\text{Prob}()\\) like \\(\\text{Prob}(\\text{coin})\\) \\(\\text{Prob}(\\text{sum two dice})\\).flipped two coins row, one ? know probability getting heads first coin 1/2. odds getting heads two times row? Let’s take look tree diagram. read diagram left right. left, probability getting heads 0.5 first toss. , tree branches .got heads first time, go top branch. probability getting heads 0.5.got tails first time, go bottom branch. probability getting heads 0.5.Notice regardless get first time flip coin, probability getting heads 0.5 throughout. Coin flips, scenario, independent. result one coin flip impact next coin flip.","code":""},{"path":"probability.html","id":"conditional-probability","chapter":"5 Probability","heading":"5.3.2 Conditional probability","text":"Imagine 60% people community disease. doctor develops test determine random person disease. However, test isn’t 100% accurate. 80% probability correctly returning positive person disease 90% probability correctly returning negative person disease.probability random person disease 0.6. Since person either disease doesn’t (two possibilities), probability person disease \\(1 - 0.6 = 0.4\\).Now tree branches .random person disease, go top branch. probability infected person testing positive 0.8 test 80% sure correctly returning positive person disease.logic, random person disease, go bottom branch. probability person incorrectly testing positive 0.1.decide go top branch random person disease. go bottom branch . called conditional probability. probability testing positive dependent whether person disease.express statistical notation? \\(\\text{Prob}(|B)\\) thing probability given B. \\(\\text{Prob}(|B)\\) essentially means probability know sure value B. Note \\(\\text{Prob}(|B)\\) thing \\(\\text{Prob}(B|)\\).","code":""},{"path":"probability.html","id":"two-models","chapter":"5 Probability","heading":"5.4 Two models","text":"simplest possible setting inference involves two models — meaning two possible states world — two outcomes experiment. Imagine disease — Probophobia, irrational fear probability — either don’t . don’t know diseases, assume two possibilities.also test 50% accurate giving test person Probophobia, 99% accurate people don’t. experiment, one two possible outcomes: positive negative.Question: test positive, probability Probophobia?generally, estimating conditional probability. Conditional outcome postive test, probability Probophobia? Mathematically, want:\\[ \\text{Prob}(\\text{Disease | Test = Postive} ) \\]answer question, need use tools joint conditional probability learned earlier Chapter. begin building, hand, joint distribution possible models (Probophobia ) possible outcomes (test positive negative). Building joint distribution involves assuming model true creating distribution outcomes might occur assumption true.example, assume Probophobia. 50% chance test positive 50% chance test negative. Similarly, assume second model true — don’t Probophobia — 1% chance test positive 99% chance negative. course, (individual) know sure happening. know disease. know test show. can use relationships construct joint distribution.first step simply create tibble consists simulated data need plot distribution. Keep mind setting two different probabilities completely separate want keep two probabilities disease results two two columns can graph using ggplot() function. ’s used rep seq functions creating table, used seq function set sequence wants, case two numbers, 0.01 (99% accuracy testing negative disease, therefore 1% testing positive disease) 0.5 (50% accuracy testing positive/negative disease), used rep functions repeat process 10,000 times probability, total 20,000 times. Note number “20,000” also represent population simulated data, simulated 20,000 results testing, 10,000 results -disease group 10,000 -disease group, often use capital N represent population, simulated data N=20,000.Plot joint distribution:joint distribution displayed 3D. Instead using “jitter” feature R unstack dots, using 3D plot visualize number dots box. number people correctly test negative far greater categories. 3D plot shows total number cases section (True positive, True negative, False positive, False negative),3D bar coming combinations. Now,pay attention two rows 3D graph, trying add length 3D bar top two sections bottom two sections, equal , 10,000 case. simulate experience two independent separate world one -disease world one -disease world.Section called “Two Models” , person, two possible states world: disease disease. assumption, outcomes. call two possible states world “models,” even though simple models.addition two models, two possible results experiment given person: test positive test negative. , assumption. allow outcome. coming sections, look complex situations consider two models two possible results experiment. meantime, built unnormalized joint distribution models results. key point! Look back earlier Chapter discussions unnormalized distributions joint distributions.want analyze plots looking different slices. instance, let’s say tested positive disease. Since test always accurate, 100% certain . isolate slice test result equals 1 (meaning positive).people test positive infected result common diseases like cold. can easily create unnormalized conditional distribution :filter() transforms joint distribution conditional distribution.Turn unnormalized distribution posterior probability distribution:Now recalled question asked start session:\ntest positive, probability Probophobia?looking posterior graph just create, can answer question easily:\npositive test, can almost 100% sure Probophobia.Stat 110 Animations video really good job explaining similar concepts.","code":"\n# Pipes generally start with tibbles, so we start with a tibble which just\n# includes an ID variable. We don't really use ID. It is just handy for getting\n# organized. We call this object `jd_disease`, where the `jd` stands for\n# joint distribution.\n\njd_disease <- tibble(p = rep(seq(0.01, 0.5, 0.49), 10000)) %>%\n  mutate(disease = map_int(p, ~ rbinom(n = 1, size = 1, p = .)))\n\n\njd_disease## # A tibble: 20,000 x 2\n##        p disease\n##    <dbl>   <int>\n##  1  0.01       0\n##  2  0.5        0\n##  3  0.01       0\n##  4  0.5        0\n##  5  0.01       0\n##  6  0.5        0\n##  7  0.01       0\n##  8  0.5        0\n##  9  0.01       0\n## 10  0.5        1\n## # … with 19,990 more rows\njd_disease %>% \n  ggplot(aes(x = as.factor(disease), \n             y = as.factor(p))) +\n  geom_point() +\n  geom_jitter(alpha = .2) +\n  labs(title = \"Unnormalized Distribution of Test Results and Disease Status\",\n       subtitle = \"Many False results in have disease than no disease\",\n       x = \"Test Result\",\n       y = \"Disease Status\") +\n  scale_x_discrete(breaks = c(0,1), \n                   labels = c(\"Negative\", \"Positive\")) +\n  scale_y_discrete(breaks = c(0.01, 0.5), \n                   labels = c(\"No Disease\", \"Have Disease\")) +\n  theme_classic()\njd_disease %>% \n  filter(disease == 1)## # A tibble: 5,169 x 2\n##        p disease\n##    <dbl>   <int>\n##  1   0.5       1\n##  2   0.5       1\n##  3   0.5       1\n##  4   0.5       1\n##  5   0.5       1\n##  6   0.5       1\n##  7   0.5       1\n##  8   0.5       1\n##  9   0.5       1\n## 10   0.5       1\n## # … with 5,159 more rows\njd_disease %>% \n  filter(disease == 1) %>% \n  ggplot(aes(p)) +\n    geom_bar() +\n    labs(title = \"Disease Status Given Positive Test\",\n         subtitle = \"Almost all of the cases were infected\",\n         x = \"Disease Status\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0.01, 0.5),\n                       labels = c(\"Healthy\", \"Infected\")) +\n  theme_classic()\njd_disease %>% \n  filter(disease == 1) %>% \n  ggplot(aes(p)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.25, \n                   color = \"white\") +\n    labs(title = \"Posterior for Probophobia Conditional on Positive Test\",\n         x = \"Probophobia Status\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = c(0.01, 0.5),\n                       labels = c(\"Healthy\", \"Infected\")) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()  "},{"path":"probability.html","id":"three-models","chapter":"5 Probability","heading":"5.5 Three models","text":"Imagine friend gives bag two marbles. either two white marbles, two black marbles, one color. Thus, bag contain 0% white marbles, 50% white marbles, 100% white marbles. Respectively, proportion, \\(p\\), white marbles 0, 0.5, 1.Question: chance bag contains exactly two white marbles, given selected white marbles three times, everytime select white marble?\\[ \\text{Prob}(\\text{2 Marbles bag | White Marble Select=3} ) \\]\nJust Probophobia models, order answer question, need start simulated data graphing joint distribution sinerio need considered possible outcomes model, based joint distribution can slice part want (Conditional distribution) end making posterior graph well normalizing see probability.Step 1: Simulate data tibbleLet’s say take marble bag, record whether ’s black white, return bag. repeat three times, observing number white marbles see three trials. get three whites, two whites, one white, zero whites result trial. three models (three different proportions white marbles bag) four possible experimental results. Let’s create 3,000 draws joint distribution:Step 2: Plot joint distribution:3D visualization:y-axes scatterplot 3D visualization labeled “Number White Marbles Bag.” value y-axis model, belief world. instance, model 0, white marbles bag, meaning none marbles pull sample white.Now recalls question, essentially care fourth column joint distribution (x-axis=3) question asking us create conditional distribution given fact 3 marbles selected. Therefore, isolate slice result simulation involves three white marbles zero black ones. unnormalized probability distribution.Step 3: Plot unnormalized conditional distribution.Step 4: Plot normalize posterior distribution.\nNext, let’s normalize distribution.plot makes sense three marbles draw bag white, pretty good chance black marbles bag. can’t certain! possible draw three white even bag contains one white one black. However, impossible zero white marbles bag.Lastly let’s answer question:\nchance bag contains exactly two white marbles, given selected white marbles three times, everytime select white marble?Answer:\nPosterior Probability Distribution shows (x-axis=2), chance bag contains exactly two white marbles given select 3 white marbles three tries 85%.","code":"\n# Create the joint distribution of the number of white marbles in the bag\n# (in_bag) and the number of white marbles pulled out in the sample (in_sample),\n# one-by-one. in_bag takes three possible values: 0, 1 and 2, corresponding to\n# zero, one and two white marbles potentially in the bag.\n\nsims <- 10000\n\n# We also start off with a tibble. It just makes things easier\n\njd_marbles <- tibble(ID = 1:sims) %>% \n  \n  # For each row, we (randomly!) determine the number of white marbles in the\n  # bag. We do not know why the `as.integer()` hack is necessary. Shouldn't\n  # `map_int()` automatically coerce the result of `sample()` into an integer?\n  \n  mutate(in_bag = map_int(ID, ~ as.integer(sample(c(0, 1, 2), \n                                                  size = 1)))) %>%\n  \n  # Depending on the number of white marbles in the bag, we randomly draw out 0,\n  # 1, 2, or 3 white marbles in our experiment. We need `p = ./2` to transform\n  # the number of white marbles into the probability of drawing out a white\n  # marble in a single draw. That probability is either 0%, 50% or 100%.\n  \n  mutate(in_sample = map_int(in_bag, ~ rbinom(n = 1, \n                                              size = 3, \n                                              p = ./2))) \n\njd_marbles## # A tibble: 10,000 x 3\n##       ID in_bag in_sample\n##    <int>  <int>     <int>\n##  1     1      0         0\n##  2     2      1         1\n##  3     3      2         3\n##  4     4      2         3\n##  5     5      0         0\n##  6     6      1         2\n##  7     7      0         0\n##  8     8      1         3\n##  9     9      1         1\n## 10    10      1         2\n## # … with 9,990 more rows\n# The distribution is unnormalized. All we see is the number of outcomes in each\n# \"bucket.\" Although it is never stated clearly, we are assuming that there is\n# an equal likelihood of 0, 1 or 2 white marbles in the bag.\n\njd_marbles %>%\n  ggplot(aes(x = in_sample, y = in_bag)) +\n    geom_jitter(alpha = 0.5) +\n    labs(title = \"Black and White Marbles\",\n         subtitle = \"More white marbles in bag mean more white marbles selected\",\n         x = \"White Marbles Selected\",\n         y = \"White Marbles in the Bag\") +\n    scale_y_continuous(breaks = c(0, 1, 2)) +\n  theme_classic()\n# The key step is the filter. Creating a conditional distribution from a joint\n# distribution is the same thing as filtering that joint distribution for a\n# specific value. A conditional distribution is a \"slice\" of the joint\n# distribution, and we take that slice with filter().\n\njd_marbles %>% \n  filter(in_sample == 3) %>% \n  ggplot(aes(in_bag)) +\n    geom_histogram(binwidth = 0.5, color = \"white\") +\n    labs(title = \"Unnormalized Conditional Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Count\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    theme_classic()\njd_marbles %>% \n  filter(in_sample == 3) %>% \n  ggplot(aes(in_bag)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Probability\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"probability.html","id":"n-models","chapter":"5 Probability","heading":"5.6 N models","text":"Assume coin \\(\\rho_h\\). guarantee 11 possible values \\(\\rho_h\\): \\(0, 0.1, 0.2, ..., 0.9, 1\\). words, 11 possible models, 11 things might true world. just like situations previously discussed, except models consider.going run experiment flip coin 20 times record number heads. result tell value \\(\\rho_h\\)? Ultimately, want calculate posterior distribution \\(\\rho_h\\), written p(\\(\\rho_h\\)).Question: likely probability certain coin getting exactly 8 heads 20 tosses. ?start, useful consider things might happen , example, \\(\\rho_h = 0.4\\). Fortunately, R functions simulating random variables makes easy.First, notice many different things can happen! Even know, certain, \\(\\rho_h = 0.4\\), many outcomes possible. Life remarkably random. Second, likely result experiment 8 heads, expect. Third, transformed raw counts many times total appeared probability distribution. Sometimes, however, convenient just keep track raw counts. shape figure cases.Either way, figures show happened model — \\(\\rho_h = 0.4\\) — true.can thing 11 possible models, calculating happen true. somewhat counterfactual since one can true. Yet assumption allow us create joint distribution models might true data experiment might generate. Let’s simplify p(models, data), although keep precise meaning mind.3D version plot.diagrams, see 11 models 21 outcomes. don’t really care p(\\(models\\), \\(data\\)), joint distribution models--might--true data---experiment-might-generate. Instead, want estimate \\(p\\), unknown parameter determines probability coin come heads tossed. joint distribution alone can’t tell us . created joint distribution even conducted experiment. creation, tool use make inferences. Instead, want conditional distribution, p(\\(models\\) | \\(data = 8\\)). results experiment. results tell us probability distribution \\(p\\)?answer question, simply take vertical slice joint distribution point x-axis corresponding results experiment.animation shows want joint distributions. take slice (red one), isolate , rotate look conditional distribution, normalize (change values along current z-axis counts probabilities), observe resulting posterior.part joint distribution care . aren’t interested object looks like , example, number heads 11. portion irrelevant observed 8 heads, 11. using filter function simulation tibble created, can conclude total 465 times simulation 8 heads observed.expect, time 8 coin tosses came heads, value \\(p\\) 0.4. , numerous occasions, . quite common value \\(p\\) like 0.3 0.5 generate 8 heads. Consider:Yet distribution raw counts. unnormalized density. turn proper probability density (.e., one sum probabilities across possible outcomes sums one) just divide everything total number observations.\nSolution :\nlikely value \\(\\rho_h\\) 0.4, . , much likely \\(p\\) either 0.3 0.5. 8% chance \\(\\rho_h \\ge 0.6\\).might wondering: use model? Well, let’s say toss coin 20 times get 8 heads .\nGiven result, Question: probability future samples 20 flips result 10 heads?three main ways go solving problem simulations.first wrong way assuming \\(\\rho_h\\) certain observed 8 heads 20 tosses. conclude 8/20 gives us 0.4. big problem ignoring uncertainty estimating \\(\\rho_h\\). lead us following code.Using Posterior distribution derived (wrong way) simulated data, probability results 10 head isabout 24.5%second method involves sampling whole posterior distribution vector previously created. lead following correct code.32.8%Third way sample actual distribution, small dataset just rows, includes \\(\\rho_h\\) probability \\(\\rho_h\\). also gives correct answer.Using Posterior distribution derived (right way 2nd) simulated data, probability results 10 head isabout 32% pretty close second method.may noticed, calculated value using first method, believe getting 10 heads less likely really . run casino based assumptions, lose money. important careful assumptions making. tossed coin 20 times got 8 heads. However, wrong assume \\(\\rho_h\\) = 0.4 just based result.","code":"\nsims <- 10000000\n\nodds <- tibble(sim_ID = 1:sims) %>%\n  mutate(heads = map_int(sim_ID, ~ rbinom(n = 1, size = 20, p = .4))) %>% \n  mutate(above_ten = ifelse(heads >= 10, TRUE, FALSE))\n\nodds## # A tibble: 10,000,000 x 3\n##    sim_ID heads above_ten\n##     <int> <int> <lgl>    \n##  1      1     9 FALSE    \n##  2      2     7 FALSE    \n##  3      3     5 FALSE    \n##  4      4     8 FALSE    \n##  5      5     9 FALSE    \n##  6      6     9 FALSE    \n##  7      7     8 FALSE    \n##  8      8     8 FALSE    \n##  9      9     4 FALSE    \n## 10     10     6 FALSE    \n## # … with 9,999,990 more rows\nodds %>%\n  ggplot(aes(x=heads,fill=above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Wrong Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nodds %>%\nsummarize(success = sum(above_ten)/sims)## # A tibble: 1 x 1\n##   success\n##     <dbl>\n## 1   0.245\np_draws <- tibble(p = rep(seq(0, 1, 0.1), 1000)) %>%\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) %>%\n  filter(heads == 8)\n  \nodds_2nd <- tibble(p = sample(p_draws$p, size = sims, replace = TRUE)) %>%\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) %>% \n  mutate(above_ten = ifelse(heads >= 10, TRUE, FALSE)) \n\nodds_2nd## # A tibble: 10,000,000 x 3\n##        p heads above_ten\n##    <dbl> <int> <lgl>    \n##  1   0.5     9 FALSE    \n##  2   0.4     7 FALSE    \n##  3   0.4     9 FALSE    \n##  4   0.3     7 FALSE    \n##  5   0.5    12 TRUE     \n##  6   0.3     9 FALSE    \n##  7   0.4    11 TRUE     \n##  8   0.3     8 FALSE    \n##  9   0.4    10 TRUE     \n## 10   0.4     7 FALSE    \n## # … with 9,999,990 more rows\nodds_2nd %>%\n  ggplot(aes(x = heads,fill = above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Right Way 1st)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nodds_2nd %>%\nsummarize(success = sum(above_ten)/sims)## # A tibble: 1 x 1\n##   success\n##     <dbl>\n## 1   0.328\np_posterior <- jd_coin %>% \n  filter(heads == 8) %>% \n  group_by(p) %>% \n  summarize(total = n(), .groups = \"drop\") %>%\n  mutate(probs = total/sum(total))\n\nodds_3rd <- tibble(p = sample(p_posterior$p, \n                          size = sims, \n                          prob = p_posterior$probs, \n                          replace = TRUE)) %>%\n  mutate(heads = map_int(p, ~ rbinom(n = 1, \n                                     size = 20, \n                                     p = .))) %>% \n  mutate(above_ten = ifelse(heads >= 10, \n                         TRUE, \n                         FALSE)) \n\nodds_3rd## # A tibble: 10,000,000 x 3\n##        p heads above_ten\n##    <dbl> <int> <lgl>    \n##  1   0.4    10 TRUE     \n##  2   0.5    13 TRUE     \n##  3   0.4     9 FALSE    \n##  4   0.5    10 TRUE     \n##  5   0.2     3 FALSE    \n##  6   0.3     6 FALSE    \n##  7   0.4     4 FALSE    \n##  8   0.5     9 FALSE    \n##  9   0.5     9 FALSE    \n## 10   0.3     8 FALSE    \n## # … with 9,999,990 more rows\nodds_3rd %>%\n  ggplot(aes(x = heads,fill = above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Right Way 3rd)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nodds_3rd %>%\nsummarize(success = sum(above_ten)/sims)## # A tibble: 1 x 1\n##   success\n##     <dbl>\n## 1   0.320"},{"path":"probability.html","id":"cardinal-virtues","chapter":"5 Probability","heading":"5.7 Cardinal Virtues","text":"four Cardinal Virtues Wisdom, Justice, Courage, Temperance. data science , ultimately, moral act, use virtues guide work.Wisdom begins ideal Preceptor Table. data, , allow us answer question easily? , explore data . data close enough data want can consider data coming population? , can’t proceed .Wisdom begins ideal Preceptor Table. data, , allow us answer question easily? , explore data . data close enough data want can consider data coming population? , can’t proceed .Justice starts Population Table – data want , data actually data population. data representative population? meaning columns consistent, .e., can assume validity? make assumption data generating mechanism. general mathematical formula connects outcome variable interested data ?Justice starts Population Table – data want , data actually data population. data representative population? meaning columns consistent, .e., can assume validity? make assumption data generating mechanism. general mathematical formula connects outcome variable interested data ?Courage allows us explore different models. Even though Justice provided basic mathematical structure model, still need decide variables include estimate values unknown parameters. avoid hypothesis tests. check models consistency data . select one model.Courage allows us explore different models. Even though Justice provided basic mathematical structure model, still need decide variables include estimate values unknown parameters. avoid hypothesis tests. check models consistency data . select one model.Temperance guides us use model created answer questions began . create posteriors quantities interest. modest claims make. posteriors create never “truth.” assumptions made create model never perfect. Yet decisions made flawed posteriors almost always better decisions made without .Temperance guides us use model created answer questions began . create posteriors quantities interest. modest claims make. posteriors create never “truth.” assumptions made create model never perfect. Yet decisions made flawed posteriors almost always better decisions made without .","code":""},{"path":"probability.html","id":"wisdom","chapter":"5 Probability","heading":"5.7.1 Wisdom","text":"\nFIGURE 5.5: Wisdom.\nWisdom helps us decide can even hope answer question data .First, start Preceptor Table. rows columns data need , , calculation quantity interest trivial? want know average height adult India, Preceptor Table include row adult column height. missing data, average easy determine, wide variety estimands, unknown numbers.One key aspect Preceptor Table whether need one potential outcome order calculate estimand. example, want know causal effect exposure Spanish-speakers attitude toward immigration need causal model, one estimates attitude treatment control. Preceptor Table require two columns outcome. , hand, want predict someone’s attitude, compare one person’s attitude another’s, need Preceptor Table one column outcome.modeling (just) prediction (also) modeling causation? Predictive models care nothing causation. Causal models often also concerned prediction, means measuring quality model.Every model predictive, sense , give new data — drawn population — can create predictive forecast. subset models causal, meaning , given individual, can change value one input figure new output , , calculate causal effect looking difference two potential outcomes.prediction, care forecasting Y given X -yet-unseen data. notion “manipulation” models. don’t pretend , Joe, turn variable X value 5 value 30 just turning knob , , cause Joe’s value Y change 17 23. can compare two people (two groups people), one X equal 5 one X equal 30, see differ Y. basic assumption predictive models one possible Y Joe. , assumption, two possible values Y, one X equal 5 another X equals 30. Preceptor Table single column Y.causal inference, however, can consider case Joe \\(X = 5\\) Joe \\(X = 30\\). mathematical model can used. models can used prediction, estimating value Y yet-unseen observation specified value X. , case, instead single column Preceptor Table Y, least two (possibly many) columns, one potential outcomes consideration.difference prediction models causal models former one column outcome variable latter one.Second, look data perform exploratory data analysis, EDA. can never look data much. important variable one want understand/explain/predict. models create later chapters, variable go lefthand side mathematical equations. academic fields refer “dependent variable.” Others use terms like “regressor” “outcome.” Whatever terminology, need explore distribution variable, min/max/range, mean median, standard deviation, .Gelman, Hill, Vehtari (2020) write:important data analyzing map research question trying answer. sounds obvious often overlooked ignored can inconvenient. Optimally, means outcome measure accurately reflect phenomenon interest, model include relevant predictors, model generalize cases applied.example, regard outcome variable, model incomes necessarily tell patterns total assets. model test scores necessarily tell child intelligence cognitive development. …care variables well, especially correlated/connected outcome variable. time spend looking variables, likely create useful model.Third, key concept “population.” need data want — Preceptor Table — data similar enough can consider come statistical population. Wikipedia:statistics, population set similar items events interest question experiment. statistical population can group existing objects (e.g. set stars within Milky Way galaxy) hypothetical potentially infinite group objects conceived generalization experience (e.g. set possible hands game poker).assume data drawn population data Preceptor Table, can use information former make inferences latter. can combine Preceptor Table data single hypothetical data set. can’t , can’t assume two sources come population, can’t use data answer questions. choice walk away. heart Wisdom knowing walk away. John Tukey noted:combination data aching desire answer ensure reasonable answer can extracted given body data.","code":""},{"path":"probability.html","id":"justice","chapter":"5 Probability","heading":"5.7.2 Justice","text":"\nFIGURE 5.6: Justice.\nWisdom, Population Table. includes rows data data want . missing values, importantly potential outcomes observed. central problem inference fill question marks Population Table.three key aspects Justice: data generating mechanism, representativeness validity.First, data generating mechanism mathematical formula, associated error term, relates outcome variable covariates.Justice requires math. Consider model coin-tossing:\\[ H_i  \\sim B(\\rho_H, n = 20) \\]\ntotal number \\(H\\) Heads experiment \\(\\) 20 flips single coin, \\(H_i\\), distributed binomial \\(n = 20\\) unknown probability \\(\\rho_h\\) coin coming Heads.Note:cheat simplification! Bayesians specified full Bayesian machinery. really need priors unknown parameter \\(\\rho_h\\) well. complex introductory class, wave hands, accept default sensible parameters built R packages use point readers advanced books, like Gelman, Hill, Vehtari (2020).cheat simplification! Bayesians specified full Bayesian machinery. really need priors unknown parameter \\(\\rho_h\\) well. complex introductory class, wave hands, accept default sensible parameters built R packages use point readers advanced books, like Gelman, Hill, Vehtari (2020).Defining \\(\\rho_h\\) “probability coin comes Heads” bit fudge. calculate hand compare tools produce, won’t . Instead, calculated value closer zero. ? \\(\\rho_h\\) really “long-run percentage time coin comes Heads.” just percentage experiment.Defining \\(\\rho_h\\) “probability coin comes Heads” bit fudge. calculate hand compare tools produce, won’t . Instead, calculated value closer zero. ? \\(\\rho_h\\) really “long-run percentage time coin comes Heads.” just percentage experiment.simple case, fortunate parameter \\(\\rho_h\\) (mostly!) simple analog real world quantity. Much time, parameters easy interpret. complex model, especially one interaction terms, focus less parameters actual predictions.simple case, fortunate parameter \\(\\rho_h\\) (mostly!) simple analog real world quantity. Much time, parameters easy interpret. complex model, especially one interaction terms, focus less parameters actual predictions.Wisdom allowed us assume rows actual Preceptor Table come population. true, also true mathematical relationship outcomes covariates consistent across rows.Wisdom allowed us assume rows actual Preceptor Table come population. true, also true mathematical relationship outcomes covariates consistent across rows.Second, check “representative” data entire population.Third, confirm data accurately capturing concepts care . data valid, given problem trying solve?Validity columns tibble. Representativeness rows.","code":""},{"path":"probability.html","id":"courage","chapter":"5 Probability","heading":"5.7.3 Courage","text":"\nFIGURE 5.7: Courage.\nthree languages data science words, math code, important code. need explain structure model using three languages, need Courage implement model code.Courage requires us take general mathematical formula provide Justice make specific. variables include model exclude? Every data science project involves creation several models. , specify precise data generating mechanism. Using formula, R code, create fitted model. models parameters. can never know true values parameters, can create, explore, posterior distributions unknown true values.Code allows us “fit” model estimating values unknown parameters, like \\(\\rho_h\\). Sadly, can never know true values parameters. , like good data scientists, can express uncertain knowledge form posterior probability distributions. distributions, can compare actual values outcome variable “fitted” “predicted” results model. can examine “residuals,” difference fitted actual values.Every outcome sum two parts: model model:\\[outcome = model + \\ \\ \\ \\ \\ model\\]doesn’t matter outcome . result coin flip, weight person, GDP country. Whatever outcome considering always made two parts. first model created. second stuff — blooming buzzing complexity real world — part model.uncertainty driven ignorance \\(\\rho_h\\).parameter something exist real world. (, , data.) Instead, parameter mental abstraction, building block use help us accomplish true goal: replace least questions marks actual Preceptor Table. Since parameters mental abstractions, always uncertain value, however much data might collect., often , uncertainty comes forces , assumption, model. example, coin fair, expect \\(T_i\\) equal 10. , often, different, even correct \\(\\rho_h\\) equals exactly 0.5.randomness intrinsic fallen world.","code":""},{"path":"probability.html","id":"temperance","chapter":"5 Probability","heading":"5.7.4 Temperance","text":"\nFIGURE 5.8: Temperance.\nimportant concepts statistics data science “Data Generating Mechanism.” data — data collect see — generated complexity confusion world. God’s mechanism brought data us. job build model process, create, computer, mechanism generates fake data consistent data see. DGM, can answer question might . particular, DGM, provide predictions data seen estimates uncertainty associated predictions. Courage helped us create DGM. Temperance guide us use.created (checked) model, now use model answer questions. Models made use, beauty. world confronts us. Make decisions must. decisions better ones use high quality models help make .Sadly, models never good like . First, world intrinsically uncertain.\nFIGURE 5.9: Donald Rumsfeld.\nknown knowns. things know know. also know known unknowns. say, know things know. also unknown unknowns, ones know know. – Donald RumsfeldWhat really care data haven’t seen yet, mostly data tomorrow. world changes, always ? doesn’t change much, maybe OK. changes lot, good model ? general, world changes . means forecasts uncertain naive use model might suggest.\nFIGURE 5.10: Three Card Monte.\nmean? Well imagine crowd playing Three Card Monte streets New York. guy running game runs demo shows cards make confident. earn money making overconfident persuading bet. odds may seem good demo round, doesn’t actually say anything likely happen real, high stakes game begins. person running game many simulations, making “victim” forget actually make conclusions odds winning. variables simply know even put lot effort making posterior probability distributions. People can using slight hand, instance.need patience order study understand unknown unknowns data. Patience also important analyze “realism” models. created mathematical probability distribution presidential elections, instance, assumed Democratic candidate 50% chance winning vote electoral college. comparing mathematical model empirical cases, however, recognize mathematical model unlikely true. mathematical model suggested getting fewer 100 votes next impossible, many past Democratic candidates empirical distribution received less 100 electoral votes.Temperance, key distinction true posterior distribution — call “Preceptor’s Posterior” — estimated posterior distribution. Recall discussion Section 2.8. Imagine every assumption made Wisdom Justice correct, correctly understand every aspect world works. still know unknown value trying estimate — recall Fundamental Problem Causal Inference — posterior created perfect. Preceptor’s Posterior. Sadly, even estimated posterior , close Preceptor’s Posterior, can never sure fact, can never know truth, never certain assumptions made correct.Even worse, must always worry estimated posterior, despite work put creating , far truth. , therefore, must cautious use posterior, humble claims accuracy. Using posterior, despite fails, better using . Yet , best, distorted map reality, glass must look darkly. Use posterior humility.","code":""},{"path":"probability.html","id":"summary-5","chapter":"5 Probability","heading":"5.8 Summary","text":"Validity columns tibble. Representativeness rows.Throughout chapter, spent time going examples conditional distributions. However, ’s worth noting probability distributions conditional something. Even simple examples, flipping coin multiple times, assuming probability getting heads versus tails change tosses.also discussed difference empirical, mathematical, posterior probability distributions. Even though developed heuristics better understand distributions, every time make claim world, based beliefs - think world. wrong. beliefs can differ. Two reasonable people can conflicting beliefs fairness die.useful understand three types distributions concept conditional distributions, almost every probability distribution conditional posterior. can leave words future discussions, generally book. implicit.keen learn probability, video featuring Professor Gary King. great way review concepts covered chapter, albeit higher level mathematics.","code":""},{"path":"one-parameter.html","id":"one-parameter","chapter":"6 One Parameter","heading":"6 One Parameter","text":"scene Hunger Games, dystopian novel children selected via lottery fight death. Primrose Everdeen selected urn. misfortune selected? , data scientists say, sampled?Chapter 5, learned probability, framework quantifying uncertainty. chapter, learn sampling, beginning journey toward inference. sample, take units population, calculate statistics based units, make inferences unknown parameters associated population.urn certain number red certain number white beads equal size, mixed well together. proportion, \\(p\\), urn’s beads red?\nFIGURE 6.1: urn red white beads.\nOne way answer question perform exhaustive count: remove bead individually, count number red beads, count number white beads, divide number red beads total number beads. Call ratio \\(p\\), proportion red beads urn. However, long tedious process. Therefore, use sampling! Consider two questions:get 17 red beads random sample size 50 taken urn, proportion \\(p\\) beads urn red?probability, using urn, draw 8 red beads use shovel size 20?begin chapter, look real sampling activity: urn. , simulate urn example using R code. help us understand standard error ways uncertainty factors predictions. create joint distribution models data urn example. derive posterior distribution joint distribution, use posterior answer questions.\n\nUse tidyverse package.","code":"\nlibrary(tidyverse)\n# Needed for the one 3-D plot below.\n\nlibrary(rayshader)\nlibrary(rgl)"},{"path":"one-parameter.html","id":"sampling-activity","chapter":"6 One Parameter","heading":"6.1 Real sampling activity","text":"\nFIGURE 6.2: urn red white beads.\n","code":""},{"path":"one-parameter.html","id":"using-the-shovel-method-once","chapter":"6 One Parameter","heading":"6.1.1 Using the shovel method once","text":"Instead performing exhaustive count, let’s insert shovel urn remove \\(5 \\cdot 10 = 50\\) beads. taking sample total population beads.\nFIGURE 6.3: Inserting shovel urn.\n\nFIGURE 6.4: Removing 50 beads urn.\nObserve 17 50 sampled beads red thus 17/50 = 0.34 = 34% shovel’s beads red. can view proportion beads red shovel guess proportion beads red entire urn. exact exhaustive count beads urn, guess 34% took much less time energy make.Recall \\(p\\) true value proportion red beads. one \\(p\\). guesses proportion red beads known \\(\\hat{p}\\), \\(\\hat{p}\\) estimated value \\(p\\) comes taking sample. can infinite number \\(\\hat{p}\\).Imagine started activity beginning, replacing 50 beads back urn starting . remove exactly 17 red beads? Maybe?repeated activity many times? guess proportion urn’s beads red, \\(\\hat{p}\\), exactly 34% every time? Surely .Let’s repeat exercise help 33 groups friends understand value varies across 33 independent trials.","code":""},{"path":"one-parameter.html","id":"student-shovels","chapter":"6 One Parameter","heading":"6.1.2 Using the shovel 33 times","text":"33 groups friends following:Use shovel remove 50 beads .Count number red beads compute proportion 50 beads red.Return beads urn.Mix contents urn let previous group’s results influence next group’s.33 groups friends make note proportion red beads sample collected. group marks proportion 50 beads red appropriate bin hand-drawn histogram seen .\nFIGURE 6.5: Constructing histogram proportions.\nHistograms allow us visualize distribution numerical variable. particular, center values falls values vary. partially completed histogram first 10 33 groups friends’ results can seen figure .\nFIGURE 6.6: Hand-drawn histogram first 10 33 proportions.\nObserve following details histogram:low end, one group removed 50 beads urn proportion red 0.20 0.25.high end, another group removed 50 beads urn proportion 0.45 0.5 red.However, frequently occurring proportions 0.30 0.35 red, right middle distribution.distribution somewhat bell-shaped.tactile_sample_urn saves results 33 groups friends.group, given names, number red_beads obtained, corresponding proportion 50 beads red, called prop_red. also ID variable gives 33 groups unique identifier. row can viewed one instance replicated activity: using shovel remove 50 beads computing proportion beads red.Let’s visualize distribution 33 proportions using geom_histogram() binwidth = 0.05. computerized complete version partially completed hand-drawn histogram saw earlier. Setting boundary = 0.4 indicates want binning scheme one bins’ boundary 0.4. color = \"white\" modifies color boundary visual clarity.","code":"## # A tibble: 33 x 4\n##    group         red_beads prop_red    ID\n##    <chr>             <dbl>    <dbl> <int>\n##  1 Mal, Francis         17     0.34     1\n##  2 Nam, Joshua          19     0.38     2\n##  3 Mark, Ramses         21     0.42     3\n##  4 Maeve, Josh          18     0.36     4\n##  5 Morgan, Emily        21     0.42     5\n##  6 Ace, Chris           18     0.36     6\n##  7 Mia, James           15     0.3      7\n##  8 Griffin, Mary        18     0.36     8\n##  9 Yuki, Harry          21     0.42     9\n## 10 Frank, Clara         21     0.42    10\n## # … with 23 more rows\ntactile_sample_urn %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # Add scale_y_continuous with breaks by 1, as the default shows the y-axis\n  # from 1 to 10 with breaks at .5. Breaks by 1 is better for this plot, as all\n  # resulting values are integers.\n  \n  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  \n  # The call expression() is used to insert a mathematical expression, like\n  # p-hat. The paste after expression allows us to paste text prior to said\n  # expression.\n  \n  labs(x = expression(paste(\"Proportion of 50 beads that were red \", hat(p))),\n       y = \"Count\",\n       title = \"Proportions Red in 33 Samples\") "},{"path":"one-parameter.html","id":"what-did-we-just-do","chapter":"6 One Parameter","heading":"6.1.3 What did we just do?","text":"just demonstrated activity statistical concept sampling. want know proportion urn’s beads red. Performing exhaustive count red white beads time-consuming. Therefore, extracted sample 50 beads using shovel. Using sample 50 beads, estimated proportion urn’s beads red 34%.\nFIGURE 6.7: Mr. DeVito correct: sampling badass.\nMoreover, mixed beads use shovel, samples random independent. sample drawn random, samples different . samples different , obtained different proportions red observed previous histogram. example sampling variation.Section 6.2, ’ll mimic hands-sampling activity just performed computer. allow us repeat sampling exercise much 33 times, also allow us use shovels different numbers slots just 50.Afterwards, ’ll present definitions, terminology, notation related sampling Section 6.3. many disciplines, necessary background knowledge may seem confusing first. However, truly understand underlying concepts practice, ’ll able master .tie contents chapter real world, ’ll present example one common uses sampling: polls. Section 6.5.2 ’ll look particular case study: 2013 poll U.S. President Barack Obama’s popularity among young Americans, conducted Kennedy School’s Institute Politics Harvard University.\n","code":""},{"path":"one-parameter.html","id":"virtual-sampling","chapter":"6 One Parameter","heading":"6.2 Virtual sampling","text":"just performed tactile sampling activity. used physical urn beads physical shovel. hand develop intuition ideas behind sampling. section, mimic physical sampling virtual sampling, using computer.","code":""},{"path":"one-parameter.html","id":"using-the-virtual-shovel-once","chapter":"6 One Parameter","heading":"6.2.1 Using the virtual shovel once","text":"Virtual sampling requires virtual urn virtual shovel. Create tibble named urn. rows urn correspond exactly contents actual urn.Observe urn 1,000 rows, meaning urn contains\n1,000 beads. first variable ID used identification variable. None beads actual urn marked numbers. second variable color indicates whether particular virtual bead red white.Note , chapter, used variable ID two different ways: first, keep track samples drawn 33 individual teams , second, keep track \n1,000 beads virtual urn. OK! ID means concept cases.virtual urn needs virtual shovel. use slice_sample() list-column take sample 50 beads virtual urn.usual, map functions list-columns powerful confusing. str() function good way explore tibble list-column.two levels. one row tibble sample. far, drawn one sample. Within row, second level, tibble sample. tibble two variables: ID color.Let’s compute proportion beads virtual sample red. First, add column indicates number red beads sample taken shovel.work? R treats TRUE like number 1 FALSE like number 0. summing number TRUEs FALSEs equivalent summing 1’s 0’s. end, operation counts number beads color equals “red.”Second, add column total number beads. (already “know” 50, never hurts make code general.)Third, calculate proportion red:Careful readers note numb_red changing example . reason, course, block re-runs shovel exercise, getting (potentially) different number red beads. wanted number block, need use set.seed() time, always providing seed time.Let’s now perform virtual analog 33 groups students use sampling shovel!","code":"\n# set.seed() ensures that the beads in our virtual urn are always in the same\n# order. This ensures that the figures in the book match their written\n# descriptions. We want 40% of the beads to be red.\n\nset.seed(10)\n\nurn <- tibble(color = c(rep(\"red\", 400), \n                        rep(\"white\", 600))) %>%\n  \n  # sample_frac() keeps all the rows in the tibble but rearranges their order.\n  # We don't need to do this. A virtual urn does not care about the order of the\n  # beads. But we find it aesthetically pleasing to mix them up.\n  \n  sample_frac() %>% \n  mutate(ID = 1:1000) %>% \n  select(ID, color)\n\nurn  ## # A tibble: 1,000 x 2\n##       ID color\n##    <int> <chr>\n##  1     1 white\n##  2     2 white\n##  3     3 red  \n##  4     4 red  \n##  5     5 white\n##  6     6 white\n##  7     7 white\n##  8     8 white\n##  9     9 white\n## 10    10 white\n## # … with 990 more rows\n# Define ID as 1 to look at one example of drawing fifty beads. When ID is\n# called within map(), we are performing slice_sample() upon our urn once (ID =\n# 1) and taking a sample of 50 beads. Note that the ID in the main tibble has\n# nothing to do wkth the ID variable in urn. The former refers to samples from\n# the urn. The latter refers to beads in the urn.\n\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50)))## # A tibble: 1 x 2\n##      ID shovel           \n##   <dbl> <list>           \n## 1     1 <tibble [50 × 2]>\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50))) %>% \n  str()## tibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ ID    : num 1\n##  $ shovel:List of 1\n##   ..$ : tibble [50 × 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ ID   : int [1:50] 812 903 227 283 229 160 523 893 66 277 ...\n##   .. ..$ color: chr [1:50] \"white\" \"white\" \"white\" \"red\" ...\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50))) %>% \n  \n  # Using map_int, perform the sum where (within the shovel -- denoted by the\n  # period) the color is equal to red. This counts the number of red beads in\n  # the shovel.\n  \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\")))## # A tibble: 1 x 3\n##      ID shovel            numb_red\n##   <dbl> <list>               <int>\n## 1     1 <tibble [50 × 2]>       20\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color)))## # A tibble: 1 x 4\n##      ID shovel            numb_red numb_beads\n##   <dbl> <list>               <int>      <int>\n## 1     1 <tibble [50 × 2]>       23         50\ntibble(ID = 1) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)## # A tibble: 1 x 5\n##      ID shovel            numb_red numb_beads prop_red\n##   <dbl> <list>               <int>      <int>    <dbl>\n## 1     1 <tibble [50 × 2]>       18         50     0.36"},{"path":"one-parameter.html","id":"using-the-virtual-shovel-33-times","chapter":"6 One Parameter","heading":"6.2.2 Using the virtual shovel 33 times","text":"tactile sampling exercise Section 6.1, 33 groups students use shovel, yielding 33 samples size 50 beads. used 33 samples compute 33 proportions. can perform repeated/replicated sampling virtually just thing 33 times.’ll save results data frame called virtual_samples.Let’s visualize variation histogram:\nset boundary binwidth arguments. Setting boundary = 0.4 ensures binning scheme one bin’s boundary 0.4. Since binwidth = 0.05, create bins boundaries 0.30, 0.35, 0.45, . Recall \\(\\hat{p}\\) equal proportion beads red samples.Observe occasionally obtained proportions red less 30%. hand, occasionally obtained proportions greater 45%. However, frequently occurring proportions 35% 45%. differences proportions red? sampling variation.Compare virtual results tactile results previous section. Observe histograms somewhat similar center variation, although identical. slight differences due random sampling variation. Furthermore, observe distributions somewhat bell-shaped.\nFIGURE 6.8: Comparing 33 virtual 33 tactile proportions red. Note , though figures differ slightly, centered around .35 .45. shows , sampling distributions, frequently occuring proportion red 35% 45%.\nvisualization allows us see results differed tactile virtual urn results. can see, variation results. cause concern, always expected sampling variation results.","code":"\nset.seed(9)\n\nvirtual_samples <- tibble(ID = 1:33) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\nvirtual_samples## # A tibble: 33 x 5\n##       ID shovel            numb_red numb_beads prop_red\n##    <int> <list>               <int>      <int>    <dbl>\n##  1     1 <tibble [50 × 2]>       21         50     0.42\n##  2     2 <tibble [50 × 2]>       19         50     0.38\n##  3     3 <tibble [50 × 2]>       17         50     0.34\n##  4     4 <tibble [50 × 2]>       15         50     0.3 \n##  5     5 <tibble [50 × 2]>       17         50     0.34\n##  6     6 <tibble [50 × 2]>       21         50     0.42\n##  7     7 <tibble [50 × 2]>        9         50     0.18\n##  8     8 <tibble [50 × 2]>       21         50     0.42\n##  9     9 <tibble [50 × 2]>       16         50     0.32\n## 10    10 <tibble [50 × 2]>       20         50     0.4 \n## # … with 23 more rows\nvirtual_samples %>% \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # To use mathematical symbols in titles and labels, use the expression()\n  # function, as here.\n  \n  labs(x = expression(hat(p)),\n       y = \"Count\",\n       title = \"Distribution of 33 proportions red\") +\n  \n  # Label the y-axis in an attractive fashion. Without this code, the axis\n  # labels would include 2.5, which makes no sense because all the values are\n  # integers.\n  \n  scale_y_continuous(breaks = seq(2, 10, 2))"},{"path":"one-parameter.html","id":"shovel-1000-times","chapter":"6 One Parameter","heading":"6.2.3 Using the virtual shovel 1,000 times","text":"\nFIGURE 6.9: much sampling, little time.\nNow say want study effects sampling variation 33 samples, larger number samples (1000). two choices point. groups friends manually take 1,000 samples 50 beads compute corresponding 1,000 proportions. However, time-consuming. computers excel: automating long repetitive tasks performing quickly. point, abandon tactile sampling favor virtual sampling.Observe now 1,000 replicates prop_red, proportion 50 beads red. Using code earlier, let’s now visualize distribution 1,000 replicates prop_red histogram:frequently occurring proportions red beads occur, , 35% 45%. Every now , obtain proportions much lower higher. rare, however. Observe now much symmetric smoother bell-shaped distribution. shape , fact, well approximated normal distribution.empty spaces among bars? Recall , 50 beads, 51 possible values \\(\\hat{p}\\): 0, 0.02, 0.04, …, 0.98, 1. value 0.03 0.39 impossible, hence gaps.","code":"\nset.seed(9)\n\nvirtual_samples <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\nvirtual_samples## # A tibble: 1,000 x 5\n##       ID shovel            numb_red numb_beads prop_red\n##    <int> <list>               <int>      <int>    <dbl>\n##  1     1 <tibble [50 × 2]>       21         50     0.42\n##  2     2 <tibble [50 × 2]>       19         50     0.38\n##  3     3 <tibble [50 × 2]>       17         50     0.34\n##  4     4 <tibble [50 × 2]>       15         50     0.3 \n##  5     5 <tibble [50 × 2]>       17         50     0.34\n##  6     6 <tibble [50 × 2]>       21         50     0.42\n##  7     7 <tibble [50 × 2]>        9         50     0.18\n##  8     8 <tibble [50 × 2]>       21         50     0.42\n##  9     9 <tibble [50 × 2]>       16         50     0.32\n## 10    10 <tibble [50 × 2]>       20         50     0.4 \n## # … with 990 more rows\nvirtual_samples %>% \n  ggplot(aes(x = prop_red)) +\n    geom_histogram(binwidth = 0.01, \n                   boundary = 0.4, \n                   color = \"white\") +\n    labs(x = expression(hat(p)), \n         y = \"Count\",\n         title = \"Distribution of 1,000 proportions red\") "},{"path":"one-parameter.html","id":"different-shovels","chapter":"6 One Parameter","heading":"6.2.4 The effect of different shovel sizes","text":"Instead just one shovel, imagine three choices shovels extract sample beads : shovels size 25, 50, 100. Using newly developed tools virtual sampling, let’s unpack effect different sample sizes. Start virtually using shovel 1,000 times. , compute resulting 1,000 replicates proportion red. Finally, plot distribution using histogram.repeat process shovel size 50.twice many bars size-50 shovel size-25 shovel former allows many possible values \\(\\hat{p}\\).Finally, perform process 1000 replicates map histogram using shovel size 100.easy comparison, present three resulting histograms single row matching x y axes:\nFIGURE 6.10: Comparing distributions proportion red different sample sizes (25, 50, 100). important takeaway center becomes concentrated sample size increases, indicating smaller standard deviation guesses.\nObserve sample size increases, variation 1,000 replicates proportion red decreases. words, sample size increases, fewer differences due sampling variation distribution centers tightly around value. three histograms center around roughly 40%.\n\ncan numerically explicit amount variation three sets 1,000 values prop_red using standard deviation. standard deviation summary statistic measures amount variation within numerical variable. three sample sizes, let’s compute standard deviation 1,000 proportions red running following data wrangling code uses sd() summary function. sample size increases, variation decreases. words, less variation 1,000 values proportion red. sample size increases, guesses true proportion urn’s beads red get precise. larger shovel, precise result.","code":"\nvirtual_samples_25 <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 25))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\nvirtual_samples_25 %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 25 beads that were red\", \n       title = \"25\") \nvirtual_samples_50 <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 50))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\n\nvirtual_samples_50  %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 50 beads that were red\", \n       title = \"50\")  \nvirtual_samples_100 <- tibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 100))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)\n\n\nvirtual_samples_100 %>%\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 100 beads that were red\", \n       title = \"100\") \n# Use bind_rows to combine the data from our three saved virtual sampling\n# objects. Use mutate() in each to clarify the n as the necessary number\n# of samples taken. This makes our data easier to interpret and prevents\n# duplicate elements.\n\nvirtual_prop <- bind_rows(virtual_samples_25 %>% \n                            mutate(n = 25), \n                          virtual_samples_50 %>% \n                            mutate(n = 50), \n                          virtual_samples_100 %>% \n                            mutate(n = 100))\n\n# Plot our new object with the x-axis showing prop_red. Add elements binwidth,\n# boundary, and color for stylistic clarity. Use labs() to add an x-axis label\n# and title. Facet_wrap() splits the graph into multiple plots by the variable\n# (~n).\n\ncomparing_sampling_distributions <- ggplot(virtual_prop, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of shovel's beads that are red\", \n       title = \"Comparing distributions of proportions red for three different shovel sizes.\") +\n  facet_wrap(~ n) \n\n# Inspect our new faceted graph. \n\ncomparing_sampling_distributions\nvirtual_samples_25 %>% \n  summarize(sd = sd(prop_red), .groups = 'drop_last')## # A tibble: 1 x 1\n##       sd\n##    <dbl>\n## 1 0.0987\nvirtual_samples_50 %>% \n  summarize(sd = sd(prop_red), .groups = 'drop_last')## # A tibble: 1 x 1\n##       sd\n##    <dbl>\n## 1 0.0673\nvirtual_samples_100 %>% \n  summarize(sd = sd(prop_red), .groups = 'drop_last')## # A tibble: 1 x 1\n##       sd\n##    <dbl>\n## 1 0.0453"},{"path":"one-parameter.html","id":"functions-are-your-friend","chapter":"6 One Parameter","heading":"6.2.5 Functions are your friend!","text":"Note last section, ran less code three times, different sizes shovel: 25, 50, 100. Whenever find writing code three times, write function thing. Let’s look code used shovel size 25 calculated proportion beads red one time:pipe want, creating function relatively easy. Just place code within function definition “pull ” specific variables arguments.See just uses code create virtual_prop_red_25, generalizes . Now can create tibbles , ready plot histograms, three lines code:still isn’t best way. Note three objects need deal , virtual_prop_red_25, virtual_prop_red_50, virtual_prop_red_100. Instead, let’s store results single tibble. can ? using map() create list-column!First, ’ll create tibble variable named shovel_size values (25, 50, 100):Next, ’ll create list column called prop_red_results output make_prop_red().Adding another map function let us get standard deviations estimated proportions:Now framework, ’s need limit sizes 25, 50, 100. try integers 1 100? can use code, except ’ll now set shovel_size = 1:100 initializing tibble.Now, standard deviation prop_red shovel sizes 1 100. Let’s plot value see changes shovel gets larger:\nFIGURE 6.11: Comparing standard deviations proportions red 100 different shovels. standard deviation decreases rate square root shovel size. red line shows standard error.\nred line represents important statistical concept: standard error (SE). mathematical definition SE purposes standard deviation divided square root sample size. shovel size increases, thus sample size increases, find standard error decreases. confusing right now, fear ! delve explanation next section.\n\nFIGURE 6.12: poets philosophers confused : don’t worry! won’t problem set.\npower running many analyses using map functions list columns: , tell standard deviation decreasing shovel size increased, looking shovel sizes 25, 50, 100, wasn’t clear quickly decreasing.","code":"\ntibble(ID = 1:1000) %>% \n  mutate(shovel = map(ID, ~ slice_sample(urn, n = 25))) %>% \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n  mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n  mutate(prop_red = numb_red / numb_beads)## # A tibble: 1,000 x 5\n##       ID shovel            numb_red numb_beads prop_red\n##    <int> <list>               <int>      <int>    <dbl>\n##  1     1 <tibble [25 × 2]>        8         25     0.32\n##  2     2 <tibble [25 × 2]>       12         25     0.48\n##  3     3 <tibble [25 × 2]>       14         25     0.56\n##  4     4 <tibble [25 × 2]>        9         25     0.36\n##  5     5 <tibble [25 × 2]>       11         25     0.44\n##  6     6 <tibble [25 × 2]>       10         25     0.4 \n##  7     7 <tibble [25 × 2]>        9         25     0.36\n##  8     8 <tibble [25 × 2]>       10         25     0.4 \n##  9     9 <tibble [25 × 2]>        6         25     0.24\n## 10    10 <tibble [25 × 2]>        8         25     0.32\n## # … with 990 more rows\nmake_prop_red <- function(x, shovel_size, reps){\n  tibble(ID = 1:reps) %>% \n    mutate(shovel = map(ID, ~ slice_sample(x, n = shovel_size))) %>% \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) %>% \n    mutate(numb_beads = map_int(shovel, ~ length(.$color))) %>% \n    mutate(prop_red = numb_red / numb_beads)\n}\nvirtual_prop_red_25  <- make_prop_red(x = urn, shovel_size = 25,  reps = 1000)\nvirtual_prop_red_50  <- make_prop_red(x = urn, shovel_size = 50,  reps = 1000)\nvirtual_prop_red_100 <- make_prop_red(x = urn, shovel_size = 100, reps = 1000)\ntibble(shovel_size = c(25, 50, 100))## # A tibble: 3 x 1\n##   shovel_size\n##         <dbl>\n## 1          25\n## 2          50\n## 3         100\ntibble(shovel_size = c(25, 50, 100)) %>%\n  mutate(prop_red_results = map(shovel_size,\n                                ~ make_prop_red(x = urn, \n                                           shovel_size = .x, \n                                           reps = 1000)))## # A tibble: 3 x 2\n##   shovel_size prop_red_results    \n##         <dbl> <list>              \n## 1          25 <tibble [1,000 × 5]>\n## 2          50 <tibble [1,000 × 5]>\n## 3         100 <tibble [1,000 × 5]>\nshovels <- tibble(shovel_size = c(25, 50, 100)) %>%\n  mutate(prop_red_results = map(shovel_size,\n                                ~ make_prop_red(x = urn, \n                                           shovel_size = .x, \n                                           reps = 1000))) %>% \n  \n  # Use map_dbl() to create a column that draws the standard deviations of our\n  # prop_red values. We can use pull after our ~ to isolate the prop_red values\n  # and use a pipe (as we normally do) here to draw the sd() of this column.\n  \n  \n  mutate(prop_red_sd = map_dbl(prop_red_results, \n                               ~ pull(., prop_red) %>% sd()))\n\nglimpse(shovels)## Rows: 3\n## Columns: 3\n## $ shovel_size      <dbl> 25, 50, 100\n## $ prop_red_results <list> [<tbl_df[1000 x 5]>], [<tbl_df[1000 x 5]>], [<tbl_df[…\n## $ prop_red_sd      <dbl> 0.099, 0.070, 0.046\nshovels_100 <- tibble(shovel_size = 1:100) %>%\n  mutate(prop_red_results = map(shovel_size,\n                                ~ make_prop_red(x = urn, \n                                           shovel_size = .x, \n                                           reps = 1000))) %>% \n  mutate(prop_red_sd = map_dbl(prop_red_results, \n                               ~ pull(., prop_red) %>% sd()))\n\nglimpse(shovels_100)## Rows: 100\n## Columns: 3\n## $ shovel_size      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n## $ prop_red_results <list> [<tbl_df[1000 x 5]>], [<tbl_df[1000 x 5]>], [<tbl_df…\n## $ prop_red_sd      <dbl> 0.483, 0.337, 0.271, 0.250, 0.214, 0.195, 0.191, 0.17…"},{"path":"one-parameter.html","id":"standard-errors","chapter":"6 One Parameter","heading":"6.3 Standard error","text":"\nFIGURE 6.13: Standard errors just way old people talk confidence intervals.\nStandard errors (SE) quantify effect sampling variation estimates. words, quantify much can expect calculated proportions shovel’s beads red vary one sample another sample another sample, . general rule, sample size increases, standard error decreases.standard error standard deviation sample statistic (aka point estimate), proportion. example, standard error mean refers standard deviation distribution sample means taken population.relationship standard error standard deviation , given sample size, standard error equals standard deviation divided square root sample size. Accordingly, standard error inversely proportional sample size. larger sample size, smaller standard error.sounds confusing, don’t worry! . can explain depth, important understand terminology.","code":""},{"path":"one-parameter.html","id":"terminology-and-notation","chapter":"6 One Parameter","heading":"6.3.1 Terminology and notation","text":"\nFIGURE 6.14: Let Yoda’s wisdom dull pain terminology section.\nconcepts underlying terminology, notation, definitions tie directly concepts underlying tactile virtual sampling activities. simply take time practice master .First, population set relevant units. population’s size upper-case \\(N\\). sampling activities, population collection \\(N\\) = 1,000 identically sized red white beads urn. simplest possible population. examples adult men US, classrooms school, wheelbarrows Massachusetts, values blood pressure, read five minute intervals, entire life. Often, population extends time, blood pressure readings , therefore, amorphous. Consider people run governor US state since 1900, people run governor 2050. also populations.Second, population parameter numerical summary quantity population unknown, wish knew. example, quantity mean, population parameter interest population mean. mathematically denoted Greek letter \\(\\mu\\) pronounced “mu.” earlier sampling urn activity, however, since interested proportion urn’s beads red, population parameter population proportion, denoted \\(p\\).Third, census exhaustive enumeration counting \\(N\\) units population order compute population parameter’s value exactly. sampling activity, correspond counting number beads \\(N\\) = 1000 red computing population proportion \\(p\\) red exactly. number \\(N\\) individuals observations population large case urn, census can quite expensive terms time, energy, money. census impossible populations includes future, like blood pressure next year candidates governor 2040. truth , even theory, calculate .Fourth, sampling act collecting sample population can , want , perform census. sample size lower case \\(n\\), opposed upper case \\(N\\) population’s size. Typically sample size \\(n\\) much smaller population size \\(N\\). sampling activities, used shovels varying slots extract samples size \\(n\\) = 1 \\(n\\) = 100.Fifth, point estimate, also known sample statistic, measure computed sample estimates unknown population parameter. sampling activities, recall unknown population parameter proportion red beads mathematically denoted \\(p\\). point estimate sample proportion: proportion shovel’s beads red. words, guess proportion urn’s beads red. point estimate parameter \\(p\\) \\(\\hat{p}\\). “hat” top \\(p\\) indicates estimate unknown population proportion \\(p\\).Sixth, sample said representative roughly looks like population. words, sample’s characteristics good representation population’s characteristics? sampling activity, samples \\(n\\) beads extracted using shovels representative urn’s \\(N\\) = 1000 beads?Seventh, sample generalizable results based sample can generalize population. sampling activity, can generalize sample proportion shovels entire urn? Using mathematical notation, akin asking \\(\\hat{p}\\) “good guess” \\(p\\)?Eighth, biased sampling occurs certain individuals observations population higher chance included sample others. say sampling procedure unbiased every observation population equal chance sampled. red beads much smaller white beads, therefore prone falling shovel, sample biased. sampling activities, since mixed \\(N = 1000\\) beads prior group’s sampling since equally sized beads equal chance sampled, samples unbiased.Ninth, sampling procedure random sample randomly population unbiased fashion. sampling activities, correspond sufficiently mixing urn use shovel.\nFIGURE 6.15: Fear look like Spongebob reading section. re-cap right now!\ngeneral:sampling sample size \\(n\\) done random, thenthe sample unbiased representative population size \\(N\\), thusany result based sample can generalize population, thusthe point estimate “good guess” unknown population parameter, thusinstead performing census, can draw inferences population using sampling.Specific sampling activity:extract sample \\(n=50\\) beads random, words, mix equally sized beads using shovel, thenthe contents shovel unbiased representation contents urn’s 1000 beads, thusany result based shovel’s beads can generalize urn, thusthe sample proportion \\(\\hat{p}\\) \\(n=50\\) beads shovel red “good guess” population proportion \\(p\\) \\(N=1000\\) beads red, thusinstead manually going 1,000 beads urn, can make inferences urn using results shovel.","code":""},{"path":"one-parameter.html","id":"sampling-definitions","chapter":"6 One Parameter","heading":"6.3.2 Statistical definitions","text":"Now, important statistical definitions related sampling. refresher 1,000 repeated/replicated virtual samples size \\(n\\) = 25, \\(n\\) = 50, \\(n\\) = 100 Section 6.2, let’s display figure showing difference proportions red according different shovel sizes.\nFIGURE 6.16: Previously seen three distributions sample proportion \\(\\hat{p}\\).\ntypes distributions special name: sampling distributions; visualization displays effect sampling variation distribution point estimate; case, sample proportion \\(\\hat{p}\\). Using sampling distributions, given sample size \\(n\\), can make statements values typically expect.example, observe centers three sampling distributions: roughly centered around \\(0.4 = 40\\%\\). Furthermore, observe somewhat likely observe sample proportions red beads \\(0.2 = 20\\%\\) using shovel 25 slots, almost never observe proportion 20% using shovel 100 slots. Observe also effect sample size sampling variation. sample size \\(n\\) increases 25 50 100, variation sampling distribution decreases thus values cluster tightly around center around 40%. quantified variation using standard deviation sample proportions, seeing standard deviation decreases square root sample size:\nFIGURE 6.17: Previously seen comparing standard deviations proportions red 100 different shovels\nsample size increases, standard deviation proportion red beads decreases. type standard deviation another special name: standard error","code":""},{"path":"one-parameter.html","id":"what-is-a-standard-error","chapter":"6 One Parameter","heading":"6.3.3 What is a “standard error?”","text":"“standard error” (SE) term measures accuracy sample distribution represents population use standard deviation. Specifically, SE used refer standard deviation sample statistic (aka point estimate), mean median. example, “standard error mean” refers standard deviation distribution sample means taken population.statistics, sample mean deviates actual mean population; deviation standard error mean.Many students struggle differentiate standard error standard deviation. relationship standard error standard deviation , given sample size, standard error equals standard deviation divided square root sample size. Accordingly, standard error inversely proportional sample size; larger sample size, smaller standard error statistic approach actual value.data points involved calculations mean, smaller standard error tends . standard error small, data said representative true mean. cases standard error large, data may notable irregularities. Thus, larger sample size = smaller standard error = representative truth.help reinforce concepts, let’s re-display previous figure using new sampling terminology, notation, definitions:\nFIGURE 6.18: Three sampling distributions sample proportion \\(\\hat{p}\\). Note increased concentration bins around .4 sample size increases.\nFurthermore, let’s display graph standard errors \\(n = 1\\) \\(n = 100\\) using new terminology, notation, definitions relating sampling.\nFIGURE 6.19: Standard errors sample proportion based sample sizes 1 100\nRemember key message last table: sample size \\(n\\) goes , “typical” error point estimate go , quantified standard error.","code":""},{"path":"one-parameter.html","id":"moral-of-the-story","chapter":"6 One Parameter","heading":"6.3.4 The moral of the story","text":"know two pieces information data, ? First, need measure center distribution. include mean median, shows center data points. Second, need measure variability distribution. understand center, must understand different (spread) data points one another. Thus, need measure like sd() MAD. summary statistics necessary understanding distribution. two figures encompass need know distribution? ! , allowed two numbers keep, valuable.mean median good estimate center posterior standard error mad good estimate variability posterior, +/- 2 standard errors covering 95% outcomes.standard error measures accuracy sample distribution compared population using standard deviation. Specifically, standard deviation data points divided square root sample size. , find larger sample sizes = lower standard errors = accurate representative guesses.really drive home point: standard error just fancy term uncertainty something don’t know. Standard error == (uncertain) beliefs.\nFIGURE 6.20: wondering much need know, follow helpful guide information learned chapter!\nhierarchy represents knowledge need understand standard error (SE). bottom, math. ’s foundation understanding, doesn’t need take away lesson. go , simplify topic. top pyramid basic levels understanding help remember future.know estimate plus minus two standard errors, know 95% confidence interval. valuable information. Standard error really just measure uncertain something know, thing estimating. recall SE, remember , , ’s complicated concept can distilled : way old people talk confidence intervals.\n\nRecall \\(\\hat{p}\\) estimated value p comes taking sample. can billions billions \\(\\hat{p}\\)’s. look large group \\(\\hat{p}\\)’s, create distribution results represent possible values p based findings, compute standard error account uncertainty predictions. 95% confidence interval prediction == estimate plus minus two standard errors.regards fifth layer hierarchy, may wonder:“thought MADs thing standard deviations. Now say things standard errors. ?”MADs standard deviations , less, thing. measures variability distribution. cases, similar values. standard error also standard deviation. Specifically, standard deviation distribution estimates, distribution estimates , less, posterior. Therefore, can use MAD, like standard error, describe distribution variability distribution.","code":""},{"path":"one-parameter.html","id":"answering-the-questions","chapter":"6 One Parameter","heading":"6.4 Answering the questions","text":"Recall questions asked beginning chapter:get 17 red beads random sample size 50 taken urn, proportion \\(p\\) beads urn red?probability, using urn, draw 8 red beads use shovel size 20?looked number different models accuracy precision. delve primary questions, must return discussion distributions: posterior, joint, marginal.","code":""},{"path":"one-parameter.html","id":"joint-distribution","chapter":"6 One Parameter","heading":"6.4.1 Joint distribution","text":"Let’s start first question: get 17 red beads random sample size 50 taken urn 1000 (white red) beads, many red beads urn?Chapter 5 outlined key intuition behind inference: joint distribution data---might-see---experiment models----considering. take joint distribution, combine actual results experiment, calculate posterior distribution set possible models. words, start \n\n\\[\\text{Prob}(\\text{models}, \\text{data})\\]add specific results experiment, calculate conditional distribution models, given data seen:\\[\\text{Prob}(\\text{models} | \\text{data} = \\text{results experiment})\\]rest just details.can thing sampling problem. Assume know 1,000 beads urn, either red white. experiment involves one-time use shovel 50 slots. 1,001 models consideration. might zero red beads one red bead … 999 red beads 1000 red beads. shovel size 50, 51 possible results experiment: zero red beads one red bead . . . 49 red beads 50 red beads. information can calculate joint distribution models considering experiment results might observe. joint distribution, can calculate conditional distribution total number red beads, given however many observed sample.useful define parameter, \\(p\\), proportion red beads urn. also takes exactly 1,001 possible values: \\(0, 1/1000, 2/1000, ..., 999/1000, 1\\). assumptions, can create unnormalized joint distribution:looking joint distribution, let’s plot . Call red_in_urn y-axis, indicate number red beads urn. Call red_in_shovel x-axis, indicate number red beads shovel. allows us see possible values red beads samples associated certain values number red beads urn.easier define “model” joint distribution terms unknown parameter \\(p\\) rather , , terms unknown number red beads urn. Showing plot terms \\(p\\) using term “proportion” rather “number red beads” closer trying model.Now, explored likely possibilities according possible models scenario. Let’s create posterior distribution taking slice joint distribution based number red beads shovel.","code":"\nset.seed(10)\n\n# Set values for the urn, shovel, and reps as this helps with creating our joint\n# distribution later. Using more than reps = 100 causes the 3-D plot to blow up.\n\nurn_size <- 1000\nshovel_size <- 50\nreps <- 100\n\n# Create a tibble where red_in_urn creates a sequence from 0 to urn_size(1000)\n# by a value of 1. Create p as red_in_urn divided by the total urn_size. Column\n# red_in_shovel will map p using rbinom(). Call n = reps (where reps is defined\n# as 100) and size as shovel_size (defined as 50). Finally, unnest!\n\njoint_dist <- \n  tibble(red_in_urn = seq(from = 0, to = urn_size, by = 1)) %>% \n    mutate(p = red_in_urn / urn_size) %>% \n    mutate(red_in_shovel = map(p, ~ rbinom(n = reps, \n                                               size = shovel_size, \n                                               prob = .))) %>%\n    unnest(red_in_shovel) \n\n# After making our joint distribution, take a sample of size 10.\n\njoint_dist %>% \n  slice_sample(n = 10)## # A tibble: 10 x 3\n##    red_in_urn     p red_in_shovel\n##         <dbl> <dbl>         <int>\n##  1        974 0.974            46\n##  2        957 0.957            47\n##  3        199 0.199            11\n##  4        860 0.86             46\n##  5         36 0.036             3\n##  6        221 0.221            13\n##  7        862 0.862            44\n##  8        117 0.117             3\n##  9        664 0.664            30\n## 10        944 0.944            49\njoint_dist %>%\n  ggplot(aes(y = red_in_urn, x = red_in_shovel)) +\n    geom_point(alpha = 0.005) +\n    labs(title = \"Joint Distribution of Red Beads in Shovel and in Urn\",\n         x = \"Number of Red Beads in Shovel\",\n         y = \"Number of Red Beads in Urn\")\njoint_dist %>%\n  ggplot(aes(y = p, x = red_in_shovel)) +\n    geom_point(alpha = 0.005) +\n    labs(title = expression(paste(\n      \"Joint Distribution of Red Beads in Shovel and Proportion Red Beads in Urn \", \n      hat(p))), \n         x = \"Number of Red Beads in Shovel\",\n         \n         # Add the symbol for p-hat. \n         \n         y = expression(paste(\"Proportion of Red Beads in Urn \", hat(p)))) +\n  \n  # Highlight y-axis change in red to view change. \n  \n  theme(axis.title.y = element_text(colour = \"red\"))"},{"path":"one-parameter.html","id":"posterior-distribution","chapter":"6 One Parameter","heading":"6.4.2 Posterior distribution","text":"look question – asking posterior probability certain case. case find , using 50 slot shovel, get result 17 red beads. make posterior probability? want:\\[\\text{Prob}(\\text{models} | \\text{data} = 17)\\]approach works . take “slice” joint distribution number red beads shovel equal 17. normalization, gives us posterior probability \\(p\\), number red beads urn. Recall one--one mapping \\(p\\) number red beads.value red beads found shovel (0 50), posterior distribution possible values red beads total urn. distribution meant illustrate , though sample gives estimate total proportion red beads entire urn, much variation actual proportion might . summarize: distribution hat things (things predicting) , less, posterior.Recall first question start Chapter:get 17 red beads random sample size 50 taken urn, proportion \\(p\\) beads urn red?answer single number. posterior distribution just : distribution. sample, many different results proportion red beads entire urn. Certain proportions, like extremes close 0% 100%, essentially impossible due sample value 32%. sample proportions, like 40%, occur far frequently sample proportions.means , can provide range possibilities (can estimate possibilities occur frequently), can never say know total number red beads certainty.key issue urn paradigm , unlike real world data science, can technically find true proportion. just perform exhaustive count ’ve desperately avoiding. real world, “true” solution can never known. require inference, tools like sampling helpful drawing conclusions world around us.","code":"\njoint_dist %>% \n  \n  # Filtering for where the red_in_shovel equals 17 allows us to isolate the\n  # data that pertains to our question.\n  \n  filter(red_in_shovel == 17) %>% \n  ggplot(aes(x = p)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Proportion of red beads in urn is centered around 0.34\",\n         x = \"Proportion p of Red Beads in Urn\",\n         y = \"Probability\") + \n  \n  # Here, the labels call allows us to format our axises by whether we would\n  # like them to display as numbers or percentages.\n  \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"one-parameter.html","id":"hypothesis-tests","chapter":"6 One Parameter","heading":"6.4.3 Hypothesis tests","text":"Recall view hypothesis tests: Amateurs test. Professionals summarize. Traditionally, scientific papers much interested estimating p. interested testing specific hypotheses. mean ?Let’s look possible hypothesis urn paradigm: equal number red white beads urn. null hypothesis denoted \\(H_0\\), alternative hypothesis denoted \\(H_a\\). Therefore, hypothesis designed :\\(H_0\\): equal number red white beads urn.\n\\(H_a\\): equal number red white beads urn.Can reject hypothesis? Convention: 95% confidence interval excludes null hypothesis, reject . , mean posterior estimate (plus minus 2 standard errors) excluded possibility red white beads equal, translating proportion red 50%, can reject null hypothesis. Otherwise, don’t reject . However, mean accept . point: rejecting rejecting hypotheses doesn’t helps us answer real questions. reason test can summarize providing full posterior probability distribution, .arguments apply case “insignificant” results, p > 0.5, can’t “reject” null hypothesis. fact difference “significant” relevance use posterior make decisions. reasoning applies every parameter estimate, every prediction make. Never test — unless boss demands test. Use judgment, make models, summarize knowledge world, use summary make decisions. next section, create posterior distribution use posterior predict outcomes.","code":""},{"path":"one-parameter.html","id":"using-the-posterior","chapter":"6 One Parameter","heading":"6.4.4 Using the posterior","text":"Now posterior distribution proportion \\(p\\) red beads urn, can use object forecast outcomes, outcomes yet observed. Recall, start Chapter:Conditional answer Question 1, probability, using urn, draw 8 red beads use shovel size 20?need create (unnormalized) posterior distribution number red beads new draw 20-slot shovel.Note two columns focus removing columns red_in_urn red_in_shovel. column p represents probability number red beads draw shovel size 20 (conditional upon previous draw 17 red beads). column new_reds represents number red beads found draw twenty beads corresponding value p. Scroll object look probabilities various results new_reds.unnormalized distribution, easy create normalized posterior probability distribution.calculate exact probability drawing 8 reds, can work directly unnormalized distribution.answer 26%. observe visually, take look post_dist object filter number red beads equal greater 8. answer measures area portion curve compared entire area curve. question requires looking new area curve. someones asks question, two things. First, providing instructions posterior create. , results shovel 20 slots. Second, asking question area curve specific region. , region number red beads greater 8 highlighted red. Therefore, area curve red get estimate.examine validity final conclusion, let’s look Cardinal Virtues.","code":"\nset.seed(17)\n\n# Define a shovel size for our joint distributions\n\nshovel_size <- 20\n\n# To create our posterior distribution, filter for where the red in shovel is\n# equal to 17. This takes our slice.\n\npost_dist <- joint_dist %>% \n  filter(red_in_shovel == 17) %>% \n  \n  # Remove the columns of red_in_urn and red_in_shovel so we are left with p.\n  \n  select(-red_in_urn, -red_in_shovel) %>% \n  \n  # Create object new_reds which maps p using rbinom with size defined as our\n  # previously created shovel_size of 20.\n  \n  mutate(new_reds = map_int(p, ~ rbinom(n = 1,\n                                        size = shovel_size,\n                                        prob = .)))\n\n# Look at our created posterior distribution.\n  \npost_dist ## # A tibble: 1,919 x 2\n##        p new_reds\n##    <dbl>    <int>\n##  1 0.161        2\n##  2 0.171        7\n##  3 0.175        3\n##  4 0.176        5\n##  5 0.176        3\n##  6 0.186        4\n##  7 0.189        2\n##  8 0.197        2\n##  9 0.198        5\n## 10 0.2          2\n## # … with 1,909 more rows\npost_dist %>% \n  ggplot(aes(x = new_reds)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in new draw of 20\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nsum(post_dist$new_reds > 8) /  length(post_dist$new_reds)## [1] 0.26\npost_dist %>% \n  \n  # Create a column above_eight to identify True or False for new_reds being\n  # above eight.\n  \n  mutate(above_eight = ifelse(new_reds > 8, \"True\", \"False\")) %>% \n  \n  # Set fill as above_eight.\n  \n  ggplot(aes(x = new_reds, fill = above_eight)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n  \n  # Scale_fill_manual() here is calling grey for the first color and red for the\n  # second color. This is going to highlight the portion of the curve that we\n  # need to isolate in red.\n  \n  scale_fill_manual(values = c('grey50', 'red'))+\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in new draw of 20\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\",\n         fill = \"Above Eight Beads?\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"one-parameter.html","id":"cardinal-virtues-1","chapter":"6 One Parameter","heading":"6.4.5 Cardinal virtues","text":"Recall virtue Wisdom. want know decent connection problem face data . problem answering following question: Conditional answer Question 1, probability, using urn, draw 8 red beads use shovel size 20? way approached answering question , posterior distribution urn, create joint distribution estimate probability drawing 8 red beads shovel size 20. question particular urn, urns? Clearly, conditional upon result specific urn, conclusions can applied particular urn; conclusions generalizable urns.Justice encompasses two main topics apply urn: predictive versus causal model mathematical formulas. modeling (just) prediction (also) modeling causation? Predictive models care nothing causation. prediction, care forecasting Y given X -yet-unseen data. notion “manipulation” models. can see, , model predictive; manipulating anything. math model can :\n\\[ T_i  \\sim B(p_H, n = 20) \\]\ntotal number \\(T\\) red beads experiment \\(\\) 20 sampled beads, \\(T_i\\), distributed binomial \\(n = 20\\) unknown probability \\(\\rho_h\\) proportion red exceeding 8 20 beads.Courage focuses code – structure model. Code allows us “fit” model estimating values unknown parameters. Though can never know true values parameters, can express uncertain knowledge form posterior probability distributions. distributions, can compare actual values outcome variable “fitted” “predicted” results model. can examine “residuals,” difference fitted actual values. answer question, taken slice posterior distribution create joint distribution. find final solution, looked area curve includes values 8 red beads .\n\nthink Temperance, think uncertainty. Asserting posterior distribution , conceptually, opening casino allowing people place wagers outcomes, odds consistent posterior. posterior wrong, take money. bad! , ? Rule thumb: future always variable models suggest. entire point virtue Temperance aware fact take account . Courage — bold bastard — comes along says, “model! Let’s calculate posterior open casino!” Temperance says, “Hold ! Yes, can calculate posterior, need cautious.” Temperance concerned tails predictions, center. best strategy opening casino, ? Limit bets first nights make sure posterior OK. Temperance concerned sensible management casino.","code":""},{"path":"one-parameter.html","id":"discussion","chapter":"6 One Parameter","heading":"6.5 Discussion","text":"","code":""},{"path":"one-parameter.html","id":"stan_glm","chapter":"6 One Parameter","heading":"6.5.1 stan_glm()","text":"Instead building joint conditional probability distributions “hand” , can just use stan_glm() rstanarm package. now, just walk code quickly. Chapter 7 provide step--step explanation. clear, need fully understand section code works. introduction, formal lesson.Assume , , shovel size 50 drawn urn returned 17 red beads 33 white.single function call done work previously: created joint distribution estimated posterior probability distribution, conditional data passed data argument. fit_1 object, easy answer two sorts questions: posterior probability distribution \\(p\\) predictions new draws urn. key new functions posterior_epred() former posterior_predict() latter.posterior probability distribution:posterior probability distribution number red beads drawn shovel size 20:two posteriors similar ones created, much laboriously, . See Chapter 7 thorough discussion use rstanarm. package main tool rest Primer.","code":"\nlibrary(rstanarm)\n\nfit_1 <- stan_glm(formula = red ~ 1, \n                  data = tibble(red = c(rep(1, 17), \n                                        rep(0, 33))),\n                  family = binomial,\n                  refresh = 0,\n                  seed = 2021)\nposterior_epred(fit_1, \n                newdata = tibble(constant = 1)) %>% \n  as_tibble() %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Proportion of red beads in urn is centered around 0.34\",\n         x = \"Proportion p of Red Beads in Urn\",\n         y = \"Probability\") + \n  \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nposterior_predict(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) %>% \n  as_tibble() %>% \n  mutate(total = rowSums(across(`1`:`20`))) %>% \n  select(total) %>% \n  ggplot(aes(total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in 20-place shovel\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"one-parameter.html","id":"sampling-case-study","chapter":"6 One Parameter","heading":"6.5.2 Case study: Polls","text":"Let’s now switch gears realistic sampling scenario: poll. practice, pollsters take 1,000 repeated samples previous sampling activities, rather take single sample ’s large possible.December 4, 2013, National Public Radio US reported poll President Obama’s approval rating among young Americans aged 18-29 article, “Poll: Support Obama Among Young Americans Eroding.” poll conducted Kennedy School’s Institute Politics Harvard University. quote article:voting large numbers 2008 2012, young Americans souring President Obama.According new Harvard University Institute Politics poll, just 41 percent millennials — adults ages 18-29 — approve Obama’s job performance, lowest-ever standing among group 11-point drop April.Let’s tie elements real life poll new article “tactile” “virtual” urn activity Sections 6.1 6.2 using terminology, notations, definitions learned Section 6.3. ’ll see sampling activity urn idealized version pollsters trying real life.\n\nFirst, (Study) Population \\(N\\) individuals observations interest?Urn: \\(N\\) = 1000 identically sized red white beadsObama poll: \\(N\\) = ? young Americans aged 18-29Second, population parameter?Urn: population proportion \\(p\\) beads urn red.Obama poll: population proportion \\(p\\) young Americans approve Obama’s job performance.Third, census look like?Urn: Manually going \\(N\\) = 1000 beads exactly computing population proportion \\(p\\) beads red.Obama poll: Locating \\(N\\) young Americans asking approve Obama’s job performance. case, don’t even know population size \\(N\\) !Fourth, perform sampling obtain sample size \\(n\\)?Urn: Using shovel \\(n\\) slots.Obama poll: One method get list phone numbers young Americans pick \\(n\\) phone numbers. poll’s case, sample size poll \\(n = 2089\\) young Americans.Fifth, point estimate (AKA sample statistic) unknown population parameter?Urn: sample proportion \\(\\hat{p}\\) beads shovel red.Obama poll: sample proportion \\(\\hat{p}\\) young Americans sample approve Obama’s job performance. poll’s case, \\(\\hat{p} = 0.41 = 41\\%\\), quoted percentage second paragraph article.Sixth, sampling procedure representative?Urn: contents shovel representative contents urn? mixed urn sampling, can feel confident .Obama poll: sample \\(n = 2089\\) young Americans representative young Americans aged 18-29? depends whether sampling random.Seventh, samples generalizable greater population?Urn: sample proportion \\(\\hat{p}\\) shovel’s beads red “good guess” population proportion \\(p\\) urn’s beads red? Given sample representative, answer yes.Obama poll: sample proportion \\(\\hat{p} = 0.41\\) sample young Americans supported Obama “good guess” population proportion \\(p\\) young Americans supported Obama time 2013? words, can confidently say roughly 41% young Americans approved Obama time poll? , depends whether sampling random.Eighth, sampling procedure unbiased? words, observations equal chance included sample?Urn: Since bead equally sized mixed urn using shovel, bead equal chance included sample hence sampling unbiased.Obama poll: young Americans equal chance represented poll? , depends whether sampling random.Ninth lastly, sampling done random?Urn: long mixed urn sufficiently sampling, samples random.Obama poll: sample conducted random? can’t answer question without knowing sampling methodology used Kennedy School’s Institute Politics Harvard University. ’ll discuss end section.words, poll Kennedy School’s Institute Politics Harvard University can thought instance using shovel sample beads urn. Furthermore, another polling company conducted similar poll young Americans roughly time, likely get different estimate 41%. due sampling variation.Let’s now revisit sampling paradigm Subsection 6.3.1:general:sampling sample size \\(n\\) done random, thenthe sample unbiased representative population size \\(N\\), thusany result based sample can generalize population, thusthe point estimate “good guess” unknown population parameter, thusinstead performing census, can infer population using sampling.Specific urn:extract sample \\(n = 50\\) beads random, words, mix equally sized beads using shovel, thenthe contents shovel unbiased representation contents urn’s 1000 beads, thusany result based shovel’s beads can generalize urn, thusthe sample proportion \\(\\hat{p}\\) \\(n = 50\\) beads shovel red “good guess” population proportion \\(p\\) \\(N = 1000\\) beads red, thusinstead manually going 1000 beads urn, can infer urn using shovel.Specific Obama poll:way contacting randomly chosen sample 2089 young Americans polling approval President Obama 2013, thenthese 2089 young Americans unbiased representative sample young Americans 2013, thusany results based sample 2089 young Americans can generalize entire population young Americans 2013, thusthe reported sample approval rating 41% 2089 young Americans good guess true approval rating among young Americans 2013, thusinstead performing expensive census young Americans 2013, can infer young Americans 2013 using polling.can see, critical sample obtained Kennedy School’s Institute Politics Harvard University truly random order infer young Americans’ opinions Obama. sample truly random? ’s hard answer questions without knowing sampling methodology used.example, Kennedy School’s Institute Politics Harvard University conducted poll using mobile phone numbers? People without mobile phones left therefore represented sample. flaw example censoring, exclusion certain datapoints due issue data collection. results incomplete observation increases prediction uncertainty estimand, Obama’s approval rating among young Americans. Ensuring samples random easy sampling urn exercises; however, real life situation like Obama poll, much harder .visualized chapter demonstration famous theorem, mathematically proven truth, called Central Limit Theorem. loosely states sample means based larger larger sample sizes, sampling distribution sample means becomes normally shaped narrow. words, sampling distribution increasingly follows normal distribution variation sampling distributions gets smaller, quantified standard errors.","code":""},{"path":"one-parameter.html","id":"sampling-mechanism","chapter":"6 One Parameter","heading":"6.5.3 Sampling Mechanism","text":"One important aspects sampling sampling mechanism: mechanism sample population. concept related, distinctly different, assignment mechanism learned Chapter 3.assignment mechanism sorts units control experiment groups, sampling mechanism means acquire sample. Assignment mechanisms place urn paradigm since measuring causal relationship assigning beads specific groups.think sampling mechanism : certain beads sampled, others ? completely random?order investigate concept, let’s revisit Preceptor Tables.\n","code":""},{"path":"one-parameter.html","id":"preceptor-tables-1","chapter":"6 One Parameter","heading":"6.5.3.1 Preceptor Tables","text":"Recall Preceptor Table table rows columns data (reasonably) like . two different types Preceptor Tables applicable urn example: actual ideal.\nFIGURE 6.21: see another Precetor Table section.\nactual Preceptor Table shows actually know. Accordingly, table riddled question marks real world saddles us . ideal Preceptor Table Preceptor Table question marks, reasonable number rows columns. ideal Preceptor Table, need inference; estimand simple matter arithmetic.visualize different Preceptor Tables usefulness us data scientists, let’s revisit urn.case Rubin Causal Model, wish knew values every single unit every possible scenario. analogous ideal table know color identity every single bead. compare previous Preceptor Table, ideal Preceptor Table look like: can create ideal Preceptor Table performing exhaustive census entire urn. Let’s say , tedious process, find true real proportion red beads exactly 40%. know color every bead.world, estimand, proportion red beads urn, simple matter arithmetic. However, emphasized , performing exhaustive count easiest way estimate proportion red beads. Real life sampling far complex. process extremely prone error. Despite , people overestimate validity conclusions drawn sampling. stress unknowns sampling, let’s look actual Preceptor Table.Let’s imagine use shovel sample 100 beads urn. taking sample, find 40% sampled beads red. Let’s visualize looking entire urn taken sample. actual Preceptor Table. know colors randomly sampled 100 beads, remaining bead colors missing data! data? Well, take sample 100 beads, color identifications 100 total 1000 beads urn. rest beads sampled say certain whether white red.Something else must consider beads get sampled, others . consequence sampling mechanism. drawing sample using shovel 100 slots, 100 known values. Therefore, shovel (addition mixing urn beforehand) sampling mechanism.Consider : information tell us, specifically, Bead 1? Bead 2? know proportion red beads sample 40%. mean bead 1 precisely 40% chance red? learned Chapter 5, true! uncertainty.can claim certain , 100 beads sampled (total 1000 beads urn), 40% red. making prediction probability one sampled beads red, 40% correct probability. making prediction probability unsampled bead red, answer 40% incorrect, likely extremely incorrect. sample tell us something.","code":""},{"path":"one-parameter.html","id":"precision-versus-accuracy-or-bias-versus-variance","chapter":"6 One Parameter","heading":"6.5.4 Precision versus accuracy (or bias versus variance)","text":"saw previous section sample size \\(n\\) increases, point estimates vary less less concentrated around true population parameter. variation quantified decreasing standard error. words, typical error point estimates decrease. sampling exercise, sample size increased, variation sample proportions \\(\\hat{p}\\) decreased. also known precise estimate.random sampling ensures point estimates accurate, hand large sample size ensures point estimates precise. terms “accuracy” “precision” may sound like mean thing, subtle difference. Accuracy describes “target” estimates , whereas precision describes “consistent” estimates . image illustrates difference.\nFIGURE 6.22: Comparing accuracy precision.\nNow, ’s obvious best case scenario precise accurate option. However, real life sampling isn’t easy!option use shovel 200 slots, minor magnetic property caused pick slightly red beads 100 slotted shovel? one hand, larger shovel gives us increased precision due larger sample size. , magnetic property gives us decreased accuracy due sampling bias. , often case real world, tradeoff!","code":""},{"path":"one-parameter.html","id":"el-jefes-or-felipes","chapter":"6 One Parameter","heading":"6.5.5 El Jefe’s or Felipe’s?","text":"\nFIGURE 6.23: El Jefe’s, Harvard Square staple, known large portions delicious horchata.\n\nFIGURE 6.24: Felipe’s, another staple, said authentic experience addicting churros.\napply complex example urn, imagine conducting poll gather information whether Harvard students prefer El Jefe’s Taqueria Felipe’s Taqueria. given two options: 1) can conduct poll 50 students walking Lamont Library 2) can conduct poll 300 students walking outside El Jefe’s Taqueria.first scenario provides accurate estimate, since students outside Lamont Library much less likely biased sample. second scenario, however, provides precise estimate due larger sample size. Even though students outside El Jefe’s might patrons, safe assume least patrons, making sample somewhat biased. better option? depends views trade-offs accuracy precision. precision matters, prefer larger sample.","code":""},{"path":"one-parameter.html","id":"summary-6","chapter":"6 One Parameter","heading":"6.6 Summary","text":"Key lesson:truth! true value \\(p\\) know! want create posterior probability distribution summarizes knowledge. care posterior probability distribution p. center distribution around mean median proportion sample. sd (mad) posterior standard deviation divided square root sample size! Note thing standard deviation repeated samples.\n\nKey lesson:journey reality, predictions, standard error predictions, posterior probability distribution p. sequence:p (truth) \\(\\Rightarrow\\) \\(\\hat{p}\\) (estimate) \\(\\Rightarrow\\) standard error \\(\\hat{p}\\) (black box math mumbo jumbo computer simulation magic) \\(\\Rightarrow\\) posterior probability distribution p (beliefs truth).journey shows beliefs truth develop work. begin p; p truth, true unknown value estimating. \\(\\hat{p}\\) estimate p. can millions millions \\(\\hat{p}\\)’s. Next, must take standard error estimates (\\(\\hat{p}\\)’s) account uncertainty predictions. Finally – thing need – create posterior probability distribution p. distribution used answer key questions p. highlights:","code":""},{"path":"one-parameter.html","id":"tactial-sampling","chapter":"6 One Parameter","heading":"6.6.1 Tactial sampling","text":"Sampling allows us make guesses unknown, difficult--obtain value looking smaller subset data generalizing larger population.Sampling preferable urn example counting 1000 beads urn intensive tedious.Sampling preferable real-world often impossible sample “beads” (people) population. sampling, see variations results. known sampling variation expected, especially draw samples random (unbiased).","code":""},{"path":"one-parameter.html","id":"virtual-sampling-1","chapter":"6 One Parameter","heading":"6.6.2 Virtual sampling","text":"creating virtual analog urn shovel, able look even samples observe effects sampling size results.samples yield even distributions resemble bell.Larger sample sizes decrease standard deviation, meaning resulting proportions red closer one another sample sizes smaller. means larger samples = lower SD = precise guesses.writing lot code something want perform frequently, write function instead! saves time functions can called within map() ease plotting results.","code":""},{"path":"one-parameter.html","id":"standard-error","chapter":"6 One Parameter","heading":"6.6.3 Standard error","text":"Standard error just fancy term uncertainty something don’t know. Standard error \\(\\approx\\) (uncertain) beliefs.standard error measures accuracy sample distribution compared population using standard deviation.find larger sample sizes \\(\\implies\\) lower standard errors \\(\\implies\\) accurate estimates.know two pieces information data, need measure center distribution (like mean median) measure variability distribution (like sd MAD).SE refers standard deviation sample statistic (aka point estimate), mean median. Therefore, “standard error mean” refers standard deviation distribution sample means taken population.","code":""},{"path":"one-parameter.html","id":"answering-the-questions-1","chapter":"6 One Parameter","heading":"6.6.4 Answering the questions","text":"joint distribution allows us explore likely possibilities according possible models scenario.posterior distribution takes slice joint distribution specific question.models valuable answering questions model.","code":""},{"path":"one-parameter.html","id":"discussion-1","chapter":"6 One Parameter","heading":"6.6.5 Discussion","text":"stan_glm() can create joint distribution estimate posterior probability distribution, conditional data passed data argument. much easier way create posterior distribution, explored detail Chapter 7.sampling mechanism responsible items/beads/people/etc selected sampled. urn example, sampling mechanism shovel.Random sampling ensures point estimates accurate, large sample size ensures estimates precise. difference precision accuracy important make informed estimates.Real life sampling extremely prone error. emphasized enough. often forced choose precision accuracy real-life sampling.chapter, performed tactile virtual sampling exercises infer unknown parameter also presented case study sampling real life polls. case, used sample proportion \\(\\hat{p}\\) estimate population proportion \\(p\\). However, just limited scenarios related proportions. words, can use sampling estimate population parameters using point estimates well.continue journey, recall case Primrose Everdeen represents: matter realistic model , predictions never certain.","code":""},{"path":"two-parameters.html","id":"two-parameters","chapter":"7 Two Parameters","heading":"7 Two Parameters","text":"Chapter 6, learned inference. created joint distribution models consideration data might observed. observed data, went joint distribution conditional distribution possible models given data , fact, observe. conditional distribution, suitably normalized, posterior probability distribution space possible models. distribution, can answer question might (reasonably) ask.pain ass whole process ! professionals actually go steps every time work data science problem? ! absurd. Instead, professionals use standard tools , automated fashion, take care steps, taking us directly assumptions data posterior:\\[\\text{Prob}(\\text{models} | \\text{data} = \\text{data observed})\\]Even , however, relative likelihood different models important. Models invisible, mental entities physical presence unicorns leprechauns. world , make test predictions. People better models make better predictions. matters.addition change tools, two key differences chapter. First, Chapter 6 used models just one parameter: number red beads, can also transform parameter, \\(p\\), number red beads divided 1,000. model Chapter 6 binomial, one unknown parameter \\(p\\) models. chapter, two unknown parameters: mean \\(\\mu\\) height US standard deviation, \\(\\sigma\\), normally distributed error term.Second, Chapter 6 dealt limited set specific models: 1,001 , precise. procedure used just saw Chapter 5. chapter, hand, continuous parameters.point, understand reason making models , primarily, making models fun (although )! reason , confronted question, face decisions. must decide variables X Y. must choose datasets , B C. Confronted decision, need make model world help us.real world complex. substantive decision problem includes great deal complexity requires even context. time get level detail now. , simplify. going create model height adult men. use model answer four questions:average height men?average height men?probability next man meet taller 180 centimeters?probability next man meet taller 180 centimeters?probability , among next 4 men meet, tallest least 10 cm taller shortest?probability , among next 4 men meet, tallest least 10 cm taller shortest?posterior probability distribution height 3rd tallest man next 100 meet?posterior probability distribution height 3rd tallest man next 100 meet?hope chapter , answering four questions, ’ll gain better thorough understanding professionals data science.Data science ultimately moral act, use four Cardinal Virtues — Wisdom, Justice, Courage Temperance — organize approach.","code":""},{"path":"two-parameters.html","id":"wisdom-1","chapter":"7 Two Parameters","heading":"7.1 Wisdom","text":"","code":""},{"path":"two-parameters.html","id":"preceptor-table","chapter":"7 Two Parameters","heading":"7.1.1 Preceptor Table","text":"rows columns data need , , calculation number interest trivial? want know average height adult India, Preceptor Table include row adult India column height. scenario, want know average height men, “men” includes males Earth age 18.One key aspect Preceptor Table whether need one potential outcome order calculate estimand. Mainly: modeling (just) prediction (also) modeling causation? need causal model, one estimates attitude treatment control? causal model, Preceptor Table require two columns outcome. case, modeling causation; thus, need two outcome columns.Predictive models care nothing causation. Causal models often also concerned prediction, means measuring quality model. , looking prediction., ideal table look like? Assuming predicting height every male planet Earth moment time, height data every male person 18 years age. means almost 4 billion rows, one male person, includes column height.sample Preceptor Table:table extend way person 4 billion--something. table, questions answered basic math. inference necessary. Now ’ve seen ideal dataset, actual data look like?","code":""},{"path":"two-parameters.html","id":"eda-for-nhanes","chapter":"7 Two Parameters","heading":"7.1.2 EDA for nhanes","text":"quest find suitable data, find nhanes dataset National Health Nutrition Examination Survey conducted 2009 2011 Centers Disease Control Prevention, examines health pieces data children adults United States.nhanes includes 15 variables, including physical attributes like weight height. Let’s restrict attention subset, focusing age, gender height.Now, let’s examine random sample data:Notice decimal height column ch7. height <dbl> <int>.Let’s also run glimpse() new data.lookout anything suspicious. NA’s data set? types data columns, .e. age characterized integer instead double? females males?can never look data closely.addition glimpse(), can run skim(), skimr package, calculate summary statistics.TABLE 7.1: Data summaryVariable type: characterVariable type: numericInteresting! 353 missing values height subset data. Just using glimpse() show us . Let’s filter NA’s using drop_na(). delete rows value variable missing. want examine height men (boys, females), let’s limit data include adult males.Let’s plot data using geom_density() geom_histogram().’ll focusing subset nhanes data, designed answer questions. , instead using data, ’ll just use 50 randomly selected observations. (set.seed() function ensures 50 observations selected every time code run.)data — sample adult American men decade ago — allow us answer questions, however roughly? notion population comes play.","code":"\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(skimr)\nglimpse(nhanes)## Rows: 10,000\n## Columns: 15\n## $ survey         <int> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2…\n## $ gender         <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male…\n## $ age            <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58…\n## $ race           <chr> \"White\", \"White\", \"White\", \"Other\", \"White\", \"White\", \"…\n## $ education      <fct> High School, High School, High School, NA, Some College…\n## $ hh_income      <fct> 25000-34999, 25000-34999, 25000-34999, 20000-24999, 350…\n## $ weight         <dbl> 87, 87, 87, 17, 87, 30, 35, 76, 76, 76, 68, 78, 75, 39,…\n## $ height         <dbl> 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170, …\n## $ bmi            <dbl> 32, 32, 32, 15, 31, 17, 21, 27, 27, 27, 24, 24, 26, 19,…\n## $ pulse          <int> 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80,…\n## $ diabetes       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ general_health <int> 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, NA…\n## $ depressed      <fct> Several, Several, Several, NA, Several, NA, NA, None, N…\n## $ pregnancies    <int> NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA,…\n## $ sleep          <int> 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA,…\nnhanes %>% \n  select(age, gender, height)## # A tibble: 10,000 x 3\n##      age gender height\n##    <int> <chr>   <dbl>\n##  1    34 Male     165.\n##  2    34 Male     165.\n##  3    34 Male     165.\n##  4     4 Male     105.\n##  5    49 Female   168.\n##  6     9 Male     133.\n##  7     8 Male     131.\n##  8    45 Female   167.\n##  9    45 Female   167.\n## 10    45 Female   167.\n## # … with 9,990 more rows\nnhanes %>% \n  select(age, gender, height) %>% \n  slice_sample(n = 5)## # A tibble: 5 x 3\n##     age gender height\n##   <int> <chr>   <dbl>\n## 1    30 Male     179.\n## 2    50 Male     175.\n## 3    32 Male     175.\n## 4    16 Female   166.\n## 5     4 Male     109.\nnhanes %>% \n  select(age, gender, height) %>% \n  glimpse()## Rows: 10,000\n## Columns: 3\n## $ age    <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9,…\n## $ gender <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fema…\n## $ height <dbl> 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170, 182, 169…\nnhanes %>% \n  select(age, gender, height) %>% \n  skim()\nch7 <- nhanes %>% \n  filter(gender == \"Male\", age >= 18) %>% \n  select(height) %>% \n  drop_na()\nch7 %>%\n  ggplot(aes(x = height)) + \n  geom_density(color = \"black\",\n               fill = \"red\",\n               alpha = .2) +\n  \n  # You can have multiple geom layers in a plot by simply adding them one after\n  # the other.\n  \n  geom_histogram(mapping = aes(y = ..density..),\n                 bins = 15,\n                 color = \"black\",\n                 fill = \"blue\",\n                 alpha = .2) +\n  \n  # In order to gave Greek letters and other mathematical expressions appear in\n  # plot labels, you need to use the expression() function, with appropriate\n  # arguments, which will often be the name of the Greek letter you want.\n  \n  labs(x = expression(mu),\n       y = \"Density\",\n       title = \"Height (cm) in NHANES Dataset\",\n       color = \"Sex\") +\n  theme_classic()\nch7_all <- nhanes %>%\n  filter(gender == \"Male\", age >= 18) %>%\n  select(height) %>%\n  drop_na() \n\nset.seed(9)\n\nch7 <- ch7_all %>% \n  slice_sample(n = 50)"},{"path":"two-parameters.html","id":"population","chapter":"7 Two Parameters","heading":"7.1.3 Population","text":"One important components Wisdom concept “population.”population set people data — participants CDC’s Health Nutrition Examination Survey conducted 2009 2011. dataset. set voters like data. rows Preceptor Table. population larger — potentially much larger — set individuals include data data want. Generally, population much larger either data data want.case, want predict average height males today, people 2009-2011! also want predict height males outside United States, group excluded dataset. reasonable generate conclusions world group? likely, . However, limited data work determine far willing generalize groups.judgment call, matter Wisdom, whether may assume data data want (.e., Preceptor Table) drawn population.Even though original question “males” general, specifically refer males United States, assume data United States citizens , uh, representative enough population interested . believe , stop right now. major part Wisdom deciding questions can’t answer data just don’t .social sciences, never perfect relationship data question trying answer. Data American males past thing data American males today. data men France Mexico.Yet, data relevant. Right? certainly better nothing.Using -perfect data generally better using data .-perfect data always better? ! problem estimating median height 5th grade girls Tokyo, doubt data relevant. Wisdom recognizes danger using non-relevant data build model mistakenly using model way make situation worse. data won’t help, don’t use data, don’t build model. Better just use common sense experience. find better data.now, accept data works.","code":""},{"path":"two-parameters.html","id":"the-population-table","chapter":"7 Two Parameters","heading":"7.1.4 The Population Table","text":"Population Table shows data actually desired population. shows rows three sources: Preceptor Table, data, population outside data (rows exist data).Preceptor rows, information covariates sex year. However, rows included data, outcome results (height). Since scenario pertains prediction average height 2021, year column read “2021” sex column read “male.”rows data everything: covariates outcomes. covariates 2009-2011 year male sex, since pieces information included data. outcome results rows.rows population data. subjects fall desired population, data. , rows missing.Preceptor TableMale2021?Preceptor TableMale2021?............DataMale2009180DataMale2010160DataMale2009168............Population???Population???Population???","code":""},{"path":"two-parameters.html","id":"justice-1","chapter":"7 Two Parameters","heading":"7.2 Justice","text":"looked data decided “close enough” questions creating model help us come better answers, move Justice.Justice emphasizes key concepts:Population Table, structure includes row every unit population. generally break rows Population Table three categories: data units want (actual data set), data units actually (Preceptor Table), data units care (rest population, included data Preceptor Table).data representative population?meaning columns consistent, .e., can assume validity?\nmake assumption data generating mechanism.inspect representativeness validity Population Table. Representativeness focuses rows table, validity focuses columns. Let’s explore means depth.","code":""},{"path":"two-parameters.html","id":"representativeness","chapter":"7 Two Parameters","heading":"7.2.1 Representativeness","text":"’ve stated , representativeness involves rows. specifically, rows data representative rows data? Ideally, data random, unbiased selection population, answer question yes.nhanes data, case? time investigate.According CDC, individuals invited participate NHANES based randomized process. First, United States divided number geographical groups (ensure counties areas). groups, counties randomly selected participate. county randomly selected, members households county notified upcoming survey, must volunteer time participate.\nclear process goes several layers randomization (promising!). said, many counties excluded end process. also possible certain groups communities less representative greater population, though know certain.also fact participation voluntary. Perhaps certain individuals (immobile, elderly, anxious) less likely participate. Perhaps individuals hospitalized get opportunity participate. impacts data!Regardless, can assume process ensures people United States somewhat equal chance surveyed. Thus, data representative population.","code":""},{"path":"two-parameters.html","id":"validity","chapter":"7 Two Parameters","heading":"7.2.2 Validity","text":"Validity involves columns. specifically, whether columns mean thing. Consider following example: predicting height two different datasets. datasets measure height centimeters, may assume validity — columns identical meaning. However, validity much complex appears first glance.Consider method measurements. first dataset, participants asked remove shoes? allowed keep thick socks? case second dataset, columns technically represent truth.even smaller differences impact validity. instance, known — course day — average human’s spine compresses inch time wake time go sleep! first set data collected morning, second set data collected evening, predictions may .best can, need investigate possible reasons may able say columns mean thing. find issue, may need adjust one set data match . instance, knew height taken evening one sample (measurements taken morning), may add inch height findings. ensure validity.","code":""},{"path":"two-parameters.html","id":"functional-form","chapter":"7 Two Parameters","heading":"7.2.3 Functional form","text":"However, little mathematical notation make modeling assumptions clear, bring precision approach. case:\\[ y_i =  \\mu + \\epsilon_i \\]\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) height male \\(\\). \\(\\mu\\) average height males population. \\(\\epsilon_i\\) “error term,” difference height male \\(\\) average height males.\\(\\epsilon_i\\) normally distributed mean 0 standard deviation \\(\\sigma\\). mean 0 relates concept accuracy; assuming data representative enough accurate, can expect average error 0. standard deviation, hand, relates concept precision; smaller \\(\\sigma\\) , precise data , larger \\(\\sigma\\) , less precise data .simplest model can construct. Note:model two unknown parameters: \\(\\mu\\) \\(\\sigma\\). can anything else need estimate values parameters. Can ever know exact value? ! Perfection lies God’s R code. , using Bayesian approach similar used Chapters 5 6, able create posterior probability distribution parameter.model wrong, models.model wrong, models.parameter care \\(\\mu\\). parameter substantively meaningful interpretation. meaning \\(\\sigma\\) difficult describe, also don’t particular care value. Parameters like \\(\\sigma\\) context nuisance auxiliary parameters. still estimate posterior distributions, don’t really care posteriors look like.parameter care \\(\\mu\\). parameter substantively meaningful interpretation. meaning \\(\\sigma\\) difficult describe, also don’t particular care value. Parameters like \\(\\sigma\\) context nuisance auxiliary parameters. still estimate posterior distributions, don’t really care posteriors look like.\\(\\mu\\) average height men sample. can calculate directly. 175.61. estimation required! Instead, \\(\\mu\\) average height men population. Recall discussions Chapter 6 population universe people/units/whatever seek draw conclusions. level, seems simple. deeper level, subtle. example, walking around Copenhagen, population really care , order answer three questions, set adult men might meet today. population adult men US 2010. close enough? better nothing? want assume men nhanes (data ) men meet Copenhagen today (data want ) drawn population. case different details matter.\\(\\mu\\) average height men sample. can calculate directly. 175.61. estimation required! Instead, \\(\\mu\\) average height men population. Recall discussions Chapter 6 population universe people/units/whatever seek draw conclusions. level, seems simple. deeper level, subtle. example, walking around Copenhagen, population really care , order answer three questions, set adult men might meet today. population adult men US 2010. close enough? better nothing? want assume men nhanes (data ) men meet Copenhagen today (data want ) drawn population. case different details matter.\\(\\sigma\\) estimate standard deviation errors, .e., variability height accounting mean.\\(\\sigma\\) estimate standard deviation errors, .e., variability height accounting mean.Consider:\\[\\text{outcome} = \\text{model} + \\text{model}\\]\ncase, outcome height individual male. variable, also called “response,” trying understand /explain /predict. model creation, mixture data parameters, attempt capture underlying structure world generates outcome.difference outcome model? definition, model, blooming buzzing complexity real world. model always incomplete won’t capture everything. Whatever model misses thrown error term.","code":""},{"path":"two-parameters.html","id":"courage-1","chapter":"7 Two Parameters","heading":"7.3 Courage","text":"data science, deal words, math, code, important code. need Courage create model, take leap faith can make ideas real.","code":""},{"path":"two-parameters.html","id":"stan_glm-1","chapter":"7 Two Parameters","heading":"7.3.0.1 stan_glm","text":"Bayesian models hard create R. rstanarm package provides tools need, importantly function stan_glm().first argument stan_glm() data, case filtered ch7 tibble contains 50 observations. mandatory argument formula want use build model. case, since predictor variables, formula height ~ 1.Details:may take time. Bayesian models, especially ones large amounts data, can take longer might like. Indeed, computational limits main reason Bayesian approaches — , extent, still — little used. creating models, often want use cache = TRUE code chunk option. saves result model don’t recalculate every time knit document.may take time. Bayesian models, especially ones large amounts data, can take longer might like. Indeed, computational limits main reason Bayesian approaches — , extent, still — little used. creating models, often want use cache = TRUE code chunk option. saves result model don’t recalculate every time knit document.data argument, like usage R, input data.data argument, like usage R, input data.don’t set refresh = 0, model puke many lines confusing output. can learn output reading help page stan_glm(). output provides details fitting process runs well diagnostics final result. details beyond scope book.don’t set refresh = 0, model puke many lines confusing output. can learn output reading help page stan_glm(). output provides details fitting process runs well diagnostics final result. details beyond scope book.always assign result call stan_glm() object, . convention, name object often include word “fit” indicate fitted model object.always assign result call stan_glm() object, . convention, name object often include word “fit” indicate fitted model object.direct connection mathematical form model created Justice code use fit model Courage. height ~ 1 code equivalent \\(y_i = \\mu\\).direct connection mathematical form model created Justice code use fit model Courage. height ~ 1 code equivalent \\(y_i = \\mu\\).default value family gaussian, need include call . Justice section, assumption \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) equivalent using gaussian. \\(\\epsilon_i\\) different distribution, need use different family. saw example situation end Chapter 6 performed urn analysis using stan_glm() setting family = binomial.default value family gaussian, need include call . Justice section, assumption \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) equivalent using gaussian. \\(\\epsilon_i\\) different distribution, need use different family. saw example situation end Chapter 6 performed urn analysis using stan_glm() setting family = binomial.Although can use standalone function set.seed() order make code reproducible, convenient use seed argument within stan_glm(). Even though fitting process, unavoidably, contains random component, get exact answer set seed = 9 rerun code. matter number use seed.Although can use standalone function set.seed() order make code reproducible, convenient use seed argument within stan_glm(). Even though fitting process, unavoidably, contains random component, get exact answer set seed = 9 rerun code. matter number use seed.","code":"\nlibrary(rstanarm)\nfit_obj <- stan_glm(data = ch7, \n                    formula = height ~ 1, \n                    family = gaussian, \n                    refresh = 0,\n                    seed = 9)"},{"path":"two-parameters.html","id":"printed-model","chapter":"7 Two Parameters","heading":"7.3.0.1.1 Printed model","text":"several ways examine fitted model. simplest print . Recall just typing x prompt writing print(x).first line telling us model used, case stan_glm().second line tells us model using Gaussian, normal, distribution. discussed distribution Section 2.8.5. typically use default unless working lefthand variable extremely non-normal, e.g., something takes two values like 0/1 TRUE/FALSE. Since height (roughly) normally distributed, Gaussian distribution good choice.third line gives us back formula provided. creating model predicting height constant — just simplest model can create. Formulas R constructed two parts. First, left side tilde (“~” symbol) “response” “dependent” variable, thing trying explain. Since model height, height goes lefthand side. Second, “explanatory” “independent” “predictor” variables righthand side tilde. often many variables , simplest possible model, one, single constant. (number 1 indicates constant. mean think everyone height 1.)fourth fifth lines output tell us 50 observations one predictor (constant). , terminology bit confusing. mean suggest \\(\\mu\\) “constant?” means , although \\(\\mu\\)’s value unknown, fixed. change person person. 1 formula corresponds parameter \\(\\mu\\) mathematical definition model.knew information fit model. R records fit_obj don’t want forget . second half display gives summary parameter values. can look just second half detail argument:see output two parameters model: “(Intercept)” “sigma.” can confusing! Recall thing care \\(\\mu\\), average height population. Preceptor Table — row every adult male population care missing data — \\(\\mu\\) trivial calculate, uncertainty. know named parameter \\(\\mu\\). R sees 1 formula. fields statistics, constant term called “intercept.” , now three things — \\(\\mu\\) (math), 1 (code), “(Intercept)” (output) — refer exact concept. last time terminology confusing.point, stan_glm() — rather print() method objects created stan_glm() — problem. full posteriors \\(\\mu\\) \\(\\sigma\\). simple printed summary. can’t show entire distribution. , best numbers provide? right answer question! , choice provide “Median” posterior “MAD_SD.”Anytime distribution, whether posterior probability otherwise, important single number associated measure location. data? two common choices measure mean median. use median posterior distributions can often quite skewed, making mean less stable measure.Anytime distribution, whether posterior probability otherwise, important single number associated measure location. data? two common choices measure mean median. use median posterior distributions can often quite skewed, making mean less stable measure.second important number summarizing distribution concerns spread. far data spread around center? common measure used standard deviation. MAD SD, scaled median absolute deviation, another. variable normal distribution, standard deviation MAD SD similar. MAD SD much robust outliers, used . (Note MAD SD measure referred mad till now. measure calculated mad() command R. Terminology confusing, usual.)second important number summarizing distribution concerns spread. far data spread around center? common measure used standard deviation. MAD SD, scaled median absolute deviation, another. variable normal distribution, standard deviation MAD SD similar. MAD SD much robust outliers, used . (Note MAD SD measure referred mad till now. measure calculated mad() command R. Terminology confusing, usual.)can also change number digits shown:Now understand meaning Median MAD_SD display, can interpret actual numbers. median intercept, 175.6, median posterior distribution \\(\\mu\\), average height men population. median sigma, 8.5, median posterior distribution true \\(\\sigma\\), can roughly understood variability height men, account estimate \\(\\mu\\).MAD SD parameter measure variability posterior distributions parameter. spread ? Speaking roughly, 95% mass posterior probability distribution located within +/- 2 MAD SDs median. example, 95% confident true value \\(\\mu\\) somewhere 173.2 178.","code":"\nfit_obj## stan_glm\n##  family:       gaussian [identity]\n##  formula:      height ~ 1\n##  observations: 50\n##  predictors:   1\n## ------\n##             Median MAD_SD\n## (Intercept) 175.6    1.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 8.5    0.9   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nprint(fit_obj, detail = FALSE)##             Median MAD_SD\n## (Intercept) 175.6    1.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 8.5    0.9\nprint(fit_obj, detail = FALSE, digits = 1)##             Median MAD_SD\n## (Intercept) 175.6    1.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 8.5    0.9"},{"path":"two-parameters.html","id":"plotting-the-posterior-distributions","chapter":"7 Two Parameters","heading":"7.3.0.1.2 Plotting the posterior distributions","text":"Instead math heads, can display posterior probability distributions. Pictures speak math mumbles. Fortunately, getting draws posteriors easy:4,000 rows draws estimated posteriors, column. like vectors result calling functions like rnorm() rbinom(). can create plot similar way:Although possible variable names like “(Intercept),” recommended. Avoid weird names! stuck , place backticks. Even better, rename , .Note title includes word “Posterior” complete term “Posterior Probability Distribution.” practice going forward. “Posterior” means “Posterior Distribution” , posterior distribution sum range 1 “posterior probability distribution.” plots, “posterior” implies “posterior probability distribution.”Always go back first principles. “truth, unknown number, fact world. knew everything, Preceptor Table, inference necessary. Algebra suffice. Alas, imperfect world, choice data scientist. always uncertain. summarize knowledge unknown numbers posterior probability distributions, ”posteriors\" short., \\(\\sigma\\) usually nuisance parameter. don’t really care value , rarely plot .","code":"\nfit_obj %>% \n  as_tibble()## # A tibble: 4,000 x 2\n##    `(Intercept)` sigma\n##            <dbl> <dbl>\n##  1          175.  8.34\n##  2          175.  8.16\n##  3          174.  8.26\n##  4          174.  8.25\n##  5          173.  9.43\n##  6          175.  9.24\n##  7          174.  8.47\n##  8          174.  8.26\n##  9          177.  8.71\n## 10          173.  9.06\n## # … with 3,990 more rows\nfit_obj %>% \n  as_tibble() %>% \n  rename(mu = `(Intercept)`) %>% \n  ggplot(aes(x = mu)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Average Male Height\",\n         subtitle = \"American men average around 176 cm in height\",\n         x = expression(mu), \n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nfit_obj %>% \n  as_tibble() %>% \n  ggplot(aes(x = sigma)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Standard Deviation of Male Height\",\n         subtitle = \"The standard deviation of height is around 7 to 11 cm\",\n         x = expression(sigma),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"two-parameters.html","id":"testing","chapter":"7 Two Parameters","heading":"7.3.1 Testing","text":"","code":""},{"path":"two-parameters.html","id":"temperance-1","chapter":"7 Two Parameters","heading":"7.4 Temperance","text":"model. can ? Let’s answer four questions started.","code":""},{"path":"two-parameters.html","id":"question-1","chapter":"7 Two Parameters","heading":"7.4.1 Question 1","text":"average height men?Preceptor Table, one row every man alive, actual height, calculate number easily. Just take average 3 billion rows! Alas, actual Preceptor Table, vast majority heights missing. Question marks make simple algebra impossible. , unknown number, need estimate posterior probability distribution. Objects created stan_glm() make easy .use posterior_epred() many times. two key arguments object, fitted model object returned stan_glm(), newdata, tibble contains covariate values associated unit (units) want make forecast. (case, newdata can tibble intercept-model make use covariates. don’t really need variable named constant, including harm.) epred posterior_epred() stands expected prediction. words, pick random adult male “expect” height . also call expected value.use as_tibble() convert matrix returned posterior_epred(). tibble 1 column 4,000 rows. column, unhelpfully named 1, 4,000 draws posterior probability distribution expected height random male. Recall earlier chapters posterior probability distribution draws posterior probability distribution , less, thing. , rather, posterior probability distribution, ownself, hard work . Draws distribution, hand, easy manipulate. use draws answer questions.Converting 4,000 draws posterior probability distribution straightforward.rest Primer filled graphics like one. make dozens . fundamental structure algebra real number. fundamental structure data science posterior probability distribution. need able create interpret .","code":"\nnewobs <- tibble(constant = 1)\n\npe <- posterior_epred(object = fit_obj,\n                      newdata = newobs) %>% \n        as_tibble()\n\npe## # A tibble: 4,000 x 1\n##      `1`\n##    <dbl>\n##  1  175.\n##  2  175.\n##  3  174.\n##  4  174.\n##  5  173.\n##  6  175.\n##  7  174.\n##  8  174.\n##  9  177.\n## 10  173.\n## # … with 3,990 more rows\npe %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Adult Male Height\",\n         subtitle = \"Note that the plot is very similar to the one created with the parameters\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"two-parameters.html","id":"question-2","chapter":"7 Two Parameters","heading":"7.4.2 Question 2","text":"probability next adult male meet taller 180 centimeters?two fundamentally different kinds unknowns care : expected values (previous question) predicted values. former, interested specific individual. individual value irrelevant. predicted values, care, average, specific person. former, use posterior_epred(). latter, relevant function posterior_predict(). functions return draws posterior probability distribution, unknown number underlies posterior different.Recall mathematics:\\[ y_i =  \\mu + \\epsilon_i \\]expected values averages, can ignore \\(\\epsilon_i\\) term formula. expected value \\(\\epsilon_i\\) zero since, assumption, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). However, can’t ignore \\(\\epsilon_i\\) predicting height single individual., straightforward turn draws posterior probability distribution graphic:Note:posterior individual much wider posterior expected value.posterior individual much wider posterior expected value.Eyeballing, seems like 1 3 chance next man meet, randomly chosen man, taller 180 cm.Eyeballing, seems like 1 3 chance next man meet, randomly chosen man, taller 180 cm.can calculate exact probability manipulating tibble draws directly.can calculate exact probability manipulating tibble draws directly.30% draws posterior probability distribution greater 180 cm, 30% chance next individual taller 180 cm., key conceptual difficulty population. problem actually involves walking around London, wherever, today. data involve America 2010. things! totally different. Knowing whether data “close enough” problem want solve heart Wisdom. Yet decision made start process, decision create model first place. Now created model, look virtue Temperance guidance using model. data never perfect match world face. need temper confidence act humility. forecasts never good naive use model might suggest. Reality surprise us. need take model’s claims family-sized portion salt.","code":"\nnewobs <- tibble(constant = 1)\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) %>% \n        as_tibble()\n\npp## # A tibble: 4,000 x 1\n##    `1`  \n##    <ppd>\n##  1 179  \n##  2 164  \n##  3 172  \n##  4 172  \n##  5 167  \n##  6 184  \n##  7 184  \n##  8 159  \n##  9 182  \n## 10 168  \n## # … with 3,990 more rows\npp %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Height of Random Male\",\n         subtitle = \"Uncertainty for a single individual is much greater than for the expected value\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nsum(pp$`1` > 180)/length(pp$`1`)## [1] 0.31"},{"path":"two-parameters.html","id":"question-3","chapter":"7 Two Parameters","heading":"7.4.3 Question 3","text":"probability , among next 4 men meet, tallest least 10 cm taller shortest?Bayesian models beautiful , via magic simulation, can answer (almost!) question. question four random individuals, need posterior_predict() give us four sets draws four identical posterior probability distributions. Start new newobs:need predict X individuals, need tibble X rows, regardless whether rows otherwise identical.Note need add mutate_all(.numeric) end pipe. caused bug — least awkwardness — whereby variable type provided posterior_predict() ppd rather dbl. Using mutate_all(.numeric) makes column type double. Avoid working columns type ppd. lead heartache.next step calculate number interest. can , directly, draw height tallest shortest 4 random men. However, drawn 4 random men, can calculate numbers, difference .steps serve template much analysis later. often hard create model directly thing want know. easy way create model estimates height difference directly. easy, however, create model allows random draws.Give us enough random draws, tibble store , can estimate world.random draws posterior distribution care , graphing posterior probability distribution -old, -old.85% chance , meeting 4 random men, tallest least 10 cm taller shortest. Exact calculation:","code":"\nnewobs <- tibble(constant = rep(1, 4))\n\nnewobs## # A tibble: 4 x 1\n##   constant\n##      <dbl>\n## 1        1\n## 2        1\n## 3        1\n## 4        1\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) %>% \n        as_tibble() %>% \n        mutate_all(as.numeric)\npp## # A tibble: 4,000 x 4\n##      `1`   `2`   `3`   `4`\n##    <dbl> <dbl> <dbl> <dbl>\n##  1  177.  170.  169.  164.\n##  2  178.  167.  164.  185.\n##  3  177.  180.  181.  170.\n##  4  174.  169.  166.  169.\n##  5  176.  191.  186.  165.\n##  6  173.  164.  160.  183.\n##  7  181.  199.  172.  157.\n##  8  169.  181.  174.  174.\n##  9  185.  180.  179.  170.\n## 10  177.  184.  164.  164.\n## # … with 3,990 more rows\n# First part of the code is the same as we did above.\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) %>% \n        as_tibble() %>% \n        mutate_all(as.numeric) %>% \n  \n        # Second part of the code requires some trickery.\n  \n        rowwise() %>% \n        mutate(tallest = max(c_across())) %>% \n        mutate(shortest = min(c_across())) %>% \n        mutate(diff = tallest - shortest) \n        \npp        ## # A tibble: 4,000 x 7\n## # Rowwise: \n##      `1`   `2`   `3`   `4` tallest shortest  diff\n##    <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> <dbl>\n##  1  167.  169.  159.  177.    177.     159.  17.6\n##  2  179.  187.  178.  170.    187.     170.  16.8\n##  3  169.  183.  168.  172.    183.     168.  15.2\n##  4  158.  179.  175.  161.    179.     158.  20.9\n##  5  182.  182.  158.  170.    182.     158.  24.5\n##  6  166.  153.  189.  165.    189.     153.  36.0\n##  7  169.  172.  190.  175.    190.     169.  20.3\n##  8  172.  164.  178.  197.    197.     164.  32.1\n##  9  190.  184.  177.  171.    190.     171.  18.7\n## 10  161.  192.  165.  193.    193.     161.  32.4\n## # … with 3,990 more rows\npp %>% \n  ggplot(aes(x = diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Max Height Difference Among Four Men\",\n         subtitle = \"The expected value for this difference would be much more narrow\",\n         x = \"Height Difference in Centimeters\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(breaks = seq(0, 50, 10),\n                       labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) \nsum(pp$diff > 10) / length(pp$diff)## [1] 0.85"},{"path":"two-parameters.html","id":"question-4","chapter":"7 Two Parameters","heading":"7.4.4 Question 4","text":"posterior probability distribution height 3rd tallest man next 100 meet?approach work almost question.Explore pp object. 101 columns: one hundred 100 individual heights one column 3rd tallest among . done hard work, plotting easy:","code":"\nnewobs <- tibble(constant = rep(1, 100))\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) %>% \n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  rowwise() %>% \n  mutate(third_tallest = sort(c_across(), \n                              decreasing = TRUE)[3])\npp %>% \n  ggplot(aes(x = third_tallest, y = after_stat(count / sum(count)))) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Posterior for Height of 3rd Tallest Man from Next 100\",\n         subtitle = \"Should we have more or less certainty about behavior in the tails?\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) "},{"path":"two-parameters.html","id":"three-levels-of-knowledge","chapter":"7 Two Parameters","heading":"7.4.5 Three Levels of Knowledge","text":"answering questions , can easy falsely believe delivering truth. case. fact, three primary levels knowledge need understand order account uncertainty.three primary levels knowledge possible knowledge scenario include: Truth (Preceptor Table), DGM Posterior, Posterior.","code":""},{"path":"two-parameters.html","id":"the-truth","chapter":"7 Two Parameters","heading":"7.4.5.1 The Truth","text":"know Truth (capital “T”), know Preceptor Table. knowledge, can directly answer question precisely. can calculate individual’s height, summary measure might interested , like average height different ages countries.level knowledge possible omniscient power, one can see every outcome every individual every treatment. Truth show, given individual, actions control, actions treatment, little factor impacted decisions.Truth represents highest level knowledge one can — , questions merely require algebra. need estimate treatment effect, different treatment effects different groups people. need predict — know.","code":""},{"path":"two-parameters.html","id":"dgm-posterior","chapter":"7 Two Parameters","heading":"7.4.5.2 DGM posterior","text":"DGM posterior next level knowledge, lacks omniscient quality Truth. posterior posterior calculate perfect knowledge data generating mechanism, meaning correct model structure exact parameter values. often falsely conflated “posterior,” subject error model structure parameter value estimations.DGM posterior posterior — estimate parameters based data predict future latest relevant information possible. difference , calculate posteriors unknown value DGM posterior, expect posteriors perfect.","code":""},{"path":"two-parameters.html","id":"our-posterior","chapter":"7 Two Parameters","heading":"7.4.5.3 Our posterior","text":"Unfortunately, posterior possesses even less certainty! real world, don’t perfect knowledge DGM: model structure exact parameter values. mean?go boss, tell best guess. informed estimate based relevant data possible. data, created posterior average height males.mean certain average height lies probable outcome posterior? course ! tell boss, shocking find actual average height less estimate.lot assumptions make process building model, processes Wisdom, subject error. Perhaps data match future well hoped. Ultimately, try account uncertainty estimates. Even safeguard, aren’t surprised bit .","code":""},{"path":"two-parameters.html","id":"zero-one-outcomes","chapter":"7 Two Parameters","heading":"7.5 0/1 Outcomes","text":"Variables well-behaved, continuous ranges easiest handle. started height simple. Sadly, however, many variables like height. Consider gender, variable nhanes takes two possible values: “Male” “Female.” way like construct model explains predicts height, like build model explains predicts gender. want answer questions like:probability random person 180 cm tall female?Wisdom suggests start looking data. models use numbers, need create new variable, female, 1 Females 0 Males.just fit linear model, ? Consider:Recall default value family gaussian, need include . Initially, fitted model seems OK.Comparing two individuals differ height 1 cm, expect taller individual 3% lower probability female. unreasonable. problems show extremes. Consider fitted values across range data.Using 1 Female 0 Male allows us interpret fitted values probability person female male. handy natural interpretation. problem linear model arises , case, model suggests values outside 0 1. values , definition, impossible. People 190 cm tall -25% chance female.Justice suggests different functional form, one restricts fitted values acceptable range. Look closely math:\\[  p(\\text{Female} = 1) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{height})}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{height})} \\]inverse logistic function, don’t worry details. Mathematical formulas never Google search away. Instead, note range restricted. Even \\(\\beta_0 + \\beta_1 \\text{height}\\) large number, ratio bound 1. Similarly, matter negative \\(\\beta_0 + \\beta_1 \\text{height}\\) , ratio can never smaller 0. model can , ever, produce impossible values.Whenever two categories outcome, use family = binomial.Courage allows us use tools fitting logistic regression fitting linear models.One major difference linear logistic models parameters latter much harder interpret. mean, substantively, \\(\\beta_1\\) -0.26? topic advanced course.Fortunately, parameters care . epiphenomenon, unicorns imagination. Instead, want answers questions, Temperance — functions rstanarm — guide. Recall question:probability random person 180 cm tall female?1 20 chance 180 centimeter tall person female.Note x y axes probabilities. Whenever create posterior probability distribution , definition, y-axis probability. x-axis unknown number know. unknown number can anything — weight average male, height 3rd tallest 100 men, probability 180 cm tall person female. probability just another number. interpretation always.Another major difference logistic models posterior_epred() posterior_predict() return different types objects. posterior_epred() returns probabilities, . posterior_predict(), hand, returns predictions, name suggests. words, returns zeros ones. Consider another question:group 100 people 180 centimeters tall, many women?show just first 4 columns convenience. column 4,000 draws posterior predictive distribution gender person 180 cm tall. (Since 100 people height, columns draws distribution.)can manipulate object row--row basis.total number women row. Manipulating draws row--row basis common.5 6 women likely number consistent answer first question. found random person 180 cm tall 5% 6% chance female. , 100 people, 5 6 seems reasonable total. expected value posterior_epred(), although provide sense center predictive distribution , tell us much range possible outcomes. , need posterior_predict().","code":"\nch7_b <- nhanes %>% \n  select(age, gender, height) %>%\n  mutate(female = ifelse(gender == \"Female\", 1, 0)) %>% \n  filter(age >= 18) %>% \n  select(female, height) %>% \n  drop_na()\n\nch7_b## # A tibble: 7,424 x 2\n##    female height\n##     <dbl>  <dbl>\n##  1      0   165.\n##  2      0   165.\n##  3      0   165.\n##  4      1   168.\n##  5      1   167.\n##  6      1   167.\n##  7      1   167.\n##  8      0   170.\n##  9      0   182.\n## 10      0   169.\n## # … with 7,414 more rows\nch7_b %>% \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Men are taller than women\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(0, 1),\n                     labels = c(\"Male\", \"Female\"))\nfit_gender_linear <- stan_glm(data = ch7_b,\n                              formula = female ~ height,\n                              family = gaussian,\n                              refresh = 0,\n                              seed = 82) \nprint(fit_gender_linear, digits = 2)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      female ~ height\n##  observations: 7424\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept)  6.22   0.07 \n## height      -0.03   0.00 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.36   0.00  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nch7_b %>% \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  geom_smooth(aes(y = fitted(fit_gender_linear)),\n              method = \"lm\",\n              formula = y ~ x,\n              se = FALSE) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Some fitted values are impossible\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(-0.5, 0, 0.5, 1, 1.5),\n                     labels = c(\"-50%\", \"0% (Male)\", \n                                \"50%\", \"100% (Female)\",\n                                \"150%\"))\nfit_2 <- stan_glm(data = ch7_b,\n                  formula = female ~ height,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 27)\nprint(fit_2, digits = 3)## stan_glm\n##  family:       binomial [logit]\n##  formula:      female ~ height\n##  observations: 7424\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept) 43.389  0.999\n## height      -0.257  0.006\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nnewobs <- tibble(height = 180)\n\npe <- posterior_epred(fit_2, newdata = newobs) %>% \n  as_tibble()\n\npe %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for p(female | height = 180 cm)\",\n         subtitle = \"There is a 5-6% chance that a person this tall is female\",\n         x = \"Probability\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) \nnewobs <- tibble(height = rep(180, 100))\n\npp <- posterior_predict(fit_2, newdata = newobs) %>% \n  as_tibble() %>% \n  mutate_all(as.numeric) \n\npp[, 1:4]## # A tibble: 4,000 x 4\n##      `1`   `2`   `3`   `4`\n##    <dbl> <dbl> <dbl> <dbl>\n##  1     0     0     0     0\n##  2     0     0     0     0\n##  3     0     0     0     0\n##  4     0     0     0     0\n##  5     0     0     0     0\n##  6     0     0     0     0\n##  7     0     0     0     0\n##  8     0     0     0     0\n##  9     0     0     0     0\n## 10     1     0     0     0\n## # … with 3,990 more rows\npp <- posterior_predict(fit_2, newdata = newobs) %>% \n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  rowwise() %>% \n  mutate(total = sum(c_across()))\n\npp[, c(\"1\", \"2\", \"100\", \"total\")]## # A tibble: 4,000 x 4\n## # Rowwise: \n##      `1`   `2` `100` total\n##    <dbl> <dbl> <dbl> <dbl>\n##  1     0     0     0     5\n##  2     0     0     0     3\n##  3     0     0     0     7\n##  4     0     0     0     8\n##  5     0     0     0     6\n##  6     0     0     0     5\n##  7     0     0     0     5\n##  8     0     0     0     2\n##  9     0     0     0     6\n## 10     0     0     0     4\n## # … with 3,990 more rows\npp %>% \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Number of Women among 100 People 180 cm Tall\",\n         subtitle = \"Consistent with probability estimate above\",\n         x = \"Number of Women\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) "},{"path":"two-parameters.html","id":"summary-7","chapter":"7 Two Parameters","heading":"7.6 Summary","text":"next five chapters follow process just completed . start decision make. luck, data guide us. (Without data, even best data scientist struggle make progress.) Wisdom asks us: “data close enough decision face make using data helpful?” Often times, answer “.”start build model, Justice guide us. model descriptive causal? mathematical relationship dependent variable trying explain independent variables can use explain ? assumptions making distribution error term?set model framework, need Courage implement model code. Without code, math world useless. created model, need understand . posterior distributions unknown parameters? seem sensible? interpret ?Temperance guides final step. model, can finally get back decision motivated exercise first place. can use model make statements world, confirm model consistent world use model make predictions numbers know.Let’s practice process another dozen times.","code":""},{"path":"three-parameters.html","id":"three-parameters","chapter":"8 Three Parameters","heading":"8 Three Parameters","text":"Models parameters. Chapter 6 created models single parameter \\(p\\), proportion red beads urn. Chapter 7, used models two parameters: \\(\\mu\\) (average height population, generically known model “intercept”) \\(\\sigma\\) (variation height population). — can guess going? — build models three parameters: \\(\\sigma\\) (serves role throughout book) two coefficients. models relate continuous predictor outcome, two parameters labeled \\(\\beta_0\\) \\(\\beta_1\\). models estimate two averages, parameters \\(\\beta_1\\) \\(\\beta_2\\). notation confusing, least different academic fields use inconsistent schemes. Follow Cardinal Virtues tackle problem step step.","code":""},{"path":"three-parameters.html","id":"wisdom-2","chapter":"8 Three Parameters","heading":"8.1 Wisdom","text":"Wisdom begins considering questions desire answer data set given.","code":""},{"path":"three-parameters.html","id":"the-questions","chapter":"8 Three Parameters","heading":"8.1.1 The Questions","text":"expected age Democrat train station?group three Democrats three Republicans, age difference oldest Democrat youngest Republican?average treatment effect, exposing people Spanish-speakers, attitudes toward immigration?largest causal effect still 1 10 chance occurring?expect income random 40 year old?Among people income $100,000, proportion liberal?Assume group eight people, two make $100,000, two $200,000, two $300,000 two $400,000. many liberal?determining questions like answer, must consider ideally data make preceptor table. words, individual population, information need answer question. Look first question. Upon first glance, appears answer , like age party affiliation population, holds true second question. Therefore, need columns age party affiliation preceptor table. third fourth question, however, require individuals exposed Spanish-speakers attitudes immigration fact, preceptor table, need columns attitude immigration exposed Spanish-speakers. Lastly, fourth question require income age every individual population, fifth sixth require knowing whether individual liberal well income, also need columns information income someone liberal . Therefore, preceptor table appear following:Now determined data ideally answer questions, take look data set . demonstrate modeling three parameters, use trains data set primer.data package.","code":""},{"path":"three-parameters.html","id":"eda-for-trains","chapter":"8 Three Parameters","heading":"8.1.2 EDA for trains","text":"Always explore data. Recall discussion Chapter 4. Enos (2014) randomly placed Spanish-speaking confederates nine train platforms around Boston, Massachusetts. Exposure Spanish-speakers – treatment – influenced attitudes toward immigration. reactions measured changes answers three survey questions. Load necessary libraries look data.data include information respondent’s gender, political affiliations, age, income . treatment indicates whether subject control treatment group. key outcomes attitudes toward immigration (att_start) (att_end) experiment. also measure ideology experiment, little change. Type ?trains read help page information variable.Let’s restrict attention subset variables recalling Preceptor table made. , can determine variables age, party, liberal income relevant questions seek answer. age age respondent, party political party affiliation, liberal whether liberal , income income respondent. Additionally, since treatment tells us whether respondent given treatment exposed Spanish-speakers train station att_end tell us respondent’s attitude immigration treatment, also variables interest us since useful answering questions.","code":"\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(skimr)\nlibrary(tidyverse)\nglimpse(trains)## Rows: 115\n## Columns: 15\n## $ treatment           <fct> Treated, Treated, Treated, Treated, Control, Treat…\n## $ att_start           <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6,…\n## $ att_end             <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7,…\n## $ gender              <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Femal…\n## $ race                <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"Whit…\n## $ liberal             <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FAL…\n## $ party               <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", …\n## $ age                 <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24…\n## $ income              <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 875…\n## $ line                <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framing…\n## $ station             <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"…\n## $ hisp_perc           <dbl> 0.026, 0.015, 0.019, 0.019, 0.019, 0.023, 0.030, 0…\n## $ ideology_start      <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3,…\n## $ ideology_end        <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3,…\n## $ income_in_thousands <dbl> 135, 105, 135, 300, 135, 88, 88, 135, 105, 135, 10…"},{"path":"three-parameters.html","id":"age-party","chapter":"8 Three Parameters","heading":"8.2 age ~ party","text":"want build model use model make claims world.expected age Democrat train station?group three Democrats three Republicans, age difference oldest Democrat youngest Republican?can answer similar questions creating model uses party affiliation predict age","code":""},{"path":"three-parameters.html","id":"wisdom-3","chapter":"8 Three Parameters","heading":"8.2.1 Wisdom","text":"\nFIGURE 8.1: Wisdom\nWisdom begins look data.","code":""},{"path":"three-parameters.html","id":"eda-for-trains-1","chapter":"8 Three Parameters","heading":"8.2.1.1 EDA for trains","text":"Always explore data. demonstrate modeling three parameters, use trains data set primer.data package. Recall discussion Chapter 4. Enos (2014) randomly placed Spanish-speaking confederates nine train platforms around Boston, Massachusetts. Exposure Spanish-speakers – treatment – influenced attitudes toward immigration. reactions measured changes answers three survey questions. Load necessary libraries look data.data include information respondent’s gender, political affiliations, age, income . treatment indicates whether subject control treatment group. key outcomes attitudes toward immigration (att_start) (att_end) experiment. also measure ideology experiment, little change. Type ?trains read help page information variable. Let’s restrict attention subset variables.always smart look random samples data:att_end measure person’s attitude toward immigration. higher number means conservative, .e., exclusionary stance toward immigration United States.Pay attention variable types. make sense? Perhaps. certainly grounds suspicion. att_end double rather integer? values data appear integers, benefit variables doubles. party character variable treatment factor variable? intentional choices made creator tibble, .e., us. , mistakes. likely, choices mixture sensible arbitrary. Regardless, responsibility notice . can’t make good model without looking closely data using.TABLE 8.1: Data summaryVariable type: characterVariable type: factorVariable type: logicalVariable type: numericskim() shows us different values treatment factor. Unfortunately, character variables like party. ranges age att_end seem reasonable. Recall participants asked three questions immigration issues, allowed answer indicated strength agreement scale form 1 5, higher values indicating agreement conservative viewpoints. att_end sum responses three questions, liberal possible value 3 conservative 15.Always plot data.Democrats dataset Republicans. Democrats also span wider range ages Republicans.data limited. 115 observations, 2012 involving train commuters Boston. useful data today, populations around Boston, cities US? judgment, along advice colleagues, can guide .key concept idea “population.” larger population data (conceptually) drawn? interested age individuals data set, need inference. know everyone’s ages already. need tools like stan_glm() seek understand individuals data. data “representative,” least extent, larger population can use data answer questions.","code":"\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(skimr)\nlibrary(tidyverse)\nglimpse(trains)## Rows: 115\n## Columns: 15\n## $ treatment           <fct> Treated, Treated, Treated, Treated, Control, Treat…\n## $ att_start           <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6,…\n## $ att_end             <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7,…\n## $ gender              <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Femal…\n## $ race                <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"Whit…\n## $ liberal             <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FAL…\n## $ party               <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", …\n## $ age                 <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24…\n## $ income              <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 875…\n## $ line                <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framing…\n## $ station             <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"…\n## $ hisp_perc           <dbl> 0.026, 0.015, 0.019, 0.019, 0.019, 0.023, 0.030, 0…\n## $ ideology_start      <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3,…\n## $ ideology_end        <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3,…\n## $ income_in_thousands <dbl> 135, 105, 135, 300, 135, 88, 88, 135, 105, 135, 10…\nch8 <- trains %>% \n  select(age, att_end, party, income, treatment, liberal)\nch8 %>% \n  slice_sample(n = 5)## # A tibble: 5 x 6\n##     age att_end party      income treatment liberal\n##   <int>   <dbl> <chr>       <dbl> <fct>     <lgl>  \n## 1    55      12 Republican 250000 Control   FALSE  \n## 2    46       8 Democrat   250000 Control   FALSE  \n## 3    41       5 Democrat   135000 Control   TRUE   \n## 4    29       8 Democrat   250000 Control   TRUE   \n## 5    41       7 Democrat   250000 Control   FALSE\nch8 %>% \n  glimpse()## Rows: 115\n## Columns: 6\n## $ age       <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40, 53, …\n## $ att_end   <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 13, 8,…\n## $ party     <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Democrat\"…\n## $ income    <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 135000…\n## $ treatment <fct> Treated, Treated, Treated, Treated, Control, Treated, Contro…\n## $ liberal   <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE,…\nch8 %>% \n  skim()\nch8 %>%\n  ggplot(aes(x = party, y = age)) + \n  geom_jitter(width = 0.1, height = 0) + \n  labs(title = \"Age by Party Affiliation in Trains Dataset\",\n       subtitle = \"Where are the old Republicans?\",\n       x = \"Party\",\n       y = \"Age\")"},{"path":"three-parameters.html","id":"justice-2","chapter":"8 Three Parameters","heading":"8.2.2 Justice","text":"\nFIGURE 8.2: Justice\nJustice, data science, consists three topics: predictive versus causal modeling, Preceptor Table, mathematical formulation model.model age dependent variable predictive, causal, simple reason nothing, time, can change age. X years old. matter changed party registration Democrat Republican vice versa. age age.one potential outcome, .e., one outcome. potential outcome Democrat different potential outcome Republican.dealing non-causal model, focus predicting things. underlying mechanism connects age party less important brute statistical fact connection. Predictive models care little causality.good way looking Preceptor Table, seen . Unlike previous table Chapter 7, now two columns addition ID. Since data include Republicans Democrats world, every row filled . now know working predictive model. Recall:\\[\\text{outcome} = \\text{model} + \\text{model}\\]words, event depends explicitly described model well influences unknown us. Everything happens world result various factors, can consider model (know influences, data ).mathematics:\\[ y_i = \\beta_1 republican_i + \\beta_2 democrat_i + \\epsilon_i\\]\n\\[republican_i, democrat_i \\\\{0,1\\}\\] \n\\[republican_i +  democrat_i = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]Don’t panic dear poets philosophers, whole thing easier looks.left-hand side outcome, \\(y_i\\), variable explained. case, age individual population.left-hand side outcome, \\(y_i\\), variable explained. case, age individual population.right-hand side first part contained model: two similar terms added together. term consists parameter data point. betas two parameters: \\(\\beta_1\\) average age Republicans population \\(\\beta_2\\) average age Democrats population. \\(republican_i\\) \\(democrat_i\\) explanatory variables take values 1 0. person \\(\\) Republican \\(republican_i = 1\\) \\(democrat_i = 0\\). person \\(\\) Democrat \\(republican_i = 0\\) \\(democrat_i = 1\\). words, values mutually exclusive – Democrat, also Republican.right-hand side first part contained model: two similar terms added together. term consists parameter data point. betas two parameters: \\(\\beta_1\\) average age Republicans population \\(\\beta_2\\) average age Democrats population. \\(republican_i\\) \\(democrat_i\\) explanatory variables take values 1 0. person \\(\\) Republican \\(republican_i = 1\\) \\(democrat_i = 0\\). person \\(\\) Democrat \\(republican_i = 0\\) \\(democrat_i = 1\\). words, values mutually exclusive – Democrat, also Republican.last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part outcome called error term. simply difference outcome model predictions. includes factors influence someone’s age connected party affiliation. assume error follows normal distribution expected value 0 (meaning 0 average).last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part outcome called error term. simply difference outcome model predictions. includes factors influence someone’s age connected party affiliation. assume error follows normal distribution expected value 0 (meaning 0 average).small \\(\\)’s index number observations. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modeled factors person \\(\\).small \\(\\)’s index number observations. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modeled factors person \\(\\).model claim world works, just 115 individuals data people population seek draw inferences.model claim world works, just 115 individuals data people population seek draw inferences.Although terminology differs across academic fields, common term describe model like “regression.” “regressing” age party order see associated . formula “regression formula,” model “regression model.” terminology also apply model height Chapter 7.Although terminology differs across academic fields, common term describe model like “regression.” “regressing” age party order see associated . formula “regression formula,” model “regression model.” terminology also apply model height Chapter 7.model Chapter 7 sometimes called “intercept-” (interesting) parameter intercept. “two intercept” model , instead estimating average whole population, estimating two averages.model Chapter 7 sometimes called “intercept-” (interesting) parameter intercept. “two intercept” model , instead estimating average whole population, estimating two averages.","code":""},{"path":"three-parameters.html","id":"courage-2","chapter":"8 Three Parameters","heading":"8.2.3 Courage","text":"\nFIGURE 8.3: Courage\nCourage allows us translate math code.get posterior distributions three parameters, use stan_glm(), just Chapter 7.variable tilde, age, outcome.variable tilde, age, outcome.explanatory variable party. variable two values, ‘Democrat’ ‘Republican.’explanatory variable party. variable two values, ‘Democrat’ ‘Republican.’also added -1 end equation, indicating want intercept, otherwise added default.also added -1 end equation, indicating want intercept, otherwise added default.resulting output:partyDemocrat corresponds \\(\\beta_1\\), average age Democrats population. partyRepublican corresponds \\(\\beta_2\\), average age Republicans population. Since don’t really care posterior distribution \\(\\sigma\\), won’t discuss . Graphically:unknown parameters \\(\\beta_1\\) (partyDemocrat) \\(\\beta_2\\) (partyRepublican) still unknown. can never know true average age Democrats population. can calculate posterior probability distribution parameter. Comments:Democrats seem slightly older Republicans. true sample , almost (quite!) definition, true posterior probability distributions.Democrats seem slightly older Republicans. true sample , almost (quite!) definition, true posterior probability distributions.estimate average age Democrats population much precise Republicans five times many Democrats Republicans sample. central lesson Chapter 6 data related parameter, narrower posterior distribution .estimate average age Democrats population much precise Republicans five times many Democrats Republicans sample. central lesson Chapter 6 data related parameter, narrower posterior distribution .great deal overlap two distributions. surprised , truth, average age Republicans population greater Democrats? really. don’t enough data sure either way.great deal overlap two distributions. surprised , truth, average age Republicans population greater Democrats? really. don’t enough data sure either way.phrase “population” great deal work said , precisely, mean “population.” set people commuter platforms days 2012 experiment done? set people platforms, including ones never visited? set Boston commuter? Massachusetts residents? US residents? include people today, can draw inferences 2012? explore questions every model create.phrase “population” great deal work said , precisely, mean “population.” set people commuter platforms days 2012 experiment done? set people platforms, including ones never visited? set Boston commuter? Massachusetts residents? US residents? include people today, can draw inferences 2012? explore questions every model create.parameters \\(\\beta_1\\) \\(\\beta_2\\) can interpreted two ways. First, like parameters, part model. need estimate . , many cases, don’t really care value parameter . exact value \\(\\sigma\\), example, really matter. Second, parameters substantive interpretaion, \\(\\beta_1\\) \\(\\beta_2\\) average ages population. often case! Fortunately, models, can use functions like posterior_epred() posterior_predict() answer questions.Consider table shows sample 8 individuals. fitted values Republicans Democrats, model produces one fitted value condition. table shows just sample 8 individuals captures wide range residuals, making difficult predict age new individual. can get better picture unmodeled variation sample plot three variables individuals data.following three histograms show actual outcomes, fitted values, residuals people trains:three plots structured like equation table . value left plot sum one value middle plot plus one right plot.actual age distribution looks like normal distribution. centered around 43, standard deviation 12 years.actual age distribution looks like normal distribution. centered around 43, standard deviation 12 years.middle plot fitted values shows two adjacent spikes, represent estimates Democrats Republicans.middle plot fitted values shows two adjacent spikes, represent estimates Democrats Republicans.Since residuals plot represents difference two plots, distribution looks like first plot.Since residuals plot represents difference two plots, distribution looks like first plot.","code":"\nfit_1 <- stan_glm(age ~ party - 1, \n                    data = trains, \n                    seed = 17,\n                    refresh = 0)\nfit_1## stan_glm\n##  family:       gaussian [identity]\n##  formula:      age ~ party - 1\n##  observations: 115\n##  predictors:   2\n## ------\n##                 Median MAD_SD\n## partyDemocrat   42.6    1.2  \n## partyRepublican 41.2    2.7  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 12.3    0.8  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nfit_1 %>% \n  as_tibble() %>% \n  select(-sigma) %>% \n  mutate(Democrat = partyDemocrat, Republican = partyRepublican) %>%\n  pivot_longer(cols = Democrat:Republican,\n               names_to = \"parameter\",\n               values_to = \"age\") %>% \n  ggplot(aes(x = age, fill = parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Average Age\",\n         subtitle = \"More data allows for a more precise posterior for Democrats\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\noutcome <- ch8 %>% \n  ggplot(aes(age)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Age\",\n         y = \"Count\") \n\nfitted <- tibble(age = fitted(fit_1)) %>% \n  ggplot(aes(age)) +\n    geom_bar() +\n    labs(x = \"Fitted Values\",\n         y = NULL) +\n    scale_x_continuous(limits = c(20, 70)) \n\nres <- tibble(resids = residuals(fit_1)) %>% \n  ggplot(aes(resids)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Residuals\",\n         y = NULL) \n  \n\noutcome + fitted + res +\n  plot_annotation(title = \"Decomposition of Height into Fitted Values and Residuals\")"},{"path":"three-parameters.html","id":"temperance-2","chapter":"8 Three Parameters","heading":"8.2.4 Temperance","text":"\nFIGURE 8.4: Temperance\nRecall first questions began section:probability , Democrat shows train station, 50 years old?far tried model people data set whose real age already knew. helpful understand model, ultimate goal understand real world, people don’t yet know much . Temperance guides us make meaningful predictions become aware known unknown limitations.Start simple question, chances random Democrat 50 years old? First, create tibble desired input model. case tibble variable named “party” contains single observation value “Democrat.” bit different Chapter 7.Use posterior_predict() create draws posterior scenario. Note new posterior distribution consideration . unknown parameter, call \\(D_{age}\\), age Democrat. age randomly selected Democrat population next Democrat meet next Democrat interview train platform. definition “population” determines appropriate interpretation. Yet, regardless, \\(D_{age}\\) unknown parameter. one — like \\(\\beta_1\\), \\(\\beta_2\\), \\(\\sigma\\) — already created posterior probability distribution. need posterior_predict().posterior_predict() takes two arguments: model simulations run, tibble indicating many parameters want run simulations. case, model one Courage tibble one just created.might expect can use as_tibble() directly object returned posterior_predict(). Sadly, obscure technical reasons, won’t quite work. , need incantation mutate_all(.numeric) make sure resulting tibble well-behaved. command ensures every column tibble simple numeric vector, want.result draws posterior distribution age Democrat. important understand concrete person trains dataset - algorithm posterior_predict() simply uses existing data trains estimate posterior distribution.posterior distribution, can answer (almost) reasonable question. case, probability next Democrat 50 around 28%.Recall second question:group three Democrats three Republicans, age difference oldest Democrat youngest Republican?start creating tibble desired input. Note name column (“party”) observations (“Democrat,” “Republican”) must always exactly original data set. tibble well model can used arguments posterior_predict():6 columns: one person. posterior_predict() name columns, arranged order specified persons newobs: D, D, D, R, R, R. determine expected age difference, add code works posterior draws:plotting code similar seen :words, expect oldest Democrat 22 years older youngest Republican, surprised oldest Democrat actually younger youngest Republican group 6.","code":"\nnew_obs <- tibble(party = \"Democrat\")\npp <- posterior_predict(fit_1, newdata = new_obs) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric)\n\nhead(pp, 10)## # A tibble: 10 x 1\n##      `1`\n##    <dbl>\n##  1  50.8\n##  2  50.8\n##  3  26.4\n##  4  44.8\n##  5  35.4\n##  6  46.9\n##  7  39.0\n##  8  49.5\n##  9  41.3\n## 10  33.1\npp %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for a Random Democrat's Age\",\n         subtitle = \"Individual predictions are always more variable than expected values\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nsum(pp$`1` > 50) / nrow(pp)## [1] 0.27\nnewobs <- tibble(party = c(\"Democrat\", \"Democrat\", \"Democrat\", \n                        \"Republican\", \"Republican\",\"Republican\"))\n\nposterior_predict(fit_1, newdata = newobs) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric)## # A tibble: 4,000 x 6\n##      `1`   `2`   `3`   `4`   `5`   `6`\n##    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1  38.7  59.6  52.6  43.3  48.8  41.5\n##  2  34.3  56.0  31.4  45.8  59.5  39.1\n##  3  25.4  15.1  28.6  41.8  38.3  43.9\n##  4  56.4  27.2  33.8  34.6  60.1  48.2\n##  5  63.2  31.6  49.3  43.7  53.5  27.8\n##  6  24.6  45.0  57.1  69.3  64.1  53.8\n##  7  52.8  35.8  41.3  62.2  60.4  58.0\n##  8  42.9  40.6  57.4  38.5  34.7  30.1\n##  9  47.7  22.6  32.5  52.9  47.6  35.1\n## 10  40.7  25.3  49.7  40.6  42.2  48.2\n## # … with 3,990 more rows\npp <- posterior_predict(fit_1, newdata = newobs) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric) %>% \n  \n    # We don't need to rename the columns, but doing so makes the subsequest\n    # code much easier to understand. We could just have worked with columns 1,\n    # 2, 3 and so on. Either way, the key is to ensure that you correctly map\n    # the covariates in newobs to the columns in the posterior_predict object.\n  \n    set_names(c(\"dem_1\", \"dem_2\", \"dem_3\", \n                \"rep_1\", \"rep_2\", \"rep_3\")) %>% \n    rowwise() %>% \n  \n  # Creating three new columns. The first two are the highest age among\n  # Democrats and the lowest age among Republicans, respectively. The third one\n  # is the difference between the first two.\n  \n  mutate(dems_oldest = max(c_across(dem_1:dem_3)),\n         reps_youngest = min(c_across(rep_1:rep_3)),\n         age_diff = dems_oldest - reps_youngest)\n\npp## # A tibble: 4,000 x 9\n## # Rowwise: \n##    dem_1 dem_2 dem_3 rep_1 rep_2 rep_3 dems_oldest reps_youngest age_diff\n##    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>       <dbl>         <dbl>    <dbl>\n##  1  44.6  39.6  50.9  62.1  33.2  36.4        50.9          33.2    17.6 \n##  2  55.1  42.5  43.1  33.9  30.4  49.5        55.1          30.4    24.6 \n##  3  16.0  34.2  30.4  60.5  41.2  55.0        34.2          41.2    -7.03\n##  4  31.9  34.6  43.5  57.7  28.3  49.5        43.5          28.3    15.2 \n##  5  50.5  48.8  31.5  43.7  44.7  24.9        50.5          24.9    25.6 \n##  6  26.6  35.1  39.2  53.5  32.4  33.0        39.2          32.4     6.79\n##  7  47.4  32.7  54.8  44.6  12.0  30.4        54.8          12.0    42.8 \n##  8  48.4  30.9  42.1  35.4  36.2  50.0        48.4          35.4    13.0 \n##  9  55.8  52.3  40.0  55.3  30.3  37.1        55.8          30.3    25.5 \n## 10  55.3  52.3  26.4  58.2  46.7  40.4        55.3          40.4    14.8 \n## # … with 3,990 more rows\npp %>%  \n  ggplot(aes(x = age_diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Age Difference\",\n         subtitle = \"Oldest of three Democrats compared to youngest of three Republicans\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"three-parameters.html","id":"addendum","chapter":"8 Three Parameters","heading":"8.2.5 Addendum","text":"Instead parameterizing model without intercept, used one. case, math :\\[ y_i = \\beta_0  + \\beta_1 democratic_i + \\epsilon_i\\]\ninterpretations parameters different prior model. \\(\\beta_0\\) now average age Republicans. interpretation \\(\\beta_1\\) original set . \\(\\beta_1\\) now difference average age Republicans Democrats.fit model, use exact code , except without -1 formula argument.intercept, 42.6, partyDemocrat estimate first model. partyRepublican estimate, previously 41.0, now -1.5, meaning difference (allowing rounding) average age Democrats Republicans.Little else models different. fitted values residuals. posterior_predict() generate posterior predictive probabilty distributions. parameterization use matter much. able interpret meaning coefficients .","code":"\nstan_glm(age ~ party, \n         data = trains, \n         seed = 98,\n         refresh = 0)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      age ~ party\n##  observations: 115\n##  predictors:   2\n## ------\n##                 Median MAD_SD\n## (Intercept)     42.6    1.3  \n## partyRepublican -1.5    3.1  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 12.3    0.8  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"},{"path":"three-parameters.html","id":"att_end-treatment","chapter":"8 Three Parameters","heading":"8.3 att_end ~ treatment","text":", created predictive model: someone’s party affiliation, can make better guess age absence information party. nothing causal model. Changing someone’s party registration can change age. example, build causal model. Consider two questions:average treatment effect, exposing people Spanish-speakers, attitudes toward immigration?largest causal effect still 1 10 chance occurring?Models help us answer questions better without models, follow Cardinal Virtues.","code":""},{"path":"three-parameters.html","id":"wisdom-4","chapter":"8 Three Parameters","heading":"8.3.1 Wisdom","text":"\nFIGURE 8.5: Wisdom\nLook data! making model seeks explain att_end, plot att_end variable think connected .Treated individuals seem higher values att_end control individuals, although great deal variation .still using data Enos (2014). Yet world different place today! data 2012, four years Donald Trump’s election president, still relevant? Can generalize data Boston commuters people Massachusetts, people US? obvious answers questions. data never perfect match problem face data always old. world constantly changing. use old data, need make assumptions stability model world parameters estimating. Whether assumptions reasonable difficult question, one requires knowledge world world now . Math won’t save us.issue always: data data want come population? connection two, progress impossible. , case, willing consider population US residents last decade (including today), willing assume data set 115 individuals representative population, can use data create model answer question.Ethical issues tricky, least trickier context models dependent variable height age. (ethical issues conducting experiment first place non-trivial.) Assume make good model. prevent someone using information , say, influence voting donations? Imagine Republican Senate candidate hires Spanish-speakers ride commuter trains order shift voters’ attitudes toward immigration conservative direction. believes (correctly?) increase odds winning election. sort knowledge seek create? can make models. Indeed, purpose chapter show ! make models?","code":"\nch8 %>%\n  ggplot(aes(x = treatment, y = att_end)) + \n  geom_jitter() + \n  labs(title = \"Attitude End and Treatment in Enos (2014)\",\n       subtitle = \"Did the treatment make people more conservative?\",\n       x = \"Treatment\",\n       y = \"Attitude After Experiment\")"},{"path":"three-parameters.html","id":"justice-3","chapter":"8 Three Parameters","heading":"8.3.2 Justice","text":"\nFIGURE 8.6: Justice\ntwo elements Justice data science project remain : Preceptor Table mathematical formula.Preceptor Table model look similar one first half chapter, except now possible outcomes individual: att_end exposed treatment att_end exposed control.math model exactly math predictive model first part chapter, although change notation bit clarity.\\[ y_i = \\beta_1 treatment_i + \\beta_2 control_i + \\epsilon_i\\]\n\\[treatment_i, control_i \\\\{0,1\\}\\] \n\\[treatment_i +  control_i = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]Nothing changed, except meaning data items interpretations parameters.left-hand side outcome, \\(y_i\\), variable explained. case, person’s attitude toward immigration experiment complete. \\(y_i\\) takes integer values 3 15 inclusive.left-hand side outcome, \\(y_i\\), variable explained. case, person’s attitude toward immigration experiment complete. \\(y_i\\) takes integer values 3 15 inclusive.right-hand side first part contained model, consisting two similar terms. two terms stand Treated Control work follows. term consists parameter data point. \\(\\beta_1\\) average attitude toward immigration treated individuals — exposed Spanish-speakers — population. \\(\\beta_2\\) average attitude toward immigration control individuals — exposed Spanish-speakers — population. \\(x\\)’s explanatory variables take values 1 0. person \\(\\) Treated, \\(treatment_i = 1\\) \\(control_i = 0\\). person \\(\\) Control, \\(treatment_i = 0\\) \\(control_i = 1\\). words, binary variables mutually exclusive – Treated, also Control.right-hand side first part contained model, consisting two similar terms. two terms stand Treated Control work follows. term consists parameter data point. \\(\\beta_1\\) average attitude toward immigration treated individuals — exposed Spanish-speakers — population. \\(\\beta_2\\) average attitude toward immigration control individuals — exposed Spanish-speakers — population. \\(x\\)’s explanatory variables take values 1 0. person \\(\\) Treated, \\(treatment_i = 1\\) \\(control_i = 0\\). person \\(\\) Control, \\(treatment_i = 0\\) \\(control_i = 1\\). words, binary variables mutually exclusive – Treated, also Control.Note formula applies everyone population, just 115 people data. index \\(\\) just go 1 115. goes 1 \\(N\\), \\(N\\) number individuals population. Conceptually, everyone att_end treatment control.Note formula applies everyone population, just 115 people data. index \\(\\) just go 1 115. goes 1 \\(N\\), \\(N\\) number individuals population. Conceptually, everyone att_end treatment control.last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part called error term. simply difference outcome model predictions. particular case, includes factors influence someone’s attitude toward immigration explained treatment status. assume error follows normal distribution expected value 0.last part, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part called error term. simply difference outcome model predictions. particular case, includes factors influence someone’s attitude toward immigration explained treatment status. assume error follows normal distribution expected value 0.small \\(\\)’s index data set. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained predictor variables (\\(treatment\\) \\(control\\)) person \\(\\), along error term.small \\(\\)’s index data set. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained predictor variables (\\(treatment\\) \\(control\\)) person \\(\\), along error term.","code":""},{"path":"three-parameters.html","id":"courage-3","chapter":"8 Three Parameters","heading":"8.3.3 Courage","text":"\nFIGURE 8.7: Courage\nJustice satisfied, gather Courage fit model. Note , except change variable names, code exactly , predictive model age. Predictive models causal models use math code. differences, important, lie interpretation results, creation.treatmentTreated corresponds \\(\\beta_1\\). always, R , behind scenes, estimated entire posterior probability distribution \\(\\beta_1\\). graph distribution next section. basic print method objects can’t show entire distribution, gives us summary numbers: median MAD SD. Speaking roughly, expect 95% values posterior within two MAD SD’s median. words, 95% confident true, unknowable, average attitude toward immigration among Treated population 9.2 10.8.treatmentControl corresponds \\(\\beta_2\\). analysis applies. 95% confident true value average attitude toward immigration Control population 7.9 9.1.now, used Bayesian interpretation “confidence interval.” also intuitive meaning , outside academia, almost universal. truth . don’t know, sometimes can’t know, truth. confidence interval, associated confidence level, tells us likely truth lie within specific range. boss asks confidence interval, almost certainly using interpretation., contemporary academic research, phrase “confidence interval” usually given “Frequentist” interpretation. (biggest divide statistics Bayesians Frequentist interpretations. Frequentist approach, also known “Classical” statistics, dominant 100 years. power fading, textbook uses Bayesian approach.) Frequentist, 95% confidence interval means , apply procedure used infinite number future situations like , expect true value fall within calculated confidence intervals 95% time. academia, distinction sometimes made confidence intervals (use Frequentist interpretation) credible intervals (use Bayesian interpretation). won’t worry difference Primer.Let’s look full posteriors \\(\\beta_1\\) \\(\\beta_2\\).appears affect treatment change people’s attitudes conservative immigration issues. somewhat surprising!can decompose dependent variable, att_end two parts: fitted values residuals. two possible fitted values, one Treated one Control. residuals, always, simply difference outcomes fitted values.smaller spread residuals, better job model explaining outcomes.","code":"\nfit_2 <- stan_glm(att_end ~ treatment - 1, \n                      data = trains, \n                      seed = 45,\n                      refresh = 0)\n\nfit_2## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ treatment - 1\n##  observations: 115\n##  predictors:   2\n## ------\n##                  Median MAD_SD\n## treatmentTreated 10.0    0.4  \n## treatmentControl  8.5    0.3  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 2.8    0.2   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nfit_2 %>% \n  as_tibble() %>% \n  select(-sigma) %>% \n  pivot_longer(cols = treatmentTreated:treatmentControl,\n               names_to = \"Parameter\",\n               values_to = \"attitude\") %>% \n  ggplot(aes(x = attitude, fill = Parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Attitude Toward Immigration\",\n         subtitle = \"Treated individuals are more conservative\",\n         x = \"Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) + \n    theme_classic()"},{"path":"three-parameters.html","id":"temperance-3","chapter":"8 Three Parameters","heading":"8.3.4 Temperance","text":"\nFIGURE 8.8: Temperance\nRecall first question began section:average treatment effect, exposing people Spanish-speakers, attitudes toward immigration?Chapter 4 defined average treatment effect. One simple estimator average treatment effect difference \\(\\beta_1\\) \\(\\beta_2\\). , definition \\(\\beta_1\\) average attitude toward immigration, population, anyone, exposure treatment. , \\(\\beta_1 - \\beta_2\\) average treatment effect population, roughly 1.5. However, estimating posterior probability distribution parameter tricky, unless make use posterior distributions \\(\\beta_1\\) \\(\\beta_2\\). information, problem simple:true value average treatment effect much 2 little 1? course! likely value around 1.5, variation data smallness sample cause estimate imprecise. However, quite unlikely true average treatment effect zero.can use posterior_epred() answer question. Create tibble use done :posterior probability distribution created posterior_epred() one produced manipulating parameters directly.second question:largest effect size still 1 10 chance occurring?Create tibble can pass posterior_predict(). variables tibble passed newdata. Fortunately, tibble created just need question also.Consider result posterior_predict() two people, one treated one control. Take difference.Create graphic:case, looking distribution treatment effect single individual. different average treatment effect. particular, much variable. looking one row Preceptor Table. single individual, att_end can anywhere 3 15, treatment control. causal effect — difference two potential outcomes can, theory, anywhere -12 +12. extreme values rare, impossible.question, however, interested value 90th percentile.expect treatment effect magnitude common, , time, effects big bigger occur 10% time.","code":"\nfit_2 %>% \n  as_tibble() %>% \n  mutate(ate = treatmentTreated - treatmentControl) %>% \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nnewobs <- tibble(treatment = c(\"Treated\", \"Control\"))\n\npe <- posterior_epred(fit_2, newobs) %>% \n    as_tibble() %>% \n    mutate(ate = `1` - `2`)\n\npe## # A tibble: 4,000 x 3\n##      `1`   `2`   ate\n##    <dbl> <dbl> <dbl>\n##  1 10.0   8.56 1.47 \n##  2  9.94  8.33 1.61 \n##  3 10.0   8.48 1.56 \n##  4 10.6   8.28 2.36 \n##  5 10.6   8.29 2.32 \n##  6  9.26  8.81 0.449\n##  7 10.4   8.42 1.96 \n##  8 10.5   8.28 2.26 \n##  9 10.8   8.51 2.25 \n## 10 10.3   8.37 1.88 \n## # … with 3,990 more rows\npe %>% \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\npp <- posterior_predict(fit_2, \n                        newdata = newobs) %>%\n    as_tibble() %>%\n    mutate_all(as.numeric) %>% \n    mutate(te = `1` - `2`)\n  \npp## # A tibble: 4,000 x 3\n##      `1`   `2`      te\n##    <dbl> <dbl>   <dbl>\n##  1  4.60  6.95 -2.35  \n##  2 10.4  10.6  -0.264 \n##  3 14.0   7.52  6.53  \n##  4 12.2   6.29  5.86  \n##  5  8.29  8.34 -0.0451\n##  6 10.3   8.00  2.29  \n##  7  8.40  5.36  3.04  \n##  8 10.0   9.96  0.0732\n##  9  6.27  5.93  0.339 \n## 10  8.27  2.04  6.22  \n## # … with 3,990 more rows\npp %>% \n  ggplot(aes(x = te)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Treatment Effect for One Person\",\n         subtitle = \"Causal effects are more variable for indvduals\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nquantile(pp$te, prob = 0.9)## 90% \n## 6.9"},{"path":"three-parameters.html","id":"income-age","chapter":"8 Three Parameters","heading":"8.4 income ~ age","text":"far, created models predictor variable discrete, two possible values. party either “Democrat” “Republican.” treatment either “Treated” “Control.” Often times, however, predictor variable continuous. Fortunately, exact approach works case. Consider:expect income 40-year old?","code":""},{"path":"three-parameters.html","id":"wisdom-5","chapter":"8 Three Parameters","heading":"8.4.1 Wisdom","text":"\nFIGURE 8.9: Wisdom\nmaking model seeks explain income, plot variable think connected .data believable? Maybe? One imagine income recorded exactly , therefore, surprising many people exactly income. Rounding makes different numbers , . (almost) complete lack people reporting income $150,000 $250,000? real data science project, puzzle require investigation. now, ignore issue blithely press .","code":"\nch8 %>%\n  ggplot(aes(x = age, y = income)) + \n  geom_point() + \n  labs(title = \"Income and Age\",\n       subtitle = \"Older people make more money. Maybe?\",\n       x = \"Age\",\n       y = \"Income\") +\n  scale_y_continuous(labels = scales::dollar_format(accuracy = 1))"},{"path":"three-parameters.html","id":"justice-4","chapter":"8 Three Parameters","heading":"8.4.2 Justice","text":"\nFIGURE 8.10: Justice\nmathematics continuous predictor unchanged intercept-including example explored Section 8.2.5:\\[y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]comparing two people (persons 1 2), first one year older second, \\(\\beta_1\\) expected difference incomes. algebra simple. Start two individuals.\\[y_1 = \\beta_0  + \\beta_1 age_1\\]\n\\[y_2 = \\beta_0  + \\beta_1 age_2\\]\nwant difference , subtract second first, performing subtraction sides equals sign.\\[y_1 - y_2 = \\beta_0  + \\beta_1 age_1 - \\beta_0 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 age_1 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 (age_1 - age_2)\\], person 1 one year older person 2, :\\[y_1 - y_2 = \\beta_1 (age_1 - age_2)\\\\\ny_1 - y_2 = \\beta_1 (1)\\\\\ny_1 - y_2 = \\beta_1\\]algebra demonstrates \\(\\beta_1\\) ages. difference expected income two people aged 23 24 difference two people aged 80 81. plausible? Maybe. algebra lie. create model like , assumption making.Note careful imply increasing age one year “causes” increase income. nonsense! causation without manipulation. Since impossible change someone’s age, one potential outcome. one potential outcome, causal effect defined.","code":""},{"path":"three-parameters.html","id":"courage-4","chapter":"8 Three Parameters","heading":"8.4.3 Courage","text":"\nFIGURE 8.11: Courage\nuse stan_glm() usual.comparing two individuals, one 30 years old one 40, expect older earn $9,000 . far certain: 95% confidence interval ranges -$3,000 $20,000.good summary models.brief! one wants listen much prattle. One sentence gives number interest. second sentence provides confidence interval.brief! one wants listen much prattle. One sentence gives number interest. second sentence provides confidence interval.rounds appropriately. one wants hear bunch decimals. Use sensible units.rounds appropriately. one wants hear bunch decimals. Use sensible units.just blindly repeat numbers printed display. one year difference age, associated $900 difference income, awkward. (think.) decade comparison sensible.just blindly repeat numbers printed display. one year difference age, associated $900 difference income, awkward. (think.) decade comparison sensible.“comparing” great phrase start summary non-causal model. Avoid language like “associated ” “leads ” “implies” anything even hints causal claim.“comparing” great phrase start summary non-causal model. Avoid language like “associated ” “leads ” “implies” anything even hints causal claim.Consider usual decomposition outcome two parts: model error term.scores different fitted values. Indeed, greater number different fitted values different outcome values! often true models continuous predictor variables, age.","code":"\nfit_3 <- stan_glm(income ~ age, \n                  data = trains, \n                  seed = 28,\n                  refresh = 0)\n\nprint(fit_3, details = FALSE)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      income ~ age\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median   MAD_SD  \n## (Intercept) 104783.8  25577.1\n## age            877.7    578.5\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74186.1  4820.3\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"},{"path":"three-parameters.html","id":"temperance-4","chapter":"8 Three Parameters","heading":"8.4.4 Temperance","text":"\nFIGURE 8.12: Temperance\nRecall question:expect income random 40 year old?Given looking expected value, use posterior_epred().Plotting always.","code":"\nnewobs <- tibble(age = 40)\n\npe <- posterior_epred(fit_3, newdata = newobs) %>% \n  as_tibble() \n\npe## # A tibble: 4,000 x 1\n##        `1`\n##      <dbl>\n##  1 131988.\n##  2 132900.\n##  3 143766.\n##  4 149269.\n##  5 138030.\n##  6 135142.\n##  7 145487.\n##  8 147361.\n##  9 131460.\n## 10 147589.\n## # … with 3,990 more rows\npe %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Expected Income\",\n         subtitle = \"A 40-years old commuter earns around $140,000\",\n         x = \"Income\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"three-parameters.html","id":"liberal-income","chapter":"8 Three Parameters","heading":"8.5 liberal ~ income","text":"far chapter, considered continuous outcome variables. age, att_end income take variety values. None , truly, continuous, course. age reported integer value. att_end can , definition, take 13 distinct values. However, modeling perspective, matters 2 possible values.liberal, however, takes two values: TRUE FALSE. order model , must use binomial family. begin, always, questions:Among people income $100,000, proportion liberal?Assume group eight people, two make $100,000, two $200,000, two $300,000 two $400,000. many liberal?","code":""},{"path":"three-parameters.html","id":"wisdom-6","chapter":"8 Three Parameters","heading":"8.5.1 Wisdom","text":"Look data.one observation income $135,000 $250,000? don’t know. important thing notice , explore variables using model.second part Wisdom examining whether data representative overall population seek draw inferences. data 2012 includes Boston-area commuters. questions , apparently, today. world, US, Boston-area, just commuters select train stations? don’t know! questions precise.One key jobs data scientist guide people — colleagues, bosses clients — toward asking precise questions. must help translate English inference. translation requires precision. case, let’s assume questions reference Boston-area commuters 2021., data 2012 representative data 2021 seek draw inferences? Maybe? rarely correct answer question. things changed Boston things . job think changes adjust confidence final answers accordingly. , worst case scenario, way understand actual data desired data coming population, must stop project right .","code":"\nch8 %>%\n  ggplot(aes(x = income, y = liberal)) + \n  geom_jitter(width = 0, height = 0.1) + \n  labs(title = \"Liberal Status by Income in Trains Dataset\",\n       subtitle = \"Where the gap in the income distribution?\",\n       x = \"Income\",\n       y = \"Liberal\") +\n  scale_x_continuous(labels = scales::dollar_format())"},{"path":"three-parameters.html","id":"justice-5","chapter":"8 Three Parameters","heading":"8.5.2 Justice","text":"Recall discussion Section 7.5 logistic regression model use whenever outcome dependent variable binary/logical. math , care math. don’t, least much. Reminder:\\[p(\\text{Liberal}_i = \\text{TRUE}) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}\\]model two parameters, \\(\\beta_0\\) \\(\\beta_1\\). parameters simple interpretations, unlike parameters linear (gaussian) model.Recall fundamental structure data science problems:\\[\\text{outcome} = \\text{model} + \\text{model}\\]\nexact mathematics model — parameters, interpretations — just dross foundry inferences: unavoidable worth much time.Even math ignorable, causal versus predictive nature model . causal model predictive model? depends! causal assume can manipulate someone’s income, , , least two potential outcomes: person \\(\\)’s liberal status makes X dollars person \\(\\)’s liberal status makes Y dollars. Remember: causation without manipulation. definition causal effect difference two potential outcomes. one outcome, model can causal.many circumstances, don’t really care model causal . might want forecast/predict/explain outcome variable. case, whether can interpret influence variable causal irrelevant use variable.","code":""},{"path":"three-parameters.html","id":"courage-5","chapter":"8 Three Parameters","heading":"8.5.3 Courage","text":"Fitting logistic model easy. use arguments usual, family = binomial added.fit model, can look printed summary. Note use digits argument display digits printout.Fitted models tell us posterior distributions parameters formula defines model estimated. assuming model true. , discussed Chapter 6, assumption always false! model never perfectly accurate representation reality. , perfect, posterior distributions created \\(\\beta_0\\), \\(\\beta_1\\), perfect well.working linear model, often interpret meaning parameters, already done first three sections chapter. interpretations much harder logistic models math much less convenient. , won’t even bother try understand meaning parameters. However, can note \\(\\beta_1\\) negative, suggesting people higher incomes less likely liberal.","code":"\nfit_4 <- stan_glm(data = ch8,\n                  formula = liberal ~ income,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 365)\nprint(fit_4, digits = 6)## stan_glm\n##  family:       binomial [logit]\n##  formula:      liberal ~ income\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median   MAD_SD  \n## (Intercept)  5.6e-01  4.3e-01\n## income      -6.0e-06  3.0e-06\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"},{"path":"three-parameters.html","id":"temperance-5","chapter":"8 Three Parameters","heading":"8.5.4 Temperance","text":"Among people income $100,000, proportion liberal?Although model now logistic, steps answering question like linear/guassian model.pe tibble single vector. vector 4,000 draws posterior distribution proportion people, among make $100,000, liberal. population proportion thing probability single individual.Assume group eight people, two make $100,000, two $200,000, two $300,000 two $400,000. many liberal?trying predict outcome small number units, use posterior_predict(). complex questions ask, care need devote making newobs tibble. use rowwise() c_across() tricks earlier chapter.Study pp tibble. Understand component parts. first column, example, 4,000 draws posterior distribution liberal status random person income $100,000. Note draws zeroes ones. different draws seen ! also makes sense. making prediction binary variable, variable two possible values: zero one. , (reasonable!) predictions zero one.second column thing first column. 4,000 draws posterior distribution liberal status random person income $100,000. Yet also different values. thing different things, way rnorm(10) rnorm(10) thing — 10 draws standard normal distribution — different things values vary.third fourth columns different first two columns. 4,000 draws posterior distribution liberal status random person income $200,000. later columns. can answer difficult questions putting together simple building blocks, set draws posterior distribution. Recall discussion Section 2.8.total column simply sum first eight columns. created building blocks 8 columns draws four different posterior distributions, can switch focus row. Consider row 2. vector 8 numbers: 1 1 1 0 0 1 0 0. can treat vector unit analysis. might happen 8 people. first three might liberal, fourth liberal . row just one example might happen, one draw posterior distribution possible outcomes groups eight people incomes.can simplify draw taking sum, anything else might answer question confronted. Posterior distributions flexible individual numbers. can, less, just use algebra work .Graphically :always, truth. , tomorrow, meet 8 new people, specified incomes, certain number liberal. ideal Preceptor Table, just look number. data science required. Alas, don’t know truth. bets can create posterior distribution unknown value, done . need translate posterior English — “likely number liberals 2 3, total low zero high 5 also plausible. 6 liberals really surprising. 7 8 almost impossible.”two posterior probability distributions perfect? ! central message virtue Temperance. must demonstrate humility use models. Recall distinction unknown true distribution estimated distribution. first posterior distribution create understood every detail process accurately model . still know true unknown number, posterior distribution number perfect. Yet, model never perfect. making sorts assumptions behind scenes. assumptions plausible. Others less . Either way, estimated distribution graphed .central lesson Temperance : Don’t confuse estimated posterior () true posterior (want). Recognize unavoidable imperfections process. can still use estimated posterior — choice ? — cautious humble . suspect estimated posterior differs true posterior, humble cautious .","code":"\nnewobs <- tibble(income = 100000)\n\npe <- posterior_epred(fit_4, \n                      newdata = newobs) %>% \n  as_tibble()\npe %>% \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Proportion Liberal Among $100,000 Earners\",\n         subtitle = \"The population proportion is the same as the probability for any individual\",\n         x = \"Income\",\n         y = \"Probability of Being Liberal\") +\n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nnewobs <- tibble(income = c(rep(100000, 2),\n                            rep(200000, 2),\n                            rep(300000, 2),\n                            rep(400000, 2)))\n                 \n\npp <- posterior_predict(fit_4, \n                        newdata = newobs) %>% \n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  rowwise() %>% \n  mutate(total = sum(c_across()))\n\npp## # A tibble: 4,000 x 9\n## # Rowwise: \n##      `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8` total\n##    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1     0     0     0     1     1     0     0     0     2\n##  2     0     0     0     0     1     0     0     0     1\n##  3     1     0     0     0     0     0     0     0     1\n##  4     0     0     0     0     0     0     0     1     1\n##  5     0     1     0     0     0     1     0     0     2\n##  6     0     1     1     0     0     0     0     0     2\n##  7     1     0     1     0     0     0     0     1     3\n##  8     0     0     0     0     1     0     1     0     2\n##  9     1     1     0     0     0     0     0     0     2\n## 10     0     1     0     1     1     0     0     1     4\n## # … with 3,990 more rows\npp %>% \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Number of Liberals in Group with Varied Incomes\",\n         subtitle = \"Two is the most likely number, but values from 0 to 5 are plausible\",\n         x = \"Number of Liberals\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"three-parameters.html","id":"summary-8","chapter":"8 Three Parameters","heading":"8.6 Summary","text":"chapter, explored relationships different variables trains data set. built three predictive models one causal model.Similar previous chapters, first task use Wisdom . judge relevant data questions ask. reasonable consider data (e.g., income age data Boston commuters 2012) drawn population data want (e.g., income age data today entire US)? Probably?Justice necessary decide best way represent models make. little math won’t kill . use Courage translate models code. goal understand, generate posterior distributions parameters, interpret meaning. Temperance leads us final stage, using models answer questions.Key commands:Create model stan_glm().Create model stan_glm().Use posterior_epred() estimate expected values. e epred stands expected.Use posterior_epred() estimate expected values. e epred stands expected.Use posterior_predict() make forecasts individuals. variable predictions always greater variability expectations predictions can’t pretend \\(\\epsilon_i\\) zero.Use posterior_predict() make forecasts individuals. variable predictions always greater variability expectations predictions can’t pretend \\(\\epsilon_i\\) zero.draws posterior distribution outcome variable — whether expectation prediction — can manipulate draws answer question.Remember:Always explore data.Always explore data.Predictive models care little causality.Predictive models care little causality.Predictive models causal models use math code.Predictive models causal models use math code.“comparing” great phrase start summary non-causal model.“comparing” great phrase start summary non-causal model.Don’t confuse estimated posterior () true posterior (want). humble cautious use posterior.Don’t confuse estimated posterior () true posterior (want). humble cautious use posterior.","code":""},{"path":"four-parameters.html","id":"four-parameters","chapter":"9 Four Parameters","heading":"9 Four Parameters","text":"haste make progress — get way process building, interpreting using models — given short shrift messy details model building evaluation. chapter fills lacunae. also introduce models four parameters, including parallel slopes models.","code":""},{"path":"four-parameters.html","id":"transforming-variables","chapter":"9 Four Parameters","heading":"9.1 Transforming variables","text":"often convenient transform predictor variable model makes sense.","code":""},{"path":"four-parameters.html","id":"centering","chapter":"9 Four Parameters","heading":"9.1.1 Centering","text":"Recall model income function age.\\[ y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]fit using trains data primer.data. also using new package, broom.mixed, allows us tidy regression data plotting.nothing wrong model. Yet interpretation \\(\\beta_0\\), intercept regression, awkward. represents average income people age zero. useless! people zero age data. , even , weird think people taking commuter train Boston filling survey forms.easy, however, transform age variable makes intercept meaningful. Consider new variable, c_age, age minus average age sample. Using centered version age change predictions residuals model, make intercept easier interpret.intercept, 141,804, expected income someone c_age = 0, .e., someone average age data, around 42.","code":"\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nfit_1 <- stan_glm(formula = income ~ age, \n         data = trains, \n         refresh = 0,\n         seed = 9)\n\nprint(fit_1, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 103379.6  24657.3\n## age            904.7    561.4\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74166.4  5146.6\ntrains_2 <- trains %>% \n  mutate(c_age = age - mean(age))\n\nfit_1_c <- stan_glm(formula = income ~ c_age, \n                    data = trains_2, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_c, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 141804.2   7188.0\n## c_age          906.7    572.9\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74273.6  4882.0"},{"path":"four-parameters.html","id":"scaling","chapter":"9 Four Parameters","heading":"9.1.2 Scaling","text":"Centering — changing variable via addition/subtraction — often makes intercept easier interpret. Scaling — changing variable via multiplication/division — often makes easier interpret coefficients. common scaling method divide variable standard deviation.s_age age scaled standard deviation. change one unit s_age change one standard deviation age, 12. interpretation \\(\\beta_1\\) now:comparing two people, one 1 standard deviation worth years older , expect older person earn 11,000 dollars ., scaled without centering, intercept now back (nonsensical) meaning expected income people age 0.","code":"\ntrains_3 <- trains %>% \n  mutate(s_age = age / sd(age))\n\nfit_1_s <- stan_glm(formula = income ~ s_age, \n                    data = trains_3, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_s, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 103061.4  25235.6\n## s_age        11121.0   6921.6\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74124.8  4959.5"},{"path":"four-parameters.html","id":"z-scores","chapter":"9 Four Parameters","heading":"9.1.3 z-scores","text":"common transformation applies centering scaling. base R function scale() subtracts mean divides standard deviation. variable transformed “z-score,” meaning variable mean zero standard deviation one. Using z-scores makes interpretation easier, especially seek compare importance different predictors.two parameters easy interpret transformation.expected income someone average age, 42 study, 142,000 dollars.comparing two individuals differ age one standard deviation, 12 years study, older person expected earn 11,000 dollars younger.Note , using z-scores, often phrase comparison terms “sigmas.” One person “one sigma” older another person means one standard deviation older. simple enough, get used , also confusing since already using word “sigma” mean \\(\\sigma\\), standard deviation \\(\\epsilon_i\\). Alas, language something deal rather control. hear word “sigma” applied concepts, even sentence. Determine meaning context.","code":"\ntrains_4 <- trains %>% \n  mutate(z_age = scale(age))\n\nfit_1_z <- stan_glm(formula = income ~ z_age, \n                    data = trains_4, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_z, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 141650.2   7101.3\n## z_age        10899.1   6712.0\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74300.4  5019.0"},{"path":"four-parameters.html","id":"taking-logs","chapter":"9 Four Parameters","heading":"9.1.4 Taking logs","text":"often helpful take log predictor variables, especially cases distribution skewed. generally take log variables values strictly positive. log negative number defined. Consider number registered voters (rv13) polling stations kenya.experienced data scientists use log rv13 rather raw value. Comments:know “true” model. say model using raw value right wrong?know “true” model. say model using raw value right wrong?Check whether choice meaningfully affects answer question. Much time, won’t. , inferences often fairly “robust” small changes model. get answer rv13 log_rv13, one cares use.Check whether choice meaningfully affects answer question. Much time, won’t. , inferences often fairly “robust” small changes model. get answer rv13 log_rv13, one cares use.Follow conventions field. everyone X, probably X, unless good reason . reason, explain prominently.Follow conventions field. everyone X, probably X, unless good reason . reason, explain prominently.professionals, presented data distributed like rv13, take log. Professionals hate (irrationally?) outliers. transformation makes distribution look normal generally considered good idea.professionals, presented data distributed like rv13, take log. Professionals hate (irrationally?) outliers. transformation makes distribution look normal generally considered good idea.Many suggestions apply every aspect modeling process.","code":"\nx <- kenya %>% \n  filter(rv13 > 0)\n\nrv_p <- x %>% \n  ggplot(aes(rv13)) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Registered Voters\",\n         y = NULL) \n\nlog_rv_p <- x %>% \n  ggplot(aes(log(rv13))) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Log of Registered Voters\",\n         y = NULL) +\n    expand_limits(y = c(0, 175))\n\nrv_p + log_rv_p +\n  plot_annotation(title = 'Registered Votes In Kenya Communities',\n                  subtitle = \"Taking logs helps us deal with outliers\")"},{"path":"four-parameters.html","id":"adding-transformed-terms","chapter":"9 Four Parameters","heading":"9.1.5 Adding transformed terms","text":"Instead simply transforming variables, can add terms transformed versions variable. Consider relation height age nhanes. Let’s start dropping missing values.Fit plot simple linear model:good model, obviously.Adding quadratic term makes better. (Note need () creating squared term within formula argument.)Still, made use background knowledge creating variables. know people don’t get taller age 18 . Let’s create variables capture break.point take variables receive given. captains souls. transform variables needed.","code":"\nno_na_nhanes <- nhanes %>% \n  select(height, age) %>% \n  drop_na() \nnhanes_1 <- stan_glm(height ~ age,\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 47)\n\nno_na_nhanes %>% \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_1)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Children are shorter, but a linear fit is poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\nnhanes_2 <- stan_glm(height ~ age + I(age^2),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 33)\n\nno_na_nhanes %>% \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_2)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Quadratic fit is much better, but still poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\nnhanes_3 <- stan_glm(height ~ I(ifelse(age > 18, 18, age)),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 23)\n\nno_na_nhanes %>% \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_3)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Domain knowledge makes for better models\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")"},{"path":"four-parameters.html","id":"transforming-the-outcome-variable","chapter":"9 Four Parameters","heading":"9.1.6 Transforming the outcome variable","text":"Transforming predictor variables uncontroversial. matter much. Change continuous predictor variables \\(z\\)-scores won’t go far wrong. keep original form, take care interpretations. ’s good.Transforming outcome variable much difficult question. Imagine seek create model explains rv13 kenya tibble. transform ?Maybe? right answers. model rv13 outcome variable different model log(rv13) outcome. two directly comparable.Maybe? right answers. model rv13 outcome variable different model log(rv13) outcome. two directly comparable.Much advice regard taking logs predictor variables applies well.Much advice regard taking logs predictor variables applies well.See Gelman, Hill, Vehtari (2020)","code":""},{"path":"four-parameters.html","id":"interpreting-coefficients","chapter":"9 Four Parameters","heading":"9.1.7 Interpreting coefficients","text":"interpret coefficients, important know difference across unit within unit comparisons. compare across unit, meaning comparing Joe George, looking causal relationship. Within unit discussions, comparing Joe treatment versus Joe control, causal. means within unit interpretation possible causal models, studying one unit two conditions. talk two potential outcomes, discussing person unit two conditions.put terms Preceptor tables, within unit comparison looking one row data: data Joe control data Joe treatment. comparing one unit, (case) one person, two conditions. across unit comparison looking multiple rows data, focus differences across columns. looking differences without making causal claims. predicting, implying causation.magnitude coefficients linear models relatively easy understand. true logistic regressions. case, use Divide--Four rule: Take logistic regression coefficient (constant term) divide 4 get upper bound predictive difference corresponding unit difference variable. means , evaluating predictor helpful logistic regression , divide coefficient four.","code":""},{"path":"four-parameters.html","id":"selecting-variables","chapter":"9 Four Parameters","heading":"9.2 Selecting variables","text":"decide variables include model? one right answer question.","code":""},{"path":"four-parameters.html","id":"general-guidelines-for-selecting-variables","chapter":"9 Four Parameters","heading":"9.2.1 General guidelines for selecting variables","text":"deciding variables keep discard models, advice keep variable X following circumstances apply:variable large well-estimated coefficient. means, roughly, 95% confidence interval excludes zero. “Large” can defined context specific model. Speaking roughly, removing variable large coefficient meaningfully changes predictions model.variable large well-estimated coefficient. means, roughly, 95% confidence interval excludes zero. “Large” can defined context specific model. Speaking roughly, removing variable large coefficient meaningfully changes predictions model.Underlying theory/observation suggests X meaningfully connection outcome variable.Underlying theory/observation suggests X meaningfully connection outcome variable.variable small standard error relative size coefficient, general practice include model improve predictions. rule thumb keep variables estimated coefficient two standard errors away zero. variables won’t “matter” much model. , coefficients, although well-estimated, small enough removing variable model affect model’s predictions much.variable small standard error relative size coefficient, general practice include model improve predictions. rule thumb keep variables estimated coefficient two standard errors away zero. variables won’t “matter” much model. , coefficients, although well-estimated, small enough removing variable model affect model’s predictions much.standard error large relative coefficient, .e., magnitude coefficient two standard errors zero, find reason include model, probably remove variable model.standard error large relative coefficient, .e., magnitude coefficient two standard errors zero, find reason include model, probably remove variable model.exception rule variable relevant answering question . example, want know ending attitude toward immigration differs men women, need include gender model, even coefficient small closer zero two standard errors.exception rule variable relevant answering question . example, want know ending attitude toward immigration differs men women, need include gender model, even coefficient small closer zero two standard errors.standard field include X regressions.standard field include X regressions.boss/client/reviewer/supervisor wants include X.boss/client/reviewer/supervisor wants include X.Let’s use trains dataset evaluate helpful certain variables creating effective model, based guidelines .","code":""},{"path":"four-parameters.html","id":"variables-in-the-trains-dataset","chapter":"9 Four Parameters","heading":"9.2.2 Variables in the trains dataset","text":"look recommendations practice, let’s focus trains dataset. variables trains include gender, liberal, party, age, income, att_start, treatment, att_end. variables best include model?","code":""},{"path":"four-parameters.html","id":"att_end-treatment-att_start","chapter":"9 Four Parameters","heading":"9.2.3 att_end ~ treatment + att_start","text":"First, let’s look model left hand variable, att_end, two right side variables, treatment att_start.decide variables useful? First, let’s interpret coefficients.variable tilde, att_end, outcome.variable tilde, att_end, outcome.explanatory variables treatment, says whether commuter relieved treatment control conditions, att_start, measures attitude start study.explanatory variables treatment, says whether commuter relieved treatment control conditions, att_start, measures attitude start study.95% confidence interval att_end equal coefficient– 2– plus minus two standard errors. shows estimate att_end commuters treatment control group.95% confidence interval att_end equal coefficient– 2– plus minus two standard errors. shows estimate att_end commuters treatment control group.variable treatmentControl represents offset att_end estimate (Intercept). offset group people Control group. find estimated att_end group, must add median treatmentControl (Intercept) median.variable treatmentControl represents offset att_end estimate (Intercept). offset group people Control group. find estimated att_end group, must add median treatmentControl (Intercept) median.variable att_start measures expected difference att_end every one unit increase att_start.variable att_start measures expected difference att_end every one unit increase att_start.causal effect variable treatmentControl -1. means , compared predicted att_end groups treatment, control group predicted attitude one entire point lower. can see, large well estimated coefficient. Recall means, roughly, 95% confidence interval excludes zero. calculate 95% confidence interval, take coefficient plus minus two standard errors: -1.3 -0.5. can see, 95% confidence interval exclude zero, suggesting treatment worthy variable.addition meaningful, enough justify inclusion model, variable satisfies number qualifications:\n- variable small standard error.\n- variable considered indicator variable, separates two groups significance (treatment control) like study.variable att_start, coefficient 1, also meaningful. Due fact MAD_SD value 0, need find 95% confidence interval know –intuitively– variable large well estimated coefficient.Conclusion: keep variables! treatment att_start significant, well satisfying requirements guidelines. worthy inclusion model.","code":"\nfit_1_model <- stan_glm(att_end ~ treatment + att_start, \n                    data = trains, \n                    refresh = 0)\n\nfit_1_model## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ treatment + att_start\n##  observations: 115\n##  predictors:   3\n## ------\n##                  Median MAD_SD\n## (Intercept)       2.3    0.4  \n## treatmentControl -0.9    0.3  \n## att_start         0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.3    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"},{"path":"four-parameters.html","id":"income-age-liberal","chapter":"9 Four Parameters","heading":"9.2.4 income ~ age + liberal","text":"Now, look income function age liberal, proxy political party.Great! estimate income fall category liberalFALSE, well data right hand side variables age liberalTRUE.First, let’s interpret coefficients.variable tilde, income, outcome.variable tilde, income, outcome.explanatory variables liberal, resulting value TRUE FALSE, age, numeric variable.explanatory variables liberal, resulting value TRUE FALSE, age, numeric variable.(Intercept) estimating income liberal == FALSE. Therefore, estimated income commuters liberals age = 0. estimate age showing increase income every additional year age.(Intercept) estimating income liberal == FALSE. Therefore, estimated income commuters liberals age = 0. estimate age showing increase income every additional year age.estimate liberalTRUE represents offset predicted income commuters liberal. find estimate, must add coefficient (Intercept) value. see , average, liberal commuters make less money.estimate liberalTRUE represents offset predicted income commuters liberal. find estimate, must add coefficient (Intercept) value. see , average, liberal commuters make less money.important note looking causal relationship either explanatory variables. noting differences two groups, without considering causality. known across unit comparison. compare across unit looking causal relationship.comparing liberalTRUE (Intercept), recall (Intercept) calculating income case liberal == FALSE. can see, , coefficient liberalTRUE, -31,754, shows liberals dataset make less average non-liberals dataset. coefficient large relative (Intercept) , rough mental math, see 95% confidence interval excludes 0. Therefore, liberal helpful variable!variable age, however, appear meaningful impact (Intercept). coefficient age low 95% confidence interval exclude 0.Conclusion: definitely keep liberal! age less clear. really matter preference point.","code":"\nfit_2 <- stan_glm(income ~ age + liberal, \n                    data = trains, \n                    refresh = 0)\n\nfit_2## stan_glm\n##  family:       gaussian [identity]\n##  formula:      income ~ age + liberal\n##  observations: 115\n##  predictors:   3\n## ------\n##             Median   MAD_SD  \n## (Intercept) 111015.1  25409.9\n## age           1061.6    562.6\n## liberalTRUE -31754.0  14086.3\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 72800.0  4974.5\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"},{"path":"four-parameters.html","id":"liberal-iincome1e05-party","chapter":"9 Four Parameters","heading":"9.2.5 liberal ~ I(income/1e+05) + party","text":"now look liberal function transformed income party. reason transformed income due fact , without transformation, income makes predictions according small income disparities. helpful. enhance significance income, divided variable \\(100000\\). (Note need () creating income term within formula argument.)Recall , logistic models, (Intercept) nearly impossible interpret. evaluate impact variables, need use Divide--Four rule. instructs us take logistic regression coefficient (constant term) divide 4 get upper bound predictive difference corresponding unit difference variable. means , evaluating predictor helpful, logistic regression , divide coefficient four.partyRepublican, take coefficient, 0, divide 4: -0.12. upper bound predictive difference corresponding unit difference variable. Therefore, see partyRepublican meaningful. want variable model.variable income, however, seems less promising. addition resulting value Divide--Four rule low (-0.02), MAD_SD income also equal posterior median , indicating standard error large relative magnitude coefficient. Though may choose include income reasons unrelated impact model, appear worthy inclusion basis predictive value alone.Conclusion: variable party significant; include model. Unless strong reason include income, exclude .","code":"\nfit_3 <- stan_glm(liberal ~ I(income/1e+05) + party, \n                    data = trains, \n                    refresh = 0)\n\nfit_3## stan_glm\n##  family:       gaussian [identity]\n##  formula:      liberal ~ I(income/1e+05) + party\n##  observations: 115\n##  predictors:   3\n## ------\n##                 Median MAD_SD\n## (Intercept)      0.7    0.1  \n## I(income/1e+05) -0.1    0.1  \n## partyRepublican -0.5    0.1  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.5    0.0   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"},{"path":"four-parameters.html","id":"final-thoughts","chapter":"9 Four Parameters","heading":"9.2.6 Final thoughts","text":"Now looked three cases variables decided whether included, let’s discuss concept selecting variables generally.variables decided keep discard necessarily variables keep discard, variables data scientist keep discard. much easier keep variable build case discarding variable. Variables helpful even significant, particularly indicator variables may separate data two groups want study.process selecting variables – though guidelines – complicated. many reasons include variable model. main reasons exclude variable variable isn’t significant variable large standard error.","code":""},{"path":"four-parameters.html","id":"comparing-models-in-theory","chapter":"9 Four Parameters","heading":"9.3 Comparing models in theory","text":"Deciding variables include model subset larger question: decide model, set possible models, choose?Consider two models explain attitudes immigration among Boston commuters.seem like good models! results make sense. People liberal liberal attitudes immigration, expect att_end scores lower. also expect people provide similar answers two surveys administered week two apart. makes sense higher (conservative) values att_start also higher values att_end.choose models?","code":"\nfit_liberal <- stan_glm(formula = att_end ~ liberal,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nprint(fit_liberal, detail = FALSE)##             Median MAD_SD\n## (Intercept) 10.0    0.3  \n## liberalTRUE -2.0    0.5  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 2.7    0.2\nfit_att_start <- stan_glm(formula = att_end ~ att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 85)\n\nprint(fit_att_start, detail = FALSE)##             Median MAD_SD\n## (Intercept) 1.6    0.4   \n## att_start   0.8    0.0   \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.4    0.1"},{"path":"four-parameters.html","id":"better-models-make-better-predictions","chapter":"9 Four Parameters","heading":"9.3.1 Better models make better predictions","text":"obvious criteria comparing models accuracy predictions. example, consider use liberal predict att_end.two possible values liberal — TRUE FALSE — two predictions model make: 10 liberal == FALSE 8 liberal == TRUE. (points plot jittered.) individuals, perfect predictions. others, poor predictions. red circles plot illustrate areas predictions equal true values. can see, model isn’t great predicting attitude end. (Note two individuals liberal == TRUE, model thinks att_end == 8, att_end == 15. model got , wrong.)Consider second model, using att_start forecast att_end.att_end takes 13 unique values, model makes 13 unique predictions. predictions perfect! others wrong. red line shows points predictions match truth. Note individual predicted att_end around 9 actual value 15. big miss!Rather looking individual cases, need look errors predictions. Fortunately, prediction error thing residual, easy enough calculate.Let’s look square root average squared error.many different measures error might calculate. squared difference common historical reasons: mathematically tractable pre-computer age. calculated squared difference observation, can sum take average take square root average. produce relative ranking, last popular (less) corresponds estimated \\(\\sigma\\) linear model. Note measures ones produced Bayesian models created .Sadly, wise simply select model fits data best can misleading. , cheating! using data select parameters , using data , turning around “checking” see well model fits data. better fit! used pick parameters! danger overfitting.","code":"\ntrains %>% \n  mutate(pred_liberal = fitted(fit_liberal)) %>% \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Add a red circle where our predictions are most accurate (where the x and y\n  # values are the same, which is where our predictions = the true attitudes).\n  # pch = 1 makes the inside of the point translucent to show the number of\n  # correct predictions.\n  \n  geom_point(aes(x = 8, y = 8), \n             size = 20, pch = 1, \n             color = \"red\") +\n  geom_point(aes(x = 10, y = 10), \n             size = 20, pch = 1, \n             color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Liberals are less conservative\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\ntrains %>% \n  mutate(pred_liberal = fitted(fit_att_start)) %>% \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Insert red line where our predictions = the truth using geom_abline with an\n  # intercept, slope, and color.\n  \n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Survey responses are somewhat consistent\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\ntrains %>% \n  select(att_end, att_start, liberal) %>% \n  mutate(pred_lib = fitted(fit_liberal)) %>% \n  mutate(resid_lib = fitted(fit_liberal) - att_end) %>% \n  mutate(pred_as = fitted(fit_att_start)) %>% \n  mutate(resid_as = fitted(fit_att_start) - att_end)## # A tibble: 115 x 7\n##    att_end att_start liberal pred_lib resid_lib pred_as resid_as\n##      <dbl>     <dbl> <lgl>      <dbl>     <dbl>   <dbl>    <dbl>\n##  1      11        11 FALSE      10.0    -0.974    10.6    -0.399\n##  2      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n##  3       5         3 TRUE        8.02    3.02      4.08   -0.916\n##  4      11        11 FALSE      10.0    -0.974    10.6    -0.399\n##  5       5         8 TRUE        8.02    3.02      8.16    3.16 \n##  6      13        13 FALSE      10.0    -2.97     12.2    -0.769\n##  7      13        13 FALSE      10.0    -2.97     12.2    -0.769\n##  8      11        10 FALSE      10.0    -0.974     9.79   -1.21 \n##  9      12        12 FALSE      10.0    -1.97     11.4    -0.584\n## 10      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n## # … with 105 more rows\ntrains %>% \n  select(att_end, att_start, liberal) %>% \n  mutate(lib_err = (fitted(fit_liberal) - att_end)^2) %>% \n  mutate(as_err = (fitted(fit_att_start) - att_end)^2) %>% \n  summarize(lib_sigma = sqrt(mean(lib_err)),\n            as_sigma = sqrt(mean(as_err))) ## # A tibble: 1 x 2\n##   lib_sigma as_sigma\n##       <dbl>    <dbl>\n## 1      2.68     1.35"},{"path":"four-parameters.html","id":"beware-overfitting","chapter":"9 Four Parameters","heading":"9.3.2 Beware overfitting","text":"One biggest dangers data science overfitting, using model many parameters fits data well , therefore, works poorly data yet see. Consider simple example 10 data points.happens fit model one predictor?reasonable model. fit data particularly well, certainly believe higher values x associated higher values y. linear fit unreasonable.can also use lessons try quadratic fit adding \\(x^2\\) predictor.better model? Maybe?stop adding \\(x^2\\) regression? add \\(x^3\\), \\(x^4\\) way \\(x^9\\)? , fit much better.criteria cared well model predicts using data parameters estimated, model parameters always better. truly matters. matters well model works data used create model.","code":"\nnine_pred <- lm(y ~ poly(x, 9),\n                       data = ovrftng)\n\nnewdata <- tibble(x = seq(1, 10, by = 0.01),\n                  y = predict(nine_pred, \n                              newdata = tibble(x = x)))\n\novrftng %>% \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = newdata, \n              aes(x, y)) +\n    labs(title = \"`y` as a 9-Degree Polynomial Function of `x`\") +\n    scale_x_continuous(breaks = seq(2, 10, 2)) +\n    scale_y_continuous(breaks = seq(2, 10, 2)) "},{"path":"four-parameters.html","id":"better-models-make-better-predictions-on-new-data","chapter":"9 Four Parameters","heading":"9.3.3 Better models make better predictions on new data","text":"sensible way test model use model make predictions compare predictions new data. fitting model using stan_glm, use posterior_predict obtain simulations representing predictive distribution new cases. instance, predict someone’s attitude changes toward immigration among Boston commuters based political affiliation, want go test theories new Boston commuters.thinking generalization new data, important consider relevant new data context modeling problem. models used predict future , cases, can wait eventually observe future check good model making predictions. Often models used obtain insight phenomenon without immediate plan predictions. case Boston commuters example. cases, also interested whether learned insights part data generalizes parts data. example, know political attitudes informed future immigration stances Boston commuters, may want know conclusions generalize train commuters different locations.Even detected clear problems predictions, necessarily mean anything wrong model fit original dataset. However, need understand generalizing commuters.Often, like evaluate compare models without waiting new data. One can simply evaluate predictions observed data. since data already used fit model parameters, predictions optimistic.cross validation, part data used fit model rest data—hold-set—used proxy future data. natural prediction task future data, can think cross validation way assess generalization one part data another part.form cross validation, model re-fit leaving one part data prediction held-part evaluated. next section, look type cross validation called leave-one-(LOO) cross validation.","code":""},{"path":"four-parameters.html","id":"comparing-models-in-practice","chapter":"9 Four Parameters","heading":"9.4 Comparing models in practice","text":"compare models without waiting new data, evaluate predictions observed data. However, due fact data used fit model parameters, predictions often optimistic assessing generalization.cross validation, part data used fit model, rest data used proxy future data. can think cross validation way assess generalization one part data another part. ?can hold individual observations, called leave-one-(LOO) cross validation; groups observations, called leave-one-group-cross validation; use past data predict future observations, called leave-future-cross validation. perform cross validation, model re-fit leaving one part data prediction held-part evaluated.purposes, performing cross validation using leave-one-(LOO) cross validation.","code":""},{"path":"four-parameters.html","id":"cross-validation-using-loo","chapter":"9 Four Parameters","heading":"9.4.1 Cross validation using loo()","text":"compare models using leave-one-(LOO) cross validation, one piece data excluded model. model re-fit makes prediction missing piece data. difference predicted value real value calculated. process repeated every row data dataset.essence: One piece data excluded model, model re-fit, model attempts predict value missing piece, compare true value predicted value, assess accuracy model’s prediction. process occurs piece data, allowing us assess model’s accuracy making predictions.perform leave-one-(LOO) cross validation, using function loo() R package. determine model superior purposes.First, refamiliarize first model, fit_liberal.Now, perform loo() model look results.mean?elpd_loo estimated log score along standard error representing uncertainty due using 115 data points.p_loo estimated “effective number parameters” model.looic LOO information criterion, −2 elpd_loo, compute comparability deviance.purposes, mostly need focus elpd_loo. Let’s explain, depth, information means.Basically, run loo(), telling R take piece data dataset, re-estimate parameters, predict value missing piece data. value elpd_loo() based close estimate truth. Therefore, elpd_loo() values inform us effectiveness model predicting data seen .higher value elpd_loo, better model performs. means , comparing models, want select model higher value elpd_loo.Let’s turn attention second model. begin, let’s observe qualities fit_att_start .Great! Now, let’s perform loo() model.elpd_loo value model -201.7. higher elpd_loo att_liberal, implying model superior. However, can’t see estimates together. simpler way calculate model better?Actually, yes! Using function loo_compare(), can compare models directly.","code":"\nfit_liberal## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ liberal\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept) 10.0    0.3  \n## liberalTRUE -2.0    0.5  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 2.7    0.2   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nloo_liberal <- loo(fit_liberal)\n\nloo_liberal## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -279.4  7.5\n## p_loo         2.9  0.5\n## looic       558.9 15.0\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\nfit_att_start## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ att_start\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept) 1.6    0.4   \n## att_start   0.8    0.0   \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.4    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nloo_att_start <- loo(fit_att_start) \n\nloo_att_start## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -201.7 12.5\n## p_loo         4.2  1.8\n## looic       403.4 25.1\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details."},{"path":"four-parameters.html","id":"comparing-models-using-loo_compare","chapter":"9 Four Parameters","heading":"9.4.2 Comparing models using loo_compare()","text":"compare two models directly, can use function loo_compare two loo objects created . calculate difference elpd_loo() models us, making job easier:value elpd_diff equal difference elpd_loo two models. These_diff shows difference standard error.interpret results directly, important note first row superior model. values elpd_diff att_start 0, columns show offset estimates compared better model. reiterate: better model compared , values 0. following rows show offset elpd se values less effective model, fit_liberal, effective model, fit_att_start.better model clear: fit_att_start. Therefore, attitude start trains study significant predicting final attitude compared variable liberal, analog political affiliation.seen, loo_compare shortcut comparing two models. deciding two models, loo_compare() great way simplify decision.value loo_compare() small? general practice, differences smaller four hard distinguish noise. words: elpd_diff less 4, advantage one model .","code":"\nloo_compare(loo_att_start, loo_liberal)##               elpd_diff se_diff\n## fit_att_start   0.0       0.0  \n## fit_liberal   -77.7      12.0"},{"path":"four-parameters.html","id":"testing-is-nonsense","chapter":"9 Four Parameters","heading":"9.5 Testing is nonsense","text":"always, important look practices professionals reasons may choose follow tactics. instance, continued problem hypothesis testing. hypothesis testing, assert null hypothesis \\(H_0\\) data alternative hypothesis \\(H_a\\).performing hypothesis testing, either reject hypothesis reject . qualifications rejecting met 95% confidence interval excludes null hypothesis. hypothesis included 95% confidence interval, reject . case “insignificant” results, p > 0.5, also can’t “reject” null hypothesis. However, mean accept .premise hypothesis testing answer specific question – one may even particularly relevant understanding world – data. , problems hypothesis testing?\n- Rejecting rejecting hypotheses doesn’t helps us answer real questions.\n- fact difference “significant” relevance use posterior make decisions.\n- Statistical significance equal practical importance.\n- reason test can summarize providing full posterior probability distribution.","code":""},{"path":"four-parameters.html","id":"parallel-lines","chapter":"9 Four Parameters","heading":"9.6 Parallel lines","text":"conclude chapter, look four parameter model. model use measure att_end function liberal att_start treatment combination variables.","code":""},{"path":"four-parameters.html","id":"wisdom-7","chapter":"9 Four Parameters","heading":"9.6.1 Wisdom","text":"\nFIGURE 9.1: Wisdom\nmaking model seeks explain att_end, plot variables think connected :data believable? Maybe? One imagine att_end predicted fairly well att_start. makes sense data points, show much difference attitudes. great disparities attitude shown individual starting attitude 9 ending attitude around 15? real data science project, require investigation. now, ignore issue blithely press .Another component Wisdom population. concept “population” subtle important. population set commuters data. dataset. population larger — potentially much larger — set individuals want make inferences. parameters models refer population, dataset.many different populations, \\(\\mu\\), might interested. instance:population Boston commuters specific train time included dataset.population Boston commuters specific train time included dataset.population Boston commuters.population Boston commuters.population commuters United States.population commuters United States.populations different, different \\(\\mu\\). \\(\\mu\\) interested depends problem trying solve. judgment call, matter Wisdom, whether data “close enough” population interested justify making model.major part Wisdom deciding questions can’t answer data don’t .","code":"\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  geom_point() +\n  labs(title = \"Attitude End Compared with Attitude Start and Liberal\",\n       x = \"Attitude at Start of Study\",\n       y = \"Attitude at End of Study\",\n       color = \"Liberal?\")"},{"path":"four-parameters.html","id":"justice-6","chapter":"9 Four Parameters","heading":"9.6.2 Justice","text":"\nFIGURE 9.2: Justice\nNow considered connection data predictions seek make, need consider model.First: model causal predictive? Recall model measures att_end function liberal att_start. variables liberal att_start involve control treatment dynamic. manipulation variables. Given causation without manipulation, predictive model.making inferences groups people according political affiliation starting attitude. measuring causality, predicting outcomes.creating parallel slops model, use basic equation line:\\[y_i = \\beta_0  + \\beta_1 x_{1,} + \\beta_2 x_{2,}\\]\\(y = att\\_end\\), \\(x_1 = att\\_start\\), \\(x\\_2 = liberal\\), equations follows:liberal = FALSE:\\[y_i = \\beta_0  + \\beta_1 x_{1,}\\]\nequates , y = b + mx form:\\[y_i = intercept +  \\beta_1 att\\_start_i\\]liberal = TRUE:\\[y_i = (\\beta_0  + \\beta_2) + \\beta_1 x_{1,}\\]equates , y = b + mx form:\\[y_i = (intercept + liberal\\_true) + \\beta_1 att\\_start_i\\]","code":""},{"path":"four-parameters.html","id":"courage-6","chapter":"9 Four Parameters","heading":"9.6.3 Courage","text":"\nFIGURE 9.3: Courage\nuse stan_glm usual. Using stan_glm, create model, fit_1.remind , recall (Intercept) representing att_end value cases liberal = FALSE treatment equal Control. next row, liberalTRUE gives median value represents offset prediction compared (Intercept). words, true intercept cases liberal = TRUE represented \\((Intercept) + liberalTRUE\\). value treatmentControl offset att_end Control group.find intercepts, need tidy regression using tidy() function broom.mixed package. tidy data extract values create parallel slopes model. parallel slopes model allows us visualize multi-variate Bayesian modeling (.e. modeling one explanatory variable). complicated way saying visualize fitted model created way allows us see intercepts slopes two different groups, liberalTRUE liberalFALSE:Now, can define following terms—- liberal_false_intercept liberal_false_att_slope; liberal_true_intercept liberal_true_att_slope:’ve done extracted values intercepts slopes, named separated two groups. allows us create geom_abline object takes unique slope intercept value, can separate liberalTRUE liberalFALSE observations.parallel slopes model. done, essentially, created unique line liberalTRUE liberalFALSE observe differences groups related attitude start attitude end.can see, commuters liberal tend start slightly higher values att_start. Commuters liberal tend lower starting values att_start.Now, want look another model? judge whether can superior model? Let’s create another object, using stan_glm, looks att_end function treatment att_start.interpret briefly:(Intercept) representing att_end value cases treatment equal Control.(Intercept) representing att_end value cases treatment equal Control.treamtmentControl gives median value represents offset prediction compared (Intercept). words, true intercept cases treatment = Control represented \\((Intercept) + treatmentControl\\).treamtmentControl gives median value represents offset prediction compared (Intercept). words, true intercept cases treatment = Control represented \\((Intercept) + treatmentControl\\).att_start represents slope groups representing unit change.att_start represents slope groups representing unit change.important point models causal. including variable treatment, measured causal effect condition. different prior parallel slopes model, modeling prediction, causation.see models compare performance, perform leave-one-(LOO) cross validation .Perform loo() second model:Recall relevant data data elpd_loo. estimates elpd_loo vary quite bit. Recall superior model value elpd_loo() closer 0. standard error (SE) models also differs . compare directly, use loo_compare.Recall , loo_compare(), resulting data shows superior model first, values 0 elpd_diff se_diff, since compares models best option. values elpd_diff se_diff fit_1 show difference models. can see, fit_2, model looks treatment + att_start, better.certain can better? Note difference two models quite two standard errors. , reasonable possibility difference due chance.","code":"\nfit_1 <- stan_glm(formula = att_end ~ liberal + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nfit_1## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ liberal + att_start\n##  observations: 115\n##  predictors:   3\n## ------\n##             Median MAD_SD\n## (Intercept)  1.9    0.5  \n## liberalTRUE -0.3    0.3  \n## att_start    0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.4    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\n# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy <- fit_1 %>% \n  tidy() %>% \n  select(term, estimate)\n\ntidy## # A tibble: 3 x 2\n##   term        estimate\n##   <chr>          <dbl>\n## 1 (Intercept)    1.93 \n## 2 liberalTRUE   -0.300\n## 3 att_start      0.799\n# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept <- tidy$estimate[1]\nliberal_true <- tidy$estimate[2]\natt_start <- tidy$estimate[3]\n# Recall that the (Intercept) shows us the estimate for the case where liberal =\n# FALSE. We want to extract the liberal_false_intercept to indicate where the\n# intercept in our visualization should be. The slope for this case, and for the\n# liberal = TRUE case, is att_start.\n\nliberal_false_intercept <- intercept\nliberal_false_att_slope <- att_start\n\n#  When wanting the intercept for liberal = TRUE, recall that the estimate for\n#  liberalTRUE is the offset from our (Intercept). Therefore, to know the true\n#  intercept, we must add liberal_true to our intercept.\n\nliberal_true_intercept <- intercept + liberal_true\nliberal_true_att_slope <- att_start\n# From the dataset trains, use att_start for the x-axis and att_end for\n# the y-axis with color as liberal. This will split our data into two color\n# coordinates (one for liberal = TRUE and one for liberal = FALSE)\n\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the liberal false values. Set the intercept\n  # equal to our previously created liberal_false_intercept, while setting slope\n  # equal to our previously created liberal_false_att_slope. The color call is\n  # for coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = liberal_false_intercept,\n              slope = liberal_false_att_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the liberal TRUE values. Set the intercept\n  # equal to our previously created liberal_true_intercept, while setting slope\n  # equal to our previously created liberal_true_att_slope. The color call is\n  # for teal, to match the colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = liberal_true_intercept,\n              slope = liberal_true_att_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Parallel Slopes Model\",\n       x = \"Attitude at Start\", \n       y = \"Attitude at End\", \n       color = \"Liberal\") \nfit_2 <- stan_glm(formula = att_end ~ treatment + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 56)\n\nfit_2## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ treatment + att_start\n##  observations: 115\n##  predictors:   3\n## ------\n##                  Median MAD_SD\n## (Intercept)       2.4    0.4  \n## treatmentControl -1.0    0.2  \n## att_start         0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.3    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nL1 <- loo(fit_1)\n\nL1## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -202.3 13.1\n## p_loo         5.5  2.3\n## looic       404.6 26.1\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     114   99.1%   2531      \n##  (0.5, 0.7]   (ok)         1    0.9%   157       \n##    (0.7, 1]   (bad)        0    0.0%   <NA>      \n##    (1, Inf)   (very bad)   0    0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\nL2 <- loo(fit_2)\n\nL2## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -195.3 12.2\n## p_loo         5.1  1.8\n## looic       390.5 24.5\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     114   99.1%   1625      \n##  (0.5, 0.7]   (ok)         1    0.9%   206       \n##    (0.7, 1]   (bad)        0    0.0%   <NA>      \n##    (1, Inf)   (very bad)   0    0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\nloo_compare(L1, L2)##       elpd_diff se_diff\n## fit_2  0.0       0.0   \n## fit_1 -7.0       3.8"},{"path":"four-parameters.html","id":"temperance-6","chapter":"9 Four Parameters","heading":"9.6.4 Temperance","text":"really care data haven’t seen yet, mostly data tomorrow. world changes, always ? doesn’t change much, maybe OK. changes lot, good model ? general, world changes . means forecasts uncertain naive use model might suggest.\ncreated (checked) model, now use model answer questions. Models made use, beauty. world confronts us. Make decisions must. decisions better ones use high quality models help make .Preceptor’s Posterior posterior calculate assumptions made Wisdom Justice correct. Sadly, never ! , can never know Preceptor’s Posterior. posterior , hope, close-ish approximation Preceptor’s Posterior.","code":""},{"path":"four-parameters.html","id":"summary-9","chapter":"9 Four Parameters","heading":"9.7 Summary","text":"chapter, covered number topics important effectively creating models.Key commands:\n- Create model using stan_glm().\n- creating model, can use loo() perform leave-one-cross validation. assesses effectively model makes predictions data seen yet.\n- command loo_compare() allows us compare two models, see one performs better leave-one-cross validation. superior model makes better predictions.Remember:\n- can transform variables – centering, scaling, taking logs, etc. – make sensible. Consider using transformation intercept awkward. instance, intercept age represents estimate people age zero, might consider transforming age easier interpret.\n- selecting variables include model, follow rule: keep variable large well-estimated coefficient. means 95% confidence interval excludes zero. Speaking roughly, removing variable large coefficient meaningfully changes predictions model.\n- compare across unit, meaning comparing Joe George, looking causal relationship. Within unit discussions, comparing Joe treatment versus Joe control, causal. means within unit interpretation possible causal models, studying one unit two conditions.\n- talk two potential outcomes, discussing person unit two conditions.","code":""},{"path":"five-parameters.html","id":"five-parameters","chapter":"10 Five Parameters","heading":"10 Five Parameters","text":"Last chapter, studied four parameters: models studied multiple right-hand side variables . next step model building education learn interactions. effect treatment relative control almost never uniform. effect might bigger women men, smaller rich relative poor. technical term effects “heterogeneous,” just Ph.D.’ese “different.” enough data, effects heterogeneous. Causal effects, least social science, always vary across units. model reality, rely interactions, allowing effect size differ. applies predictive models. relationship outcome variable \\(y\\) predictor variable \\(x\\) rarely constant. relationship varies based values variables. take account interactions, need models least 5 parameters.Packages:Consider following questions:many years expect two gubernatorial candidates — one male one female, 10 years older average candidate — live election? different lifespans ? broadly, long candidates, general, live election? winning election affect longevity?Note far come Primer. difficult questions, involving issues prediction causation. Yet, follow Cardinal Virtues, can provide sophisticated answers.","code":"\nlibrary(primer.data)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(broom.mixed)"},{"path":"five-parameters.html","id":"wisdom-8","chapter":"10 Five Parameters","heading":"10.1 Wisdom","text":"\nFIGURE 10.1: Wisdom\nRecall important aspects wisdom: Preceptor Table, EDA (exploratory data analysis), population, Population Table. always, start Preceptor Table — table data make questions answerable mere arithmetic (inferences).","code":""},{"path":"five-parameters.html","id":"the-preceptor-table","chapter":"10 Five Parameters","heading":"10.1.1 The Preceptor Table","text":"create Preceptor Table, must first revisit questions:many years expect two gubernatorial candidates — one male one female, 10 years older average candidate — live election? different lifespans ? broadly, long candidates, general, live election? winning election affect longevity?(imagined) dataset make questions easy solve little bit math? Well, obviously need data gubernatorial candidate elections United States. also need know dates birth, age time election, age time death, data age time death minus age time election. pieces information, answer questions simple math.Also, idealized table, know age time death assuming victory age time death assuming loss. possible real world due Fundamental Problem Causal Inference — observe unit two different conditions (victory loss).sample:rows every gubernatorial election candidate U.S. history. may want details, election year. merely sketch ideal dataset. Now know “perfect” reality, data actually work ?","code":""},{"path":"five-parameters.html","id":"eda-of-governors","chapter":"10 Five Parameters","heading":"10.1.2 EDA of governors","text":"primer.data package includes governors data set features demographic information candidates governor United States. Barfort, Klemmensen, Larsen (2020) gathered data concluded winning gubernatorial election increases candidate’s lifespan.14 variables 1,092 observations. Chapter, looking variables last_name, year, state, sex, lived_after, election_age, won, close_race.election_age lived_after many years candidate lived election, respectively. consequence, politicians already deceased included data set. means handful observations elections last 20 years. candidates time period still alive , therefore, excluded. created won variable indicate whether candidate won election. define close_race true winning margin less 5%.One subtle issue: candidate included multiple times? example:now, leave multiple observations single person.First, let’s sample dataset.might expect, sex often “Male.” precise inspecting data, let’s skim() dataset.TABLE 10.1: Data summaryVariable type: characterVariable type: logicalVariable type: numericskim() groups variable together type, provides analysis variable. also given histograms numeric data.Looking histogram year, see skewed right — meaning data bunched left smaller tail right — half observations election years 1945 1962. makes sense logically, looking deceased candidates, candidates recent elections likely still alive.using data set, left-side variable lived_after. trying understand/predict many years candidate live election.Note rough line see observations. might ? looking year elected years lived post-election, missing data upper right quadrant due fact impossible elected post-2000 lived 21 years. Simply put: “edge” data represents, approximately, years candidate lived, still died, given year elected.reason data slanted downward maximum value scenario greater earlier years. , candidates ran governor earlier years live long time election still died prior data set creation, giving higher lived_after values ran office recent years. edge scatter plot perfectly straight , many election years, candidate decency die just data collection. reason observations later years fewer recent candidates died.begin visualizing lived_after data, inspect difference years lived post election male female candidates.plot shows men live much longer, average, women election. intuitive explanation might ?","code":"\nglimpse(governors)## Rows: 1,092\n## Columns: 14\n## $ state        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"A…\n## $ year         <int> 1946, 1946, 1950, 1954, 1954, 1958, 1962, 1966, 1966, 197…\n## $ first_name   <chr> \"James\", \"Lyman\", \"Gordon\", \"Tom\", \"James\", \"William\", \"G…\n## $ last_name    <chr> \"Folsom\", \"Ward\", \"Persons\", \"Abernethy\", \"Folsom\", \"Long…\n## $ party        <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Republican\", \"Demo…\n## $ sex          <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n## $ died         <date> 1987-11-21, 1948-12-17, 1965-05-29, 1968-03-07, 1987-11-…\n## $ status       <chr> \"Challenger\", \"Challenger\", \"Challenger\", \"Challenger\", \"…\n## $ win_margin   <dbl> 77.3, -77.3, 82.2, -46.7, 46.7, -77.5, 100.0, -34.3, 34.3…\n## $ region       <chr> \"South\", \"South\", \"South\", \"South\", \"South\", \"South\", \"So…\n## $ population   <dbl> 2906000, 2906000, 3058000, 3014000, 3014000, 3163000, 332…\n## $ election_age <dbl> 38, 79, 49, 47, 46, 33, 43, 48, 40, 51, 68, 55, 45, 52, 6…\n## $ death_age    <dbl> 79, 81, 63, 60, 79, 88, 79, 99, 42, 79, 75, 79, 76, 81, 7…\n## $ lived_after  <dbl> 41.0, 2.1, 14.6, 13.3, 33.0, 54.5, 35.9, 51.0, 1.5, 27.9,…\nch10 <- governors %>% \n  mutate(won = ifelse(win_margin > 0, TRUE, FALSE)) %>% \n  mutate(close_race = ifelse(abs(win_margin) < 5, TRUE, FALSE)) %>% \n  select(last_name, year, state, sex, lived_after, election_age, won, close_race)\nch10 %>% \n  filter(last_name == \"Cuomo\")## # A tibble: 4 x 8\n##   last_name  year state    sex   lived_after election_age won   close_race\n##   <chr>     <int> <chr>    <chr>       <dbl>        <dbl> <lgl> <lgl>     \n## 1 Cuomo      1982 New York Male         32.2         50.4 TRUE  TRUE      \n## 2 Cuomo      1986 New York Male         28.2         54.4 TRUE  FALSE     \n## 3 Cuomo      1990 New York Male         24.2         58.4 TRUE  FALSE     \n## 4 Cuomo      1994 New York Male         20.2         62.4 FALSE TRUE\nch10 %>% \n  slice_sample(n = 5)## # A tibble: 5 x 8\n##   last_name  year state        sex    lived_after election_age won   close_race\n##   <chr>     <int> <chr>        <chr>        <dbl>        <dbl> <lgl> <lgl>     \n## 1 Ristine    1964 Indiana      Male          44.6         44.8 FALSE FALSE     \n## 2 Sundlun    1986 Rhode Island Male          24.7         66.8 FALSE FALSE     \n## 3 Richards   1990 Texas        Female        15.9         57.2 TRUE  TRUE      \n## 4 Turner     1946 Oklahoma     Male          26.6         52.0 TRUE  FALSE     \n## 5 Williams   1948 Michigan     Male          39.2         37.7 TRUE  FALSE\nskim(ch10)\nch10 %>%\n  ggplot(aes(x = year, y = lived_after)) +\n  geom_point() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Candidates who died more recently can't have lived for long post-election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Year Elected\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() \nch10 %>%\n  ggplot(aes(x = sex, y = lived_after)) +\n  geom_boxplot() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Male candidates live much longer after the election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Gender\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() "},{"path":"five-parameters.html","id":"population-1","chapter":"10 Five Parameters","heading":"10.1.3 Population","text":"concept “population” subtle important. population set candidates data. dataset. population larger — potentially much larger — set individuals want make inferences. parameters models refer population, dataset.Consider simple example. Define \\(\\mu\\) average number years lived candidates governor Election Day. Can calculate \\(\\mu\\) data? ! many candidates governor still alive, included data even though part “population” want study. \\(\\mu\\) can calculated. can estimated.Another problem like estimate effect winning lifespan present day. data excludes recent candidates (since still alive), predictions mirror future well may hope.Even though original question “gubernatorial candidates” general, specifically refer United States, assume data US governors representative enough population interested (global politicians) exercise useful. believe , stop right now. major part Wisdom deciding questions can’t answer data just don’t .truth : social sciences, never perfect relationship data question trying answer. Data gubernatorial candidates past analog gubernatorial candidates today. data candidates countries. Yet, data relevant. Right? certainly better nothing.Generally speaking, using -perfect data better using data .course, always true. wanted predict lifespans gubernatorial candidates U.S., data lifespans presidential candidates France… better making predictions . data won’t help, don’t use data.","code":""},{"path":"five-parameters.html","id":"justice-7","chapter":"10 Five Parameters","heading":"10.2 Justice","text":"\nFIGURE 10.2: Justice\ninspecting data deciding “close enough” questions useful, move Justice.Justice emphasizes key concepts:Population Table, structure includes row every unit population. generally break rows Population Table three categories: data units want (actual data set), data units actually (Preceptor Table), data units care (rest population, included data Preceptor Table).data representative population?meaning columns consistent, .e., can assume validity? make assumption data generating mechanism.inspect representativeness validity Population Table. Representativeness focuses rows table, validity focuses columns.","code":""},{"path":"five-parameters.html","id":"representativeness-1","chapter":"10 Five Parameters","heading":"10.2.1 Representativeness","text":"Recall representativeness involves rows; specifically, rows dataset match rows desired population.want know data representative population, primary question ask : everyone desired population equal chance sampled? simple example, think back urn example Chapter 6. able determine , barring issue sampling mechanism (shovel), every bead equal chance sampled. case ?looking Barfort, Klemmensen, Larsen (2020), source dataset, see :“collect data… candidates running gubernatorial election 1945 2012. limit attention two candidates received highest number votes.”data , , highly representative gubernatorial candidates, includes every candidate 1945 2012. However, one large caveat: two candidates votes included dataset. unfortunate, ideally look gubernatorial candidates (regardless votes). Regardless, still deem dataset somewhat representative larger population.","code":""},{"path":"five-parameters.html","id":"validity-1","chapter":"10 Five Parameters","heading":"10.2.2 Validity","text":"Validity, hand, involves columns. put simply, column lifespan Preceptor Table equate column lifespan dataset. , look source data: Barfort, Klemmensen, Larsen (2020).collection birth death dates winning candidates well documented. birth death dates losing candidates, however, easily gathered. fact, Barfort, Klemmensen, Larsen (2020) perform independent research information:“losing candidates, use information gathered several online sources, including Wikipedia, Political Graveyard…, Find Grave… Campaigns.”nearly reliable data collection candidates won election. , complication:“cases, able identify year birth death, exact date event. candidates, impute date July 1 given year.”candidates, , estimate longevity inaccurate. also hope birth death dates listed unreliable internet sources accurate. possible , especially older candidates.mission exploration ensure validity much possible — , equate columns equated themseleves. case, fix issues data collection, accept estimates may slightly skewed.","code":""},{"path":"five-parameters.html","id":"functional-form-1","chapter":"10 Five Parameters","heading":"10.2.3 Functional form","text":"","code":""},{"path":"five-parameters.html","id":"courage-7","chapter":"10 Five Parameters","heading":"10.3 Courage","text":"\nFIGURE 10.3: Courage\n","code":""},{"path":"five-parameters.html","id":"election_age","chapter":"10 Five Parameters","heading":"10.3.1 election_age","text":"begin, let’s model candidate lifespan election function candidate lifespan prior election. data:math fairly simple:\\[ lived\\_after_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i \\]\\(\\epsilon_i \\sim N(0, \\sigma^2)\\).\n- \\(lived\\_after_i\\) number years lived election candidate \\(\\).\n- \\(election\\_age_i\\) number years lived election candidate \\(\\).\n- \\(\\epsilon_i\\) “error term,” difference actual years-lived candidate \\(\\) modeled years-lived. \\(\\epsilon_i\\) normally distributed mean 0 standard deviation \\(\\sigma\\).key distinction :Variables, always subscripted \\(\\), whose values (potentially) vary across individuals.Variables, always subscripted \\(\\), whose values (potentially) vary across individuals.Parameters, never subscripted \\(\\), whose values constant across individuals.Parameters, never subscripted \\(\\), whose values constant across individuals.use \\(lived\\_after_i\\) formula instead \\(y_i\\)? often remind variable’s actual substance, better. another common convention: always use \\(y_i\\) symbol dependent variable. unusual describe model :\\[ y_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i\\]mean thing.Either way, \\(\\beta_0\\) “intercept” regression, average value population \\(lived\\_after\\), among \\(election\\_age = 0\\).\\(\\beta_1\\) “coefficient” \\(election\\_age\\). comparing two individuals, first \\(election\\_age\\) one year older second, expect first \\(lived\\_after\\) value \\(\\beta_1\\) different second. words, expect older fewer years remaining, \\(\\beta_1\\) negative. , value population data drawn.three unknown parameters — \\(\\beta_0\\), \\(\\beta_1\\) \\(\\sigma\\) — just models used Chapter 8. get five parameter case, useful review earlier material.may recall middle school algebra equation line \\(y = m x + b\\). two parameters: \\(m\\) \\(b\\). intercept \\(b\\) value \\(y\\) \\(x = 0\\). slope coefficient \\(m\\) \\(x\\) increase \\(y\\) every one unit increase \\(x\\). defining regression line, use slightly different notation fundamental relationship .can implement model stan_glm().discussed Chapter 8, common term model like “regression.” “regressed” lived_after, dependent variable, election_age, () independent variable.parameter values:almost always case, \\(\\sigma\\) nuisance parameter, value interested . stan_glm() refers “Auxiliary” parameter.posterior distributions \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (coefficient \\(election\\_age_i\\)), hand, important. looking posteriors , let’s examine fitted values:Consider someone 40 years old Election Day. score data points candidates around age. area highlighted red box plot. can see, two died soon election. lived 50 years election. Variation fills world. However, fitted line tells us , average, expect candidate age live 37 years election.model, continuous independent (“predictor”) variable, infinite number fitted values, one possible value election_age.can create formula fitted values placing median values posterior distribution parameters mathematical formula:\\[lived\\_after_i =  72.7 - 0.9 election\\_age_i\\]Consider intercept. Since independent variable \\(election\\_age_i\\), intercept \\(lived\\_after_i\\) value \\(election\\_age_i\\) zero. , interpret intercept average lifespan gubernatorial candidate election, candidate alive zero years prior election., course, substantively nonsense. one runs office year born. next model, explore ways making intercept interpretable. meantime, math math.Consider coefficient \\(election\\_age_i\\), \\(\\beta_1\\). median posterior, -1, represents slope model. every unit increase independent variable, dependent variable change amount. every additional year candidate alive election, lifespan election -1 years lower, average. given number years candidate lived election want estimate long live , multiply years alive prior beta -0.9, combine intercept.descriptive model, causal model. Remember motto Chapter 4: causation without manipulation. way, person \\(\\), change years alive Election Day. day election, X years old. way change . , two () potential outcomes. Without one potential outcome, can causal effect.Given , important monitor language. believe changes election_age “cause” changes lived_after. obvious. words phrases — like “associated ” “change ” — close causal. (guilty using just paragraphs ago!) wary use. Always think terms comparisons using predictive model. can’t change election_age 40 50 individual candidate. can compare two candidates (two groups candidates), one election_age equal 40 election_age equal 50. model correct, candidates , average, differ lived_after \\(\\beta_1\\) times difference 40 50.Let’s look posterior \\(\\beta_1\\), coefficient \\(election\\_age_i\\):","code":"\nch10 %>% \n  ggplot(aes(x = election_age, y = lived_after)) +\n    geom_point() +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Younger candidates live longer\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Years on Election Day\",\n         y = \"Years Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic()\nfit_1 <- stan_glm(data = ch10,\n                      formula = lived_after ~ election_age,\n                      refresh = 0,\n                      seed = 9)\nprint(fit_1, detail = FALSE)##              Median MAD_SD\n## (Intercept)  72.7    2.1  \n## election_age -0.9    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 11.1    0.2\nch10 %>% \n  ggplot(aes(x = election_age, y = lived_after)) +\n    geom_point() +\n    geom_line(aes(y = fitted(fit_1)), color = \"blue\") +\n  \n    # Add red box to highlight the slice of data we are discussing below using\n    # geom_rect().\n  \n    geom_rect(aes(xmin = 38.5 , xmax = 41.5, ymin = 0, ymax = 60),\n               fill = \"transparent\", color = \"red\", size = .5) +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Blue line shows fitted values\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Years at Time of Election\",\n         y = \"Years Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic()\nfit_1 %>% \n  as_tibble() %>% \n  ggplot(aes(election_age)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of the Coefficient of `election_age`\",\n         y = \"Probability\",\n         x = \"Coefficient of `election_age`\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"five-parameters.html","id":"sex","chapter":"10 Five Parameters","heading":"10.3.2 sex","text":"Let’s now regress lived_after sex see candidates’ post-election lifespans differ sex.Note workflow. Try one model. Interpret . Try another model. another. one “true” model. infinite number possible models. Good data science involves intelligent tour space.regression, use -1 formula make output straightforward, intercept interpret. math model saw Chapter 8:\\[ lived\\_after_i = \\beta_1 female_i + \\beta_2 male_i + \\epsilon_i\\]\n\\[female_i, male_i \\\\{0,1\\}\\] \n\\[female_i +  male_i = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]meaning \\(lived\\_after_i\\) \\(\\epsilon_i\\) first model. Indeed, throughout exercises. \\(female_i\\) \\(male_i\\) 0/1 variables, just like last chapter. variables whose values vary across individuals.important parameters \\(\\beta_1\\) \\(\\beta_2\\). average years-lived post-election , respectively, women men. , “average” data . easy calculate! estimation required. \\(\\beta_1\\) \\(\\beta_2\\) averages entire “population,” however chosen define term. averages can calculated directly. can estimated, creating posterior probability distribution.Looking back regression model just created, see intercept. Instead \\(\\beta_0\\) value, \\(\\beta_1\\) \\(\\beta_2\\) female male. makes things easier interpret. Without add subtract anything intercept, regression tells us , average, women expected live 16 years running governor, men expected live 28 years.strange result. men live almost twice long women election? One explanation might women don’t run governor later life, therefore expected live long.Now interpreted model using -1 formula estimate \\(\\beta_1\\) \\(\\beta_2\\), let’s take away -1 regress lived_after intercept sex see equation changes.longer value female. However intercept. regression mathematical formula :\\[ lived\\_after_i = \\beta_0  + \\beta_1 male_i + \\epsilon_i\\]\\(\\beta_0\\) intercept, around 16 years. result similar female value . type model, intercept represents variable represented model. \\(\\beta_1\\) affects outcome candidate male. (candidate female, \\(male_i = 0\\). Therefore, intercept value represents male, .e., females.)candidate male, add coefficient male intercept value, gives us average lifespan male gubernatorial candidate election. can see adding \\(\\beta_0\\) \\(\\beta_1\\), value got males previous model.careful notation! \\(\\beta_1\\) -intercept model different \\(\\beta_1\\) model intercept! Notation varies. must pay attention time make model.posterior distribution \\(\\beta_0 + \\beta_1\\) can constructed via simple addition.actually “simple?” really! Manipulating parameters directly bothersome. Dealing variables named (Intercept) error-prone. Nothing approach scales realistic cases involving numerous predictor variables. Using posterior_epred() associated functions much easier.interpretation parameter seen . true average, across entire population, number years male candidates live election. can never know true average . , seems likely true average somewhere 27.5 29.5 years.","code":"\nfit_2 <- stan_glm(data = ch10,\n                      formula = lived_after ~ sex - 1,\n                      refresh = 0,\n                      seed = 16)\nprint(fit_2, detail = FALSE)##           Median MAD_SD\n## sexFemale 16.0    2.9  \n## sexMale   28.5    0.4  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 13.3    0.3\nfit_2a <- stan_glm(data = ch10,\n                       formula = lived_after ~ sex,\n                       refresh = 0,\n                       seed = 76)\nprint(fit_2a, detail = FALSE)##             Median MAD_SD\n## (Intercept) 16.1    2.9  \n## sexMale     12.3    2.9  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 13.3    0.3\nfit_2a %>% \n  as_tibble() %>% \n  mutate(male_intercept = `(Intercept)` + sexMale) %>% \n  ggplot(aes(male_intercept)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Male Candidate Years Left\",\n         y = \"Probability\",\n         x = \"Years To Live After the Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nposterior_epred(fit_2a,\n                tibble(sex = \"Male\")) %>% \n  as_tibble() %>% \n  ggplot(aes(`1`)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Male Candidate Years Left\",\n         y = \"Probability\",\n         x = \"Years To Live After the Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"five-parameters.html","id":"election_age-and-sex","chapter":"10 Five Parameters","heading":"10.3.3 election_age and sex","text":"model, outcome variable continues lived_after, now two different explanatory variables: election_age sex. Note sex categorical explanatory variable election_age continuous explanatory variable. type model — parallel slopes — saw Chapter 9.Math:\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\epsilon_i \\]wait! variable name sex, male. male come ?answer male indicator variable, meaning 0/1 variable. male takes value one candidate “Male” zero otherwise. \\(male_i\\) variable used previous two examples.outcome variable \\(lived\\_after_i\\), number years person alive election. \\(male_i\\) one explanatory variables. predicting number years male candidate lives election, value 1. making prediction female candidates, value 0. \\(c\\_election\\_age_i\\) explanatory variable. number years candidate lived election, scaled subtracting average number years lived candidates.outcome variable \\(lived\\_after_i\\), number years person alive election. \\(male_i\\) one explanatory variables. predicting number years male candidate lives election, value 1. making prediction female candidates, value 0. \\(c\\_election\\_age_i\\) explanatory variable. number years candidate lived election, scaled subtracting average number years lived candidates.\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates (.e. male female). \\(\\beta_0\\) also intercept equation. words, \\(\\beta_0\\) expected value \\(lived\\_after_i\\), \\(male_i = 0\\) \\(c\\_election\\_age_i = 0\\).\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates (.e. male female). \\(\\beta_0\\) also intercept equation. words, \\(\\beta_0\\) expected value \\(lived\\_after_i\\), \\(male_i = 0\\) \\(c\\_election\\_age_i = 0\\).\\(\\beta_1\\) almost meaningless . time meaning value connected intercept (.e. \\(\\beta_0 + \\beta_1\\)). two added together, get average number years lived election men, day election, alive average number years candidates.\\(\\beta_1\\) almost meaningless . time meaning value connected intercept (.e. \\(\\beta_0 + \\beta_1\\)). two added together, get average number years lived election men, day election, alive average number years candidates.\\(\\beta_2\\) , entire population, average difference \\(lived\\_after_i\\) two individuals, one \\(c\\_election\\_age_i\\) value 1 greater .\\(\\beta_2\\) , entire population, average difference \\(lived\\_after_i\\) two individuals, one \\(c\\_election\\_age_i\\) value 1 greater .Let’s translate model code.Looking results, can see intercept value around 66. average female candidate, alive average number years candidates, live another 66 years election.Note sexMale around 6. coefficient, \\(\\beta_1\\). need connect value intercept value get something meaningful. Using formula \\(\\beta_0 + \\beta_1\\), find number years average male candidate — , day election, average age candidates — live around 72 years.Now take look coefficient \\(c\\_election\\_age_i\\), \\(\\beta_2\\). median posterior, -0.8, represents slope model. comparing two candates differ one year election_age, expect differ -0.8 years lived_after. makes sense value negative. years candidate lived, fewer years candidate left live. , every extra year candidate alive election, lifespan election 0.8 years lower, average.Let’s now look posteriors.graph displays posterior probability distributions \\(\\beta_0\\) \\(\\beta_0 + \\beta_1\\). two distributions fairly different; distribution females spread males. greater number men dataset, far fewer women. data , precise can .Let’s now take look posterior distribution \\(\\beta_2\\), coefficient \\(c\\_election\\_age_i\\).","code":"\nfit_3 <- stan_glm(data = ch10,\n                      formula = lived_after ~ sex + election_age,\n                      refresh = 0,\n                      seed = 12)\nprint(fit_3, detail = FALSE)##              Median MAD_SD\n## (Intercept)  66.0    3.2  \n## sexMale       6.1    2.4  \n## election_age -0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 11.1    0.2\nfit_3 %>% \n  as_tibble() %>% \n  mutate(male_years = `(Intercept)` + sexMale) %>% \n  rename(female_years = `(Intercept)`) %>% \n  select(female_years, male_years) %>% \n  pivot_longer(cols = female_years:male_years, \n               names_to = \"parameters\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n     labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Men live longer\",\n         x = \"Average Years Lived Post Election\",\n         y = \"Probability\",\n         fill = \"Parameters\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nfit_3 %>% \n  as_tibble() %>% \n  ggplot(aes(election_age)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of the Coefficient of `election_age`\",\n         y = \"Probability\",\n         x = \"Coefficient of `election_age`\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"},{"path":"five-parameters.html","id":"election_age-sex-and-election_agesex","chapter":"10 Five Parameters","heading":"10.3.4 election_age, sex and election_age*sex","text":"Let’s create another model. time, however, numeric outcome variable lived_after function two explanatory variables used , election_age sex, interaction. look interactions, need 5 parameters, needed wait chapter introduce concept.Math:\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\\\ \\beta_3 male_i *  c\\_election\\_age_i + \\epsilon_i \\]outcome variable still \\(lived\\_after_i\\). want know many years candidate live election. explanatory variables . \\(male_i\\) one male candidates zero female candidates. \\(c\\_election\\_age_i\\) number years candidate lived election, relative average value candidates. model, third predictor variable: interaction \\(male_i\\) \\(c\\_election\\_age_i\\).outcome variable still \\(lived\\_after_i\\). want know many years candidate live election. explanatory variables . \\(male_i\\) one male candidates zero female candidates. \\(c\\_election\\_age_i\\) number years candidate lived election, relative average value candidates. model, third predictor variable: interaction \\(male_i\\) \\(c\\_election\\_age_i\\).\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates. sense, meaning previous model, without interaction term. , always remember meaning parameter conditional model embedded. Even parameter called \\(\\beta_0\\) two different regressions necessitate means thing regressions. Parameter names arbitrary, least simply matter convention.\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates. sense, meaning previous model, without interaction term. , always remember meaning parameter conditional model embedded. Even parameter called \\(\\beta_0\\) two different regressions necessitate means thing regressions. Parameter names arbitrary, least simply matter convention.\\(\\beta_1\\) simple interpretation stand-alone parameter. measure different women men. However, \\(\\beta_0 + \\beta_1\\) straightforward meaning exactly analogous meaning \\(\\beta_0\\). sum average number years lived election men, day election, alive average number years candidates.\\(\\beta_1\\) simple interpretation stand-alone parameter. measure different women men. However, \\(\\beta_0 + \\beta_1\\) straightforward meaning exactly analogous meaning \\(\\beta_0\\). sum average number years lived election men, day election, alive average number years candidates.\\(\\beta_2\\) coefficient \\(c\\_election\\_age_i\\). just slope women. average difference \\(lived\\_after_i\\) two women, one \\(c\\_election\\_age_i\\) value 1 greater . last example, \\(\\beta_2\\) slope whole population. Now different slopes different genders.\\(\\beta_2\\) coefficient \\(c\\_election\\_age_i\\). just slope women. average difference \\(lived\\_after_i\\) two women, one \\(c\\_election\\_age_i\\) value 1 greater . last example, \\(\\beta_2\\) slope whole population. Now different slopes different genders.\\(\\beta_3\\) alone difficult interpret. However, added \\(\\beta_2\\), result slope men.\\(\\beta_3\\) alone difficult interpret. However, added \\(\\beta_2\\), result slope men.Code:intercept increased. \\(\\beta_0\\) around 20. intercept females. still means average number years lived election women 20 . sexMale coefficient, \\(\\beta_1\\), refers value must added intercept order get average males. calculated, result 72. Keep mind, however, values apply \\(c\\_election\\_age_i = 0\\), , , candidate \\(\\) around 52 years old.coefficient \\(c\\_election\\_age_i\\), \\(\\beta_2\\), -0.1. mean? slope females. , comparing two female candidates differ one year age, expect older candidate live 0.1 years less. Now direct attention coefficient sexMale:election_age, \\(\\beta_3\\), -0.8. value must added coefficient \\(c\\_election\\_age_i\\) (recall \\(\\beta_2 + \\beta_3\\)) order find slope males. two added together, value, slope, -0.9. comparing two male candidates differ age one year, expect older candidate live 0.9 years less.Key point: interpretation intercepts apply candidates \\(c\\_election\\_age_i = 0\\). Candidates 52 years-old different expected number years live. interpretation slope applies everyone. words, relationship \\(lived\\_after_i\\) \\(c\\_election\\_age_i\\) , regardless gender old .posterior:, recommend working directly parameters. analysis much easier posterior_epred(), see Temperance Section.Male candidates live longer average female candidates. Note, also, average years live election females 20 model. previous model, 66 years. difference? interpretation “average” different! previous model, average women. model, average 52 years-old women. different things, hardly surprised different posteriors.Slope posteriors:posterior distribution shows average slope values men women. can see men steeper slope, slope women practically 0! trying forecast number years women live election, may ignore number years already lived. definitely true men. difference?","code":"\nfit_4 <- stan_glm(data = ch10,\n                      formula = lived_after ~ sex*election_age,\n                      refresh = 0,\n                      seed = 13)\nprint(fit_4, detail = FALSE)##                      Median MAD_SD\n## (Intercept)          20.5   20.4  \n## sexMale              51.9   20.5  \n## election_age         -0.1    0.3  \n## sexMale:election_age -0.8    0.3  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 11.1    0.2\nfit_4 %>% \n  as_tibble() %>% \n  mutate(male_years = `(Intercept)` + sexMale) %>% \n  rename(female_years = `(Intercept)`) %>% \n  select(female_years, male_years) %>% \n  pivot_longer(cols = female_years:male_years, \n               names_to = \"parameters\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Years Lived After the Election\",\n         subtitle = \"Men live longer\",\n         x = \"Average Years Lived Post Election\",\n         y = \"Probability\", \n         fill = \"Parameters\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nfit_4 %>% \n  as_tibble() %>% \n  mutate(slope_men = election_age + `sexMale:election_age`) %>% \n  rename(slope_women = election_age) %>% \n  select(slope_women, slope_men) %>% \n  pivot_longer(cols = slope_women:slope_men, \n               names_to = \"parameters\",\n               values_to = \"slope\") %>% \n  ggplot(aes(slope, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Slope of Years-Lived on Years-to-Live\",\n         subtitle = \"Men have a steeper slope\",\n         x = \"Slope\",\n         y = \"Probability\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic() "},{"path":"five-parameters.html","id":"interaction-model","chapter":"10 Five Parameters","heading":"10.3.5 Interaction model","text":"Recall parallel slopes model created Chapter 9. Another visualization can create, one also uses slopes intercepts model, interaction model. model, slopes two groups different, creating non-parallel visualization.process creating interaction model similar creating parallel slopes model. Let us begin way — tidying data inspecting .tidying data, extract values assign sensible names later use. Note identical process Chapter 9, addition fourth term (interaction term):Now extracted values, create intercept slope values two different groups, females males. Recall following details finding slopes intercepts interaction model:intercept intercept females. represents average number years lived election females.sexMale coefficient refers value must added intercept order get average years lived post-election males.coefficient \\(c\\_election\\_age_i\\) slope females.coefficient sexMale:election_age value must added coefficient \\(c\\_election\\_age_i\\) order find slope males.creating objects different intercepts slopes, now create interaction model using geom_abline() male female line.final interaction model! interesting takeaways. First, may note far fewer data points female candidates — concern previously mentioned. makes sense, , slope less dramatic compared male candidates. also see female candidates run older, compared male candidates. might explain intercept years lived post-election lower female candidates.male line seems sensible, might expect far datapoints. male candidates, see clear (logical) pattern: older candidates time election, less years post-election live. makes sense, limited human lifespan.","code":"\n# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy <- fit_4 %>% \n  tidy() %>% \n  select(term, estimate)\n\ntidy## # A tibble: 4 x 2\n##   term                 estimate\n##   <chr>                   <dbl>\n## 1 (Intercept)           20.5   \n## 2 sexMale               51.9   \n## 3 election_age          -0.0766\n## 4 sexMale:election_age  -0.774\n# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept <- tidy$estimate[1]\nsex_male <- tidy$estimate[2]\nelection_age <- tidy$estimate[3]\ninteraction_term <- tidy$estimate[4]\n# Recall that the intercept and the estimate for election_age act as the\n# estimates for female candidates only. Accordingly, we have assigned those\n# values (from the previous code chunk) to more sensible names: female_intercept\n# and female_slope.\n\nfemale_intercept <- intercept\nfemale_slope <- election_age\n\n# To find the male intercept, we must add the intercept for the estimate for\n# sex_male. To find the male slope, we must add election_age to our\n# interaction term estimate.\n\nmale_intercept <- intercept + sex_male\nmale_slope <- election_age + interaction_term\n# From the ch10 data, create a ggplot object with election_age as the x-axis\n# and lived_after as the y-axis. We will use color = sex.\n\nggplot(ch10, aes(x = election_age, y = lived_after, color = sex)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the female intercept and slope. Set the\n  # intercept qual to our previously created female_intercept, while setting\n  # slope equal to our previously created female_slope. The color call is for\n  # coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = female_intercept,\n              slope = female_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the male values. Set the intercept equal to\n  # our previously created male_intercept, while setting slope equal to our\n  # previously created male_slope. The color call is for teal, to match the\n  # colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = male_intercept,\n              slope = male_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Interaction Model\",\n       subtitle = \"Comparing post election lifespan across sex\",\n       x = \"Average Age at Time of Election\", \n       y = \"Years Lived Post-Election\", \n       color = \"Sex\") +\n  theme_classic()"},{"path":"five-parameters.html","id":"temperance-7","chapter":"10 Five Parameters","heading":"10.4 Temperance","text":"\nFIGURE 10.4: Temperance\nRecall questions began chapter:long two political candidates — one male one female, 10 years older average candidate — live election? different lifespans ?questions , purposely, less precise ones tackled Chapters 7 8, written conversational style. normal people talk.However, data scientists, job bring precision questions. two commonsense interpretations. First, curious expected values questions. averaged data thousand candidates like , answer ? Second, curious two specific individuals. long live? Averages involve questions parameters. fates individuals require predictions. general claims, violated often firm rules. Yet, highlight key point: expected values less variable individual predictions.calculate expected values, use posterior_epred(). forecast individuals, use posterior_predict().","code":""},{"path":"five-parameters.html","id":"expected-values","chapter":"10 Five Parameters","heading":"10.4.1 Expected values","text":"Consider “average” interpretation first. answer begins posterior distributions parameters fit_4.Looking posterior probability distributions , can see male candidates expected live longer. much longer? previous chapters, can manipulate distributions , less, way manipulate simple numbers. want know difference two posterior distributions, can simply subtract.average value difference years--live probably positive, likely value around 45 years. still 1% chance true value less zero, .e., expect female candidates live longer.Instead using posterior_epred(), answered questions using posterior probability distributions parameters model, along simple math. Don’t ! First, much likely make mistake. Second, approach generalize well complex models scores parameters interactions.","code":"\nnewobs = tibble(sex = c(\"Male\", \"Female\"), \n                 election_age = 10)\n\npe <- posterior_epred(object = fit_4, \n                      newdata = newobs) %>% \n  as_tibble() %>% \n  rename(\"Male\" = `1`,\n         \"Female\" = `2`)\n\n\npe %>% \n pivot_longer(cols = Male:Female, \n               names_to = \"Gender\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = Gender)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Years Lived Post-Election\",\n         subtitle = \"Male candidates live longer\",\n         x = \"Years\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = \n                         scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\npe <- posterior_epred(object = fit_4, \n                      newdata = newobs) %>% \n  as_tibble() %>% \n  mutate(diff = `1` - `2`)\n\n\npe %>% \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Additional Male Years Lived\",\n         subtitle = \"Male candidates live about 4 years longer\",\n         x = \"Expected Additional Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"five-parameters.html","id":"individual-predictions","chapter":"10 Five Parameters","heading":"10.4.2 Individual predictions","text":", instead, interpret question asking prediction small number individuals, need use posterior_predict().Use posterior_predict() create draws posterior probability distribution prediction cases. posterior_predict() takes two arguments: model simulations run, tibble indicating covariate values individual(s) want predict. case, using fit_4 model tibble one just created . words, inputs posterior_predict() posterior_epred() identical.resulting tibble 2 columns, first male candidate second female candidate. columns draws posterior predictive distributions. cases, forecasts depend values covariates. , provide different forecast candidates younger older.need weird mutate_all(.numeric) incantation? reason posterior_epred() returns simple matrix, easy transform tibble. posterior_predict(), hand, returns special sort matrix much harder work . , need little hackery make next steps easier.Let’s look posterior predictive distribution candidate.big overlap predictions individuals , time, much less overlap averages. Random stuff happens individual time. Random stuff cancels take average many individuals. Consider difference posterior predictive distributions two individuals.words, predict male candidate live longer female candidate. much? Well, number unknown parameter. looking posterior , best estimate 44.5 years. However, quite possible , given male/female candidates, female live longer.fact, 4 10 chance female candidate lives longer.Note different move question averages question individuals. cases, likely value . , average behavior expected value given individual. uncertainty much greater individual prediction. chance true average male candidates less female candidates low. Yet, individual pair candidates, even slightly surprising female candidate outlive male candidate. Individuals vary. Averages never tell whole story.","code":"\npp <- posterior_predict(object = fit_4, \n                        newdata = newobs) %>%\n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  rename(\"Male\" = `1`,\n         \"Female\" = `2`)\n\npp  ## # A tibble: 4,000 x 2\n##     Male  Female\n##    <dbl>   <dbl>\n##  1  60.5  -2.92 \n##  2  45.1   4.04 \n##  3  60.8  12.3  \n##  4  75.6   0.535\n##  5  83.5   5.88 \n##  6  54.2 -19.7  \n##  7  64.5 -21.5  \n##  8  55.8 -10.2  \n##  9  87.2  -5.71 \n## 10  81.0   6.50 \n## # … with 3,990 more rows\npp %>% \n  pivot_longer(cols = Male:Female, \n               names_to = \"Gender\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = Gender)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for a Candidate's Years Lived Post-Election\",\n         subtitle = \"Individual lifespans have a great deal of variation\",\n         x = \"Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\npp %>% \n  mutate(diff = Male - Female) %>% \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for a Male Candidate's Extra Years Lived\", \n         subtitle = \"Any random male candidate may die before a random female candidate\",\n         x = \"Years\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\npp %>% \n  mutate(diff = Male - Female) %>% \n  summarize(f_live_longer = sum(diff < 0),\n            total = n(),\n            f_live_longer / total)## # A tibble: 1 x 3\n##   f_live_longer total `f_live_longer/total`\n##           <int> <int>                 <dbl>\n## 1           113  4000                0.0282"},{"path":"five-parameters.html","id":"expectation-versus-individual-variation","chapter":"10 Five Parameters","heading":"10.4.3 Expectation versus individual variation","text":"Let’s compare results posterior_epred() posterior_predict() scenario directly. code shown , think useful look everything together.Expected values vary much less predictions. chart makes easy see. somewhat sure true underlying average numbers years male candidates live post-election female candidates. , two individual candidates, good chance female candidate live longer. can ignore \\(\\epsilon\\) predicting outcome individuals. estimating expected values long-run averages, \\(\\epsilon\\)’s cancel .","code":"\nnewobs <- tibble(sex = c(\"Male\", \"Female\"),\n                  election_age = 10)\n\npe <- posterior_epred(fit_4, \n                      newdata = newobs) %>%\n  as_tibble() %>% \n  mutate(diff = `1` - `2`)\n\npp <- posterior_predict(fit_4, \n                        newdata = newobs) %>% \n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  mutate(diff = `1` - `2`)\n\ntibble(Expectation = pe$diff,\n       Prediction = pp$diff) %>% \n  pivot_longer(cols = Expectation:Prediction, \n               names_to = \"Type\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = Type)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected and Individual Male Advantage\",\n         subtitle = \"Expected male advantage is much more precisely estimated\",\n         x = \"Additional Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()"},{"path":"five-parameters.html","id":"testing-1","chapter":"10 Five Parameters","heading":"10.4.4 Testing","text":"normal statistics book, already discussed concept “testing” extensively. terminology varies field. “Tests,” “testing,” “hypothesis tests,” “tests significance,” “null hypothesis significance testing” refer concept. refer collection approaches NHST, common abbreviation derived initials last phrase. Wikipedia provides overview.view: Amateurs test. Professionals summarize.Consider question expected difference lifespans South Dakota Washington candidates. difference “significant?” Can “reject null hypothesis?” conventional answer Yes. Anytime zero outside 95% confidence interval, can declare result significant. ? help us? already full posterior probability distribution. knowledge. Yes/question throws away much information (almost) ever useful. reason test can summarize providing full posterior probability distribution.arguments apply case “insignificant” results, \\(p > 0.5\\), can’t “reject” null hypothesis. Instead expected values, consider case two candidates, one South Dakota one Washington. difference predicted lifespans “significant?” cares!? full posterior probability distribution prediction — also known posterior predictive distribution — graphed . 95% confidence interval includes zero. mean throw away? ! nonsense. Yes, 20% chance South Dakota candidate lives longer, can hardly surprised happens. still think much likely Washington candidate lives longer. Indeed, consider 4--1 odds fair. fact difference “significant” relevance use posterior make decisions.reasoning applies every parameter estimate, every prediction make. Never test — unless boss demands test. Use judgment, make models, summarize knowledge world, use summary make decisions.","code":""},{"path":"five-parameters.html","id":"summary-10","chapter":"10 Five Parameters","heading":"10.5 Summary","text":"major part Wisdom deciding questions can’t answer data don’t .Avoid answering questions working parameters directly. Use posterior_epred() instead.Good data science involves intelligent tour space possible models.Always think terms comparisons using predictive model.Spend less time thinking parameters mean time using posterior_epred() posterior_predict() examine implications models.","code":""},{"path":"n-parameters.html","id":"n-parameters","chapter":"11 N Parameters","heading":"11 N Parameters","text":"chapter still DRAFT.created models one parameter Chapter 6, two parameters Chapter 7, three parameters Chapter 8, four parameters Chapter 9 five parameters Chapter 10, now ready make jump \\(N\\) parameters.chapter, consider models many parameters complexities arise therefrom. models grow complexity, need pay extra attention basic considerations like validity, population, representativeness. easy jump right start interpreting! harder, necessary, ensure models really answering questions.Imagine running Governor Texas want better job getting voters vote. can encourage voters go polls election day?","code":""},{"path":"n-parameters.html","id":"wisdom-9","chapter":"11 N Parameters","heading":"11.1 Wisdom","text":"\nFIGURE 11.1: Wisdom\nresearch ways increase voting, come across large-scale experiment showing effect sending voting reminder “shames” citizens vote. considering sending “shaming” voting reminder .looking shaming dataset primer.data package. dataset “Social Pressure Voter Turnout: Evidence Large-Scale Field Experiment” Gerber, Green, Larimer (2008). Check paper . can, , familiarize data typing ?shaming.Recall initial question: can encourage voters go polls election day? now need translate precise question, one can answer data.question:causal effect, likelihood voting, different postcards voters different levels political engagement?","code":""},{"path":"n-parameters.html","id":"ideal-preceptor-table","chapter":"11 N Parameters","heading":"11.1.1 Ideal Preceptor Table","text":"Recall ideal Preceptor Table. rows columns data need , , calculation number interest trivial? want know average height adult India, ideal Preceptor Table include row adult India column height.One key aspect ideal Preceptor Table whether need one potential outcome order calculate estimand. Mainly: need causal model, one estimates attitude treatment control? Preceptor Table require two columns outcome. case, trying see causal effect mailed voting reminders voting.modeling (just) prediction (also) modeling causation? Predictive models care nothing causation. Causal models often also concerned prediction, means measuring quality model. , looking causation., ideal table look like? Assuming running governor United States, ideally data every citizen voting age. means approximately 200 million rows.missing data ideal Preceptor Table, also know outcomes treatment (receiving reminder) control (receiving reminder). sample row table:ideal table, rows American citizens voting age. good start! However, may want even information ideal Preceptor Table. Perhaps column sex informative. column age? Political affiliation? perfect world, know pieces information. perfect world, measure exact causal effect voting reminders different subsets US population.may also want narrow ideal Preceptor Table. running governor Florida, may want study citizens Florida. running Democrat, may want study citizens registered Democrats.However, main point exercise see want know compared actually know.","code":""},{"path":"n-parameters.html","id":"eda-of-shaming","chapter":"11 N Parameters","heading":"11.1.2 EDA of shaming","text":"loading packages need, let’s perform EDA, starting running glimpse() shaming tibble primer.data package.glimpse() gives us look raw data contained within shaming data set. top output, can see number rows columns, observations variables respectively. see 344,084 observations, row corresponding unique respondent. summary provides idea variables working .Variables particular interest us sex, hh_size, primary_06. variable hh_size tells us size respondent’s household, sex tells us sex respondent, primary_06 tells us whether respondent voted 2006 Primary election.\nthings note exploring data set. may – may – noticed response general_04 variable “Yes.” published article, authors note “registered voters voted November 2004 selected sample” (Gerber, Green, Larimer, 2008). , authors found history sent mailings. Thus, non-registered voters excluded data.also important identify dependent variable meaning. shaming experiment, dependent variable primary_06, variable coded either 0 1 whether respondent voted 2006 primary election. dependent variable authors trying measure effect treatments voting behavior 2006 general election.yet discussed important variable : treatment. treatment variable factor variable 5 levels, including control. Since curious sending mailings affects voter turnout, treatment variable tell us impact type mailing can make. Let’s start taking broad look different treatments.Four types treatments used experiment, voters receiving one four types mailing. mailing treatments carried message, “CIVIC DUTY - VOTE!”first treatment, Civic Duty, also read, “Remember rights responsibilities citizen. Remember vote.” message acted baseline treatments, since carried message similar one displayed mailings.second treatment, Hawthorne, households received mailing told voters studied voting behavior examined public records. adds small amount social pressure households receiving mailing.third treatment, Self, mailing includes recent voting record member household, placing word “Voted” next name fact vote 2004 election blank space next name . mailing, households also told, “intend mail updated chart” voting record household members 2006 primary. emphasizing public nature voting records, type mailing exerts social pressure voting Hawthorne treatment.fourth treatment, Neighbors, provides household members’ voting records, well voting records live nearby. mailing also told recipients, “intend mail updated chart” voted 2006 election entire neighborhood.","code":"\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(ggthemes)\nlibrary(ggdist)\nlibrary(gt)\nlibrary(janitor)\nlibrary(broom.mixed)\nlibrary(gtsummary)\nglimpse(shaming)## Rows: 344,084\n## Columns: 15\n## $ cluster       <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n## $ primary_06    <int> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,…\n## $ treatment     <fct> Civic Duty, Civic Duty, Hawthorne, Hawthorne, Hawthorne,…\n## $ sex           <chr> \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\", \"F…\n## $ age           <int> 65, 59, 55, 56, 24, 25, 47, 50, 38, 39, 65, 61, 57, 37, …\n## $ primary_00    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n## $ general_00    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n## $ primary_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n## $ general_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n## $ primary_04    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n## $ general_04    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n## $ hh_size       <int> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1,…\n## $ hh_primary_04 <dbl> 0.095, 0.095, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, …\n## $ hh_general_04 <dbl> 0.86, 0.86, 0.86, 0.86, 0.86, 0.90, 0.90, 0.90, 0.90, 0.…\n## $ neighbors     <int> 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, …\nshaming %>%\n  count(treatment)## # A tibble: 5 x 2\n##   treatment       n\n##   <fct>       <int>\n## 1 Control    191243\n## 2 Civic Duty  38218\n## 3 Hawthorne   38204\n## 4 Self        38218\n## 5 Neighbors   38201"},{"path":"n-parameters.html","id":"population-2","chapter":"11 N Parameters","heading":"11.1.3 Population","text":"One important components Wisdom concept “population.” Recall questions asked earlier:discussed , population set people, voters, data. dataset. set voters like data. rows ideal Preceptor Table. population larger — potentially much larger — set individuals include data data want. Generally, population much larger either data data want.case, viewing data perspective someone running Governor year wants increase voter turnout. want increase turnout now, people voting 2006! also may want increase turnout citizens registered vote, group excluded dataset. reasonable generate conclusions group? likely, . However, limited data work determine far willing generalize groups.judgment call, matter Wisdom, whether may assume data data want (.e., ideal Preceptor Table) drawn population.Even though original question “voters” general, specifically refer specific states might interested, assume data random voters , uh, representative enough population interested . believe , stop right now. major part Wisdom deciding questions can’t answer data just don’t .","code":""},{"path":"n-parameters.html","id":"justice-8","chapter":"11 N Parameters","heading":"11.2 Justice","text":"\nFIGURE 11.2: Justice\nJustice emphasizes key concepts:actual Preceptor Table, structure includes row every unit population. generally break rows actual Preceptor Table three categories: data units want , data units actually , data units care .data representative population?meaning columns consistent, .e., can assume validity?make assumption data generating mechanism.","code":""},{"path":"n-parameters.html","id":"actual-preceptor-table","chapter":"11 N Parameters","heading":"11.2.1 Actual Preceptor Table","text":"Recall actual Preceptor Table, bunch missing data! can use simple arithmetic calculate causal effect voting reminders voting behavior. Instead, required estimate . estimand, variable real world trying measure.estimand value calculated, rather unknown variable want estimate.Let’s build basic visualization actual Preceptor Table scenario:Citizen 1Voted??Citizen 23Did vote??Citizen 40?Voted?Citizen 53?vote?Citizen 80Voted??, two possible outcomes: vote vote. really want know Average Treatment Effect (ATE) treatment, voting reminder. want estimate much voting reminder impacts odds someone voting.Note simplified version actual Preceptor Table. dataset, number columns know subjects: age, sex, past voting history. expanded actual Preceptor Table, columns included.Now, can fill question marks? Fundamental Problem Causal Inference, can never know missing values. can never know missing values, must make assumptions. “Assumption” just means need “model,” models parameters.","code":""},{"path":"n-parameters.html","id":"the-population-table-1","chapter":"11 N Parameters","heading":"11.2.2 The Population Table","text":"Population Table shows data actually desired population. shows rows three sources: ideal Preceptor Table, data, population outside data (rows exist data).ideal Preceptor rows, information covariates sex, year, state. However, rows included data, outcome results. Since scenario pertains upcoming election Texas, state column read Texas year column read 2021.rows data everything: covariates outcomes. covariates Michigan state 2006 year, since pieces information included data. course, still values Treatment minus Control, since observe one subject two conditions.rows population data. subjects fall desired population, data. , rows missing.ideal Preceptor TableFemale2021Texas???ideal Preceptor TableMale2021Texas???.....................DataMale2006MichiganDid vote??DataFemale2006Michigan?Voted?DataFemale2006MichiganVoted??.....................Population??????Population??????Population??????","code":""},{"path":"n-parameters.html","id":"representativenessvalidity","chapter":"11 N Parameters","heading":"11.2.3 Representativeness/validity","text":"good time consider really means accept data representative population. mind, let’s break real, current question:running governor Texas year 2021. year United States, consider sending voting reminder postcard citizens voting age. reminder encourage voting, much?Now, let’s break data shaming dataset:data gathered Michigan prior August 2006 primary election. population experiment 180,002 households state Michigan. data included voted 2004 general election. Therefore, include non-voters. reminders mailed households random., similar groups? Let’s start differences.\n* data 2006. question asking answers 2021. small gap time. lot changes decade half!\n* data excludes non-voters last election. question, seeks increase voting turnout citizens, want non-voters included. , can make claims citizens? Probably .\n* data includes voters Michigan. want make inferences Texas, perhaps United States whole. within reason ?\n* voting reminders data sent mail. practical option 2021, may easier send electronic notice?\n* contents voting reminders 2006 going differ voting reminders sent 2021. Thus, “treatments” fundamentally different.small issues. big, --face issues. say data voters 2006 data voters 2021 exchangeable — meaning represent thing — also consider means time frames. Can data 2006 used predict 2030, 2040, beyond? makes 2021 acceptable option specifically?can helpful cases like make data exchangeable conditional another assumption. instance, trend less voting 2021 compared 2006, may need account order predictions helpful. Alternatively, studying state historically voting participation Michigan, may want make predictions based assumption “people, average, vote State X.”purpose section make us think critically assumptions making whether assumptions can reasonably made. Though continue using dataset remainder chapter, clear must make predictions caution.","code":""},{"path":"n-parameters.html","id":"functional-form-2","chapter":"11 N Parameters","heading":"11.2.4 Functional form","text":"","code":""},{"path":"n-parameters.html","id":"courage-8","chapter":"11 N Parameters","heading":"11.3 Courage","text":"\nFIGURE 11.3: Courage\n","code":""},{"path":"n-parameters.html","id":"set-up","chapter":"11 N Parameters","heading":"11.3.1 Set-up","text":"Now, create object named object_1 includes 3-level factor classifying voters level civic engagement.Convert primary general election variables already 1/0 binary binary format.Convert primary general election variables already 1/0 binary binary format.Create new column named civ_engage sums person’s voting behavior , including, 2006 primary.Create new column named civ_engage sums person’s voting behavior , including, 2006 primary.Create column named voter_class classifies voters 3 bins: “Always Vote” voted least 5 times, “Sometimes Vote” voted 3 4 times, “Rarely Vote” voted 2 fewer times. variable classified factor.Create column named voter_class classifies voters 3 bins: “Always Vote” voted least 5 times, “Sometimes Vote” voted 3 4 times, “Rarely Vote” voted 2 fewer times. variable classified factor.Create column called z_age z-score age.Create column called z_age z-score age.Let’s inspect object:Great! Now, create first model: relationship primary_06, represents whether citizen voted , sex treatment.","code":"\nobject_1 <- shaming %>% \n  \n  # Converting the Y/N columns to binaries with the function we made \n  # note that primary_06 is already binary and also that we don't \n  # need it to predict construct previous voter behavior status variable.\n  \n  mutate(p_00 = (primary_00 == \"Yes\"),\n         p_02 = (primary_02 == \"Yes\"),\n         p_04 = (primary_04 == \"Yes\"),\n         g_00 = (general_00 == \"Yes\"),\n         g_02 = (general_02 == \"Yes\"),\n         g_04 = (general_04 == \"Yes\")) %>% \n  \n  # A sum of the voting action records across the election cycle columns gives\n  # us an idea (though not weighted for when across the elections) of the voters\n  # general level of civic involvement.\n  \n  mutate(civ_engage = p_00 + p_02 + p_04 + \n                      g_00 + g_02 + g_04) %>% \n  \n  # If you look closely at the data, you will note that g_04 is always Yes, so\n  # the lowest possible value of civ_engage is 1. The reason for this is that\n  # the sample was created by starting with a list of everyone who voted in the\n  # 2004 general election. Note how that fact makes the interpretation of the\n  # relevant population somewhat subtle.\n  \n  mutate(voter_class = case_when(civ_engage %in% c(5, 6) ~ \"Always Vote\",\n                                 civ_engage %in% c(3, 4) ~ \"Sometimes Vote\",\n                                 civ_engage %in% c(1, 2) ~ \"Rarely Vote\"),\n         voter_class = factor(voter_class, levels = c(\"Rarely Vote\", \n                                                      \"Sometimes Vote\", \n                                                      \"Always Vote\"))) %>% \n  \n  # Centering and scaling the age variable. Note that it would be smart to have\n  # some stopifnot() error checks at this point. For example, if civ_engage < 1\n  # or > 6, then something has gone very wrong.\n  \n  mutate(z_age = as.numeric(scale(age))) %>% \n  select(primary_06, treatment, sex, civ_engage, voter_class, z_age)\nobject_1 %>% \n  slice(1:3)## # A tibble: 3 x 6\n##   primary_06 treatment  sex    civ_engage voter_class    z_age\n##        <int> <fct>      <chr>       <int> <fct>          <dbl>\n## 1          0 Civic Duty Male            4 Sometimes Vote 1.05 \n## 2          0 Civic Duty Female          4 Sometimes Vote 0.638\n## 3          1 Hawthorne  Male            4 Sometimes Vote 0.361"},{"path":"n-parameters.html","id":"primary_06-treatment-sex","chapter":"11 N Parameters","heading":"11.3.2 primary_06 ~ treatment + sex","text":"section, look relationship primary voting treatment + sex.math:Without variable names:\\[ y_{} = \\beta_{0} + \\beta_{1}x_{, 1} + \\beta_{2}x_{,2} ... + \\beta_{n}x_{,n} + \\epsilon_{} \\]\nvariable names:\\[ y_{} = \\beta_{0} + \\beta_{1}civic\\_duty_i + \\beta_{2}hawthorne_i + \\beta_{3}self_i + \\beta_{4}neighbors_i + \\beta_{5}male_i + \\epsilon_{} \\]two ways formalize model used fit_1: without variable names. former related concept Justice acknowledge model constructed via linear sum n parameters times value n variables, along error term. words, linear model. model learned semester logistic model, kinds models, defined mathematics assumptions error term.\nsecond type formal notation, associated virtue Courage, includes actual variable names using. trickiest part transformation character/factor variables indicator variables, meaning variables 0/1 values. treatment 5 levels, need 4 indicator variables. fifth level — , default, first variable alphabetically (character variables) first level (factor variables) — incorporated intercept.Let’s translate model code.now create table nicely formats results fit_1 using tbl_regression() function gtsummary package. also display associated 95% confidence interval coefficient.\n          1\n          \n           \n          CI = Confidence Interval\n          Interpretation:\n* intercept model expected value probability someone voting 2006 primary given part control group female. case, estimate women control group vote ~29.1% time.\n* coefficient sexMale indicates difference likelihood voting male female. words, comparing men women, 0.01 implies men ~1.2% likely vote women. Note , linear model interactions sex variables, difference applies male, regardless treatment received. sex can manipulated (assumption), use causal interpretation coefficient.\n* coefficients treatments, hand, causal interpretation. single individual, either sex, sent Self postcard increases probability voting 4.8%. appears Neighbors treatment effective ~8.1% Civic Duty least effective ~1.8%.","code":"\nfit_1 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ treatment + sex,\n                  refresh = 0,\n                  seed = 987)\nprint(fit_1, digits = 3)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      primary_06 ~ treatment + sex\n##  observations: 344084\n##  predictors:   6\n## ------\n##                     Median MAD_SD\n## (Intercept)         0.291  0.001 \n## treatmentCivic Duty 0.018  0.003 \n## treatmentHawthorne  0.026  0.003 \n## treatmentSelf       0.048  0.002 \n## treatmentNeighbors  0.081  0.003 \n## sexMale             0.012  0.002 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.464  0.001 \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\ntbl_regression(fit_1, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%\n  \n  # Using Beta as the name of the parameter column is weird.\n  \n  as_gt() %>%\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Age Predict Likelihood of Voting\") %>%\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) %>% \n  cols_label(estimate = md(\"**Parameter**\"))"},{"path":"n-parameters.html","id":"primary_06-z_age-sex-treatment-voter_class-voter_classtreatment","chapter":"11 N Parameters","heading":"11.3.3 primary_06 ~ z_age + sex + treatment + voter_class + voter_class*treatment","text":"time look interactions! Create another model named fit_2 estimates primary_06 function z_age, sex, treatment, voter_class, interaction treatment voter classification.math:\n\\[y_{} = \\beta_{0} + \\beta_{1}z\\_age + \\beta_{2}male_i + \\beta_{3}civic\\_duty_i + \\\\ \\beta_{4}hawthorne_i + \\beta_{5}self_i + \\beta_{6}neighbors_i + \\\\ \\beta_{7}Sometimes\\ vote_i + \\beta_{8}Always\\ vote_i + \\\\ \\beta_{9}civic\\_duty_i Sometimes\\ vote_i + \\beta_{10}hawthorne_i Sometimes\\ vote_i + \\\\ \\beta_{11}self_i Sometimes\\ vote_i + \\beta_{11}neighbors_i Sometimes\\ vote_i + \\\\ \\beta_{12}civic\\_duty_i Always\\ vote_i + \\beta_{13}hawthorne_i Always\\ vote_i + \\\\ \\beta_{14}self_i Always\\ vote_i + \\beta_{15}neighbors_i Always\\ vote_i + \\epsilon_{}\\]\nTranslate code:first model, create regression table observe findings:\n          1\n          \n           \n          CI = Confidence Interval\n          Now summarized visual data, let’s interpret findings:\n* intercept fit_2 expected probability voting upcoming election woman average age (~ 50 years old data), assigned Control group, Rarely Voter. \nestimate 15.3%.\n* coefficient z_age, 0, implies change ~3.5% likelihood voting increment one standard deviation (~ 14.45 years). example: comparing someone 50 years old someone 65, latter 3.5% likely vote.\n* Exposure Neighbors treatment shows ~4.4% increase voting likelihood someone Rarely Vote category. random assignment treatment, can interpret coefficient estimate\naverage treatment effect.\n* someone different voter classification, calculation complex need account interaction term. example, individuals Sometimes Vote, treatment effect Neighbors 0.1%. Always Vote Neighbors, 0.1%.","code":"\nfit_2 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ z_age + sex + treatment + voter_class + \n                            treatment*voter_class,\n                  family = gaussian,\n                  refresh = 0,\n                  seed = 789)\nprint(fit_2, digits = 3)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      primary_06 ~ z_age + sex + treatment + voter_class + treatment * \n##     voter_class\n##  observations: 344084\n##  predictors:   17\n## ------\n##                                               Median MAD_SD\n## (Intercept)                                    0.153  0.003\n## z_age                                          0.035  0.001\n## sexMale                                        0.008  0.002\n## treatmentCivic Duty                            0.010  0.007\n## treatmentHawthorne                             0.008  0.007\n## treatmentSelf                                  0.024  0.007\n## treatmentNeighbors                             0.044  0.007\n## voter_classSometimes Vote                      0.114  0.003\n## voter_classAlways Vote                         0.294  0.004\n## treatmentCivic Duty:voter_classSometimes Vote  0.014  0.007\n## treatmentHawthorne:voter_classSometimes Vote   0.019  0.007\n## treatmentSelf:voter_classSometimes Vote        0.030  0.008\n## treatmentNeighbors:voter_classSometimes Vote   0.042  0.008\n## treatmentCivic Duty:voter_classAlways Vote    -0.001  0.009\n## treatmentHawthorne:voter_classAlways Vote      0.025  0.009\n## treatmentSelf:voter_classAlways Vote           0.025  0.009\n## treatmentNeighbors:voter_classAlways Vote      0.046  0.009\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.451  0.001 \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\ntbl_regression(fit_2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%\n  as_gt() %>%\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Other Variables Predict Likelihood of Voting\") %>%\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) %>% \n  cols_label(estimate = md(\"**Parameter**\"))"},{"path":"n-parameters.html","id":"temperance-8","chapter":"11 N Parameters","heading":"11.4 Temperance","text":"\nFIGURE 11.4: Temperance\nFinally, let’s remember virtue Temperance. gist temperance : humble inferences, inferences always, certainly, unfortunately going match real world. apply shaming scenario?Recall initial question: causal effect, likelihood voting, different postcards voters different levels political engagement?answer question, want look different average treatment effects treatment type voting behavior. real world, treatment effect person almost always different treatment effect person B.section, create plot displays posterior probability distributions average treatment effects men average age across combinations 4 treatments 3 voter classifications. means making total 12 inferences.Important note: look lots ages Male Female subjects. However, change estimates treatment effects. model linear, terms associated z_age sex disappear subtraction. one great advantages linear models.begin, need create newobs object.Now newobs work , need create object named plot_data collects treatment effect calculations.Recall , calculating treatment effect, need subtract estimate category control group category. example, wanted find treatment effect Always Vote Neighbors group, need: Always Vote Neighbors - Always Vote Control.Therefore, use mutate() twelve times, treatments voting frequencies. , pivot_longer order treatment effects sensibly categorized plotting. sounds confusing, read code comments carefully.Finally, plot data! Read code comments explanations aesthetic choices, well helpful discussion fct_reorder().interesting! shows us valuable bits information:interested average treatment effect postcards. 4 different postcards, can compared happened voter receive postcard.four treatment effects, however, heterogeneous. vary depending individual’s voting history, organize three categories: Rarely Vote, Sometimes Vote Always Vote. , 12 different\naverage treatment effects, one possible combination postcard voting history.combinations, graphic shows posterior distribution.mean us, consider postcards send?\n* Consider highest yellow distribution, posterior distribution average treatment effect receiving Neighbors postcard (compared getting postcard) Always Voters. posterior centered around 9% 95% confidence interval , roughly, 8% 10%.\n* Overall, Civic Duty Hawthorne postcards small average treatment effects, across three categories voter. causal effect Rarely Voters much smaller, regardless treatment. also much less precisely estimated many fewer Rarely Voters data.\n*best way increase turnover, assuming limits many postcards can send, focus Sometimes/Always voters use Neighbors postcard.Conclusion: limited number postcards, send Neighbors postcard citizens already demonstrate tendency vote.confident findings? needed convince boss right strategy, need explain confident assumptions. , must understand three levels knowledge world posteriors.","code":"\n# Because our model is linear, the terms associated with z_age and sex disappear\n# when we perform subtraction. The treatment effects calculated thereafter will\n# not only apply to males of the z-scored age of ~ 50 years. The treatment\n# effects apply to all participants, despite calling these inputs.\n\n\nsex <- \"Male\"\nz_age <- 0\ntreatment <- c(\"Control\",\n               \"Civic Duty\",\n               \"Hawthorne\",\n               \"Self\",\n               \"Neighbors\")\nvoter_class <- c(\"Always Vote\",\n                 \"Sometimes Vote\",\n                 \"Rarely Vote\")\n\n# This question requires quite the complicated tibble! Speaking both\n# hypothetically and from experience, keeping track of loads of nondescript\n# column names after running posterior_epred() while doing ATE calculations\n# leaves you prone to simple, but critical, errors. expand_grid() was created\n# for cases just like this - we want all combinations of treatments and voter\n# classifications in the same way that our model displays the interaction term\n# parameters.\n\nnewobs <- expand_grid(sex, z_age, treatment, voter_class) %>% \n  \n  # This is a handy setup for the following piece of code that allows us to\n  # mutate the ATE columns with self-contained variable names. This is what\n  # helps to ensure that the desired calculations are indeed being done. If you\n  # aren't familiar, check out the help page for paste() at `?paste`.\n  \n  mutate(names = paste(treatment, voter_class, sep = \"_\"))\n\npe <- posterior_epred(fit_2,\n                        newdata = newobs) %>% \n  as_tibble() %>% \n  \n  # Here we can stick the names that we created in newobs onto the otherwise\n  # unfortunately named posterior_epred() output. \n  \n  set_names(newobs$names)\nplot_data <- pe %>% \n  \n  # Using our cleaned naming system, ATE calculations are simple enough. Note\n  # how much easier the code reads because we have taken the trouble to line up\n  # the columns.\n  \n  mutate(`Always Civic-Duty`    = `Civic Duty_Always Vote`     - `Control_Always Vote`,\n         `Always Hawthorne`     = `Hawthorne_Always Vote`      - `Control_Always Vote`,\n         `Always Self`          = `Self_Always Vote`           - `Control_Always Vote`,\n         `Always Neighbors`     = `Neighbors_Always Vote`      - `Control_Always Vote`,\n         `Sometimes Civic-Duty` = `Civic Duty_Sometimes Vote`  - `Control_Sometimes Vote`,\n         `Sometimes Hawthorne`  = `Hawthorne_Sometimes Vote`   - `Control_Sometimes Vote`,\n         `Sometimes Self`       = `Self_Sometimes Vote`        - `Control_Sometimes Vote`,\n         `Sometimes Neighbors`  = `Neighbors_Sometimes Vote`   - `Control_Sometimes Vote`,\n         `Rarely Civic-Duty`    = `Civic Duty_Rarely Vote`     - `Control_Rarely Vote`,\n         `Rarely Hawthorne`     = `Hawthorne_Rarely Vote`      - `Control_Rarely Vote`,\n         `Rarely Self`          = `Self_Rarely Vote`           - `Control_Rarely Vote`,\n         `Rarely Neighbors`     = `Neighbors_Rarely Vote`      - `Control_Rarely Vote`) %>% \n  \n  # This is a critical step, we need to be able to reference voter\n  # classification separately from the treatment assignment, so pivoting in the\n  # following manner reconstructs the relevant columns for each of these\n  # individually. \n  \n  pivot_longer(names_to = c(\"Voter Class\", \"Group\"),\n               names_sep = \" \",\n               values_to = \"values\",\n               cols = `Always Civic-Duty`:`Rarely Neighbors`) %>% \n  \n    # Reordering the factors of voter classification forces them to be displayed\n    # in a sensible order in the plot later.\n  \n    mutate(`Voter Class` = fct_relevel(factor(`Voter Class`),\n                                     c(\"Rarely\",\n                                       \"Sometimes\",\n                                       \"Always\")))\nplot_data  %>% \n  \n  # Reordering the y axis values allows a smoother visual interpretation - \n  # you can see the treatments in sequential ATE.\n  \n  ggplot(aes(x = values, y = fct_reorder(Group, values))) +\n  \n  # position = \"dodge\" is the only sure way to see all 3 treatment distributions\n  # identity, single, or any others drop \"Sometimes\" - topic for further study\n  \n    stat_slab(aes(fill = `Voter Class`),\n              position = 'dodge') +\n    scale_fill_calc() +\n  \n    # more frequent breaks on the x-axis provides a better reader interpretation\n    # of the the shift across age groups, as opposed to intervals of 10%\n    \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1),\n                       breaks = seq(-0.05, 0.11, 0.01)) +\n    labs(title = \"Treatment Effects on The Probability of Voting\",\n         subtitle = \"Postcards work less well on those who rarely vote\",\n         y = \"Postcard Type\",\n         x = \"Average Treatment Effect\",\n         caption = \"Source: Gerber, Green, and Larimer (2008)\") +\n    theme_clean() +\n    theme(legend.position = \"bottom\")"},{"path":"n-parameters.html","id":"the-three-levels-of-knowledge","chapter":"11 N Parameters","heading":"11.4.1 The Three Levels of Knowledge","text":"exist three primary levels knowledge possible knowledge scenario: Truth (ideal Preceptor Table), DGM Posterior, Posterior.","code":""},{"path":"n-parameters.html","id":"the-truth-1","chapter":"11 N Parameters","heading":"11.4.1.1 The Truth","text":"know Truth (capital “T”), know ideal Preceptor Table. knowledge, can directly answer question precisely. can calculate individual’s treatment effect, summary measure might interested , like average treatment effect.level knowledge possible omniscient power, one can see every outcome every individual every treatment. Truth show, given individual, actions control, actions treatment, little factor impacted decisions.Truth represents highest level knowledge one can — , questions merely require algebra. need estimate treatment effect, different treatment effects different groups people. need predict — know.","code":""},{"path":"n-parameters.html","id":"dgm-posterior-1","chapter":"11 N Parameters","heading":"11.4.1.2 DGM posterior","text":"DGM posterior next level knowledge, lacks omniscient quality Truth. posterior posterior calculate perfect knowledge data generating mechanism, meaning correct model structure exact parameter values. often falsely conflated “posterior,” subject error model structure parameter value estimations.DGM posterior, certain individual’s causal effect, Fundamental Problem Causal Inference. words, can never measure one person’s causal effect unable see person’s resulting behavior treatment control; data one two conditions.DGM posterior posterior — estimate parameters based data predict future latest relevant information possible. difference , calculate posteriors unknown value DGM posterior, expect posteriors perfect.go boss estimates posterior, expect 95% confidence interval perfectly calibrated. , expect true value lie within 95% confidence interval 95% time. world, surprised see values outside confidence interval 5% time.","code":""},{"path":"n-parameters.html","id":"our-posterior-1","chapter":"11 N Parameters","heading":"11.4.1.3 Our posterior","text":"Unfortunately, posterior possesses even less certainty! real world, don’t perfect knowledge DGM: model structure exact parameter values. mean?go boss, tell best guess. informed estimate based relevant data possible. data, created 95% confidence interval treatment effect various postcards. estimate treatment effect Neighbors postcard 8% 10%.mean certain treatment effect Neighbors values? course ! tell boss, shocking find actual treatment effect less estimate.lot assumptions make process building model, processes Wisdom, subject error. Perhaps data match future well hoped. Ultimately, try account uncertainty estimates. Even safeguard, aren’t surprised bit .instance, shocked treatment effect Neighbors postcard 7%? 12%? course ! slightly , know posterior subject error. surprised treatment effect found 20%? Yes. large enough difference suggest real problem model, real world change forgot factor predictions., amounts large enough difference cause concern? words, wrong one-boss suspicious? “bad luck” sign stupidity? delve question next section chapter.","code":""},{"path":"n-parameters.html","id":"bad-luck-or-bad-work","chapter":"11 N Parameters","heading":"11.4.1.4 Bad luck or bad work?","text":"one problem, hard know “right,” posterior similar DGM posterior. , 5% time answer outside 95% confidence interval. truth ends , far away median posterior, boss rightly suspicious. many MAD SDs standard errors away truth obviously fool?many ways judge forecast. , ’re looking two main things: calibration forecast — , whether events said happen 30 percent time actually happened 30 percent time — forecast compared unskilled estimate relies solely historical averages. can answer questions using calibration plots skill scores, respectively. concepts bit advanced course, foundations important understand.Calibration plots compare predicted actually happened. Single predictions can difficult judge , often want group many predictions together bins plot averages bin’s forecasted increase voting actual increase voting. forecasts well-calibrated, bins calibration plot close 45 degree line; forecast poorly calibrated, bins away. second tool, skill scores, lets us evaluate forecasts even , combining accuracy appetite risk single number.Brier skill scores tell us much valuable forecasts unskilled estimate, one informed historical averages — e.g., guess postcard increase voting 5%.technical ways can judge work’s accuracy. boss likely judge using methods.instance, answer many questions (creating many posteriors different problems) , time, boss get sense actual skill, median truth proportion confidence intervals correctly calibrated.know, experience, posteriors often narrow. assume know DGM , fact, know . knowledge? First, prepare boss fact. humility Temperance. Second, estimate dozens different models combine posteriors. result might well median correct posterior, confidence intervals much wider. concepts advanced Primer, important consider making predictions.","code":""},{"path":"n-parameters.html","id":"summary-11","chapter":"11 N Parameters","heading":"11.5 Summary","text":"Use tidy() function broom.mixed package make models \\(N\\) parameters easier interpret.Use tidy() function broom.mixed package make models \\(N\\) parameters easier interpret.function familiar , stan_glm(), used create models \\(N\\) parameters.function familiar , stan_glm(), used create models \\(N\\) parameters.important remember data equal truth.important remember data equal truth.population like make inferences population data. matter wisdom whether data maps closely enough population studying.population like make inferences population data. matter wisdom whether data maps closely enough population studying.dealing models many parameters, double check know find true slope intercepts — often, requires adding numerous values coefficient studying.dealing models many parameters, double check know find true slope intercepts — often, requires adding numerous values coefficient studying.","code":""},{"path":"tools.html","id":"tools","chapter":"Tools","heading":"Tools","text":"chapter broken following sections. Read whichever ones relevant.Absolute relative file pathsWorking terminalGit, GitHub, RStudioPDFStyle guideHow use RpubsHow get helpHow make table","code":""},{"path":"tools.html","id":"absolute-and-relative-file-paths","chapter":"Tools","heading":"11.6 Absolute and relative file paths","text":"read data R, first need tell R data lives. times, data file. file live computer (local) somewhere internet (remote).place file lives computer called “path.” can think path directions file. path includes location file name file . two kinds paths: relative absolute. relative path describes location file relative currently computer. absolute path file respect base (root) folder computer’s filesystem. Absolute paths always start forward slash, “/.”Consider file called report.csv. Read file using relative path:Read report.csv using absolute path:ensure code can run different computer, use relative paths. added bonus ’s also less typing! absolute path file (names folders computer’s root / file) isn’t usually across different computers. example, suppose Fatima Jayden working project together report.csv data. Fatima’s file stored /home/Fatima/files/report.csv,Jayden’s stored /home/Jayden/files/report.csv.Even though Fatima Jayden stored files place computers, absolute paths different due different usernames. Jayden code loads report.csv data using absolute path, code won’t work Fatima’s computer. relative path inside files folder (files/report.csv) computers; code uses relative paths work !One important part using paths recognizing spaces file name. example, trying use path/home/preceptor/desktop/projects/important data/report.csvdoes work includes space file name, “important data.” Instead, path/home/preceptor/desktop/projects/important\\ data/report.csvis valid allows access report.csv data. \\ tells computer treat next character character instead something special. example, space character normally used show break code, computer treats break point whenever sees space. However, \\ tells computer space isn’t break point instead part file path, solving issue.See video another explanation:Source: Udacity course “Linux Command Line Basics”","code":"x <- read_csv(\"data/report.csv\")x <- read_csv(\"/home/preceptor/desktop/projects/data/report.csv\")"},{"path":"tools.html","id":"working-with-the-terminal","chapter":"Tools","heading":"Working with the terminal","text":"Terminal powerful window allows interact computer’s filesystem directly, uses file paths find files want interact .Let’s open Terminal tab left window start learning use Terminal.","code":""},{"path":"tools.html","id":"pwd-working-directory","chapter":"Tools","heading":"pwd: Working directory","text":"first question may working Terminal might : can’t see folders, know ? Well ’s great place start learning Terminal. see current folder , type pwd (print working directory):currently directory (folder) called Yao, directory named /Users. forward slash front “Users” tells directory lowest possible level.","code":""},{"path":"tools.html","id":"ls-seeing-items-in-the-directory","chapter":"Tools","heading":"ls: Seeing items in the directory","text":"see items current folder, use command ls (list). Type ls terminal hit return/enter. see something like :Notice lists exactly items bottom right window RStudio. Terminal just another way interact computer’s filesystem. Anything can normally mouse/trackpad, like opening folder, can also Terminal.","code":""},{"path":"tools.html","id":"cd-changing-directories","chapter":"Tools","heading":"cd: Changing directories","text":"move one directory another, use cd (change directory). ’ll using cd change Desktop folder.change Desktop directory, type cd Desktop/. helpful hint, type first letters folder file name, can hit tab computer auto complete name. Try ! Type cd Desk hit tab auto complete name!type ls , can see item Desktop listed.go back previous folder (aka directory ), can type cd .. two periods represent one level . can see hierarchy view Mac:","code":""},{"path":"tools.html","id":"mkdir-and-rmdir-make-and-remove-a-directory","chapter":"Tools","heading":"mkdir and rmdir: Make and remove a directory","text":"Now ’re Desktop folder, let’s get set-stay organized Gov 1005. Staying organized critical working many data projects. , using mkdir Gov-1005 (make directory) can create folder exclusively Gov 1005 like :Now, type ls, can see new folder created. Note used hyphen Gov 1005. Terminal can’t recognize spaces unless put \\ , like : mkdir Gov\\ 1005. Never use spaces weird characters file directory names.remove folder, use rmdir (remove directory). won’t using right now don’t need remove anything.","code":""},{"path":"tools.html","id":"touch-creating-files","chapter":"Tools","heading":"touch: Creating files","text":"order experiment next commands Terminal, ’ll need test file. Type touch text.txt create test file., course, can see test.txt file created using ls.","code":""},{"path":"tools.html","id":"mv-moving-files","chapter":"Tools","heading":"mv: Moving files","text":"Oh ! created test.txt file, Gov-1005 folder, right now ’s desktop. happened created Gov-1005 folder using mkdir, forgot move using cd Gov-1005/. worries, can move file folder using mv:using mv first thing type mv file want move. next thing location want move . case want move test.txt Gov-1005/, type mv test.txt Gov-1005/. , can use cd enter Gov-1005 folder use ls see test.txt file successfully moved Gov-1005 directory.","code":""},{"path":"tools.html","id":"cp-copying-files","chapter":"Tools","heading":"cp: Copying files","text":"Copying files similar moving files Terminal. Using previous example, wanted copy test.txt Gov-1005 folder delete original test.txt file, just replace mv cp (copy paste):","code":"cp test.txt Gov-1005/"},{"path":"tools.html","id":"rm-removing-files","chapter":"Tools","heading":"rm: Removing files","text":"Ok, last Terminal command book teaching . , ’re done test.txt file. Let’s remove rm (remove):Make sure Gov-1005 folder type rm test.txt! Using ls, can see test file now gone.Congrats! now able basic tasks Terminal! want learn Terminal commands, check Sean Kross’s Unix Workbench.","code":""},{"path":"tools.html","id":"git-github-and-rstudio","chapter":"Tools","heading":"Git, GitHub, and RStudio","text":"next section focuses connecting GitHub RStudio using Git. care GitHub? Think Google Drive R code projects. computer blows , GitHub save R work just Google Drive saves paper.","code":""},{"path":"tools.html","id":"installing-git","chapter":"Tools","heading":"Installing Git","text":"first step using GitHub installing Git computer. first, may already Git installed computer. check, go Terminal type git --version. already Git, command return Git version installed. get error, can download install git .","code":""},{"path":"tools.html","id":"github-accounts","chapter":"Tools","heading":"GitHub accounts","text":"installing Git, ’ll need GitHub account. like Google account. However, one difference GitHub account visible public. want pick name carefully. professional since sending potential employers link GitHub account near future. Check former Gov 1005 students’ GitHub profiles inspiration:Evelyn CaiEvelyn CaiJessica EdwardsJessica EdwardsBeau MecheBeau MecheOnce GitHub account, ready connect Git RStudio account. Type following two commands Terminal pane. Replace Name name @email.com email used sign GitHub.","code":"git config --global user.name \"Your Name\"\ngit config --global user.email \"your@email.com\""},{"path":"tools.html","id":"github-repositories","chapter":"Tools","heading":"GitHub repositories","text":"now ready create GitHub repository (repo). GitHub repo similar Google Drive folder. make first repo, make sure signed go GitHub homepage click green new button left.want choose good name repo add brief description. use productivity. can choose make repo public private, recommend make repo public important world see. keeps public GitHub profile clean professional. repo probably private. Let’s also add README file repo. document can add information.now first repo GitHub. next step download computer — process often known “cloning” — start editing syncing using Git. , ’ll need copy link repo use RStudio. , green button friend. Click copy link shown. can use clipboard button right automatically copy .","code":""},{"path":"tools.html","id":"connecting-github-to-rstudio","chapter":"Tools","heading":"Connecting GitHub to RStudio","text":"now ready connect productivity repo RStudio. link productivity repo copied, can go back RStudio begin new project. Go File, New Project:Next, ’ll need go steps create project: Version Control Git paste link GitHub click Create Project.Congrats! ’ve linked productivity repo RStudio. Note Github ask location place projects. recommend creating folder desktop called “projects” placing RStudio projects . Don’t just scatter across computer mess. dozens . organized!","code":""},{"path":"tools.html","id":"updating-.gitignore","chapter":"Tools","heading":"Updating .gitignore","text":"first thing always working new repo updating .gitignore file. can open file bottom right window Files tab. file includes files don’t want uploaded GitHub. can come handy working big datasets files private information. case, want add productivityl.Rproj file .gitignore list.file private project file usually don’t want uploaded GitHub. , .gitignore, ’ll want add *.Rproj * tells computer want prevent files ending .Rproj uploaded. also just add productivity.Rproj.Save .gitignore file see productivity.Rproj file disappear Git tab top right window. don’t see changes, click refresh button upper left.symbols Git tab part “conversation” Git. “?” Git’s way saying: “new file . want ?” Adding line .gitignore way replying “Ignore file.”","code":""},{"path":"tools.html","id":"commit-and-push","chapter":"Tools","heading":"Commit and Push","text":"Now ’ve updated .gitignore file, want upload new version GitHub. , first select .gitignore file click Commit button Git window:open new window write commit message. message short note ’re adding/changing repo. case, ’ve updated .gitignore let’s write just :Press commit. way telling Git “Yes files want upload. ’m committed.” Next, press Push. pushes uploads files GitHub. (can probably guess pull , won’t using yet)Now, go GitHub repo refresh page, can see .gitignore file uploaded commit message:Congrats! just uploaded first file GitHub.One tricky aspect caching Github ID password. likely, type things first push. bad. , , Github needs know , otherwise people mess repo. hundreds commits/pushes term. don’t want type ID/password time! Follow instructions . Read link, key commands, issued R Console, :, logging , bring back Github. Accept defaults press Generate token button bottom. (may need change Note generated tokens .) Copy token created. look something like:8be3e800891425f8462c4491d9a4dbb5b1c1f35cThen, issue R command:Provide token. start new RStudio instance, Github ask login/password . might just ask one time. Seek help work.Happy Git GitHub useR best source Git Github problems arise.depart thinking setup Git GitHub, aware updates scheduled coming months affect able access GitHub account. Currently, able establish connection account R session username, password, email. present, primary change coming imposition 2 factor authentication (2FA) current setup. able now edit github account settings, recommend familiarize process fully enabling 2FA event wish work cloud Spring 2021 iteration course. 2FA sign-requirement often creates disruption connection virtual R session won’t allow save work efficiently. continue learn work R Rstudio (certainly encourage !), aware ‘brownouts’ June July year designed remind users switch 2FA. summary, none information invalidates initial setup guides , rather serves memo find future self struggling complications related matters.","code":"\nusethis::create_github_token()\ngitcreds::gitcreds_set()"},{"path":"tools.html","id":"pdf","chapter":"Tools","heading":"PDF","text":"Generating PDF files RStudio easy hard. easy R markdown designed produce files variety output formats, including PDF. hard , RStudio make PDF files, computer set must set LaTeX installation. four options:Making PDF files may just “work,” especially using Mac. Give try!Making PDF files may just “work,” especially using Mac. Give try!doesn’t just work, strongly recommend using tinytex R package. First, install R package.doesn’t just work, strongly recommend using tinytex R package. First, install R package.Second, use R package install underlying LaTeX distribution.Depending operating system, may work. error message providing instructions. Follow instructions.Restart R everything just work.can just generate html file, open Chrome, select Print . . . drop-menu. get pop-window. Click arrow right Destination choose Save PDF drop-menu. ’ll see preview. Choose Save PDF option. convenient workflow , disaster strikes problem set due 10 minutes, reasonable option.can just generate html file, open Chrome, select Print . . . drop-menu. get pop-window. Click arrow right Destination choose Save PDF drop-menu. ’ll see preview. Choose Save PDF option. convenient workflow , disaster strikes problem set due 10 minutes, reasonable option.can install full LaTeX installation . Good luck! Don’t come us help.can install full LaTeX installation . Good luck! Don’t come us help.","code":"\ninstall.packages('tinytex')\ntinytex::install_tinytex()"},{"path":"tools.html","id":"style-guide","chapter":"Tools","heading":"Style guide","text":"Much material comes Tidyverse Style Guide. take points work submitted violates guidelines. extremis, may go advice, add code comment work explaining decision .","code":""},{"path":"tools.html","id":"comments","chapter":"Tools","heading":"Comments","text":"Include comments code. Easy--understand chunks code comments. code comment. code merit many, many lines comments, lines code . given file, many total lines comments lines code.Make comments meaningful. simple description code . best comments descriptions approaches tried considered. (code already tells us .) Good comments often “Dear Diary” quality: “. tried . finally chose thing reasons X, Y Z. work , look approach.” , structure often paragraph comments followed several lines code.line comment begin comment symbol (“hash”) followed single space: #. Code comments must separated code one empty line sides. Format code comments neatly. Ctrl-Shift-/ easiest way . Name R code chunks, without using weird characters spaces. download_data good R code chunk name. Plot #1 .Spelling matters. Comments constructed sentences, appropriate capitalization punctuation.","code":""},{"path":"tools.html","id":"graphics","chapter":"Tools","heading":"Graphics","text":"Use captions, titles, axis labels make clear tables graphics mean.Anytime make graphic without title (explaining graphic ), subtitle (highlighting key conclusion draw), caption (information source data) axis labels (information variables), justify decision code comment. (try ) always include items situations makes less sense. Ultimately, decisions , need understand reasoning.Use best judgment. example, sometimes axis labels unnecessary. Read Data Visualization: practical introduction Kieran Healy guidance making high quality graphics.","code":""},{"path":"tools.html","id":"formating","chapter":"Tools","heading":"11.6.1 Formating","text":"","code":""},{"path":"tools.html","id":"long-lines","chapter":"Tools","heading":"Long Lines","text":"Limit code 80 characters per line. fits comfortably printed page reasonably sized font. calling functions, can omit argument names common arguments (.e. arguments used almost every invocation function). Short unnamed arguments can also go line function name, even whole function call spans multiple lines.","code":""},{"path":"tools.html","id":"whitespace","chapter":"Tools","heading":"Whitespace","text":"%>% always space , usually followed new line. first step pipe, line indented two spaces. structure makes easier add new steps (rearrange existing steps) harder overlook step.ggplot2 code handled similar fashion. commands initial invocation ggplot() indented.","code":"\n# Good\n\niris %>%\n  group_by(Species) %>%\n  summarize_if(is.numeric, mean) %>%\n  ungroup() %>%\n  gather(measure, value, -Species) %>%\n  arrange(value)\n\n# Bad\n\niris %>% group_by(Species) %>% summarize_all(mean) %>%\nungroup %>% gather(measure, value, -Species) %>%\narrange(value)\n# Good\n\ndiamonds %>% \n  ggplot(aes(x = depth)) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\n# Bad\n\ndiamonds %>% \nggplot(aes(x = depth)) +\ngeom_histogram(bins = 100) + labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")"},{"path":"tools.html","id":"commas","chapter":"Tools","heading":"Commas","text":"Always put space comma, never , just like regular English.","code":"\n# Good\n\nx[, 1]\n\n# Bad\n\nx[,1]\nx[ ,1]\nx[ , 1]"},{"path":"tools.html","id":"parentheses","chapter":"Tools","heading":"Parentheses","text":"put spaces inside outside parentheses regular function calls.","code":"\n# Good\n\nmean(x, na.rm = TRUE)\n\n# Bad\n\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )"},{"path":"tools.html","id":"infix-operators","chapter":"Tools","heading":"Infix operators","text":"infix operators (=, ==, +, -, <-, et cetera) surrounded one space.operators — like ~, ::, :::, $, @, [, [[, ^, : — never surrounded spaces.may add extra spaces improves alignment = <-.add extra spaces places space usually allowed.","code":"\n# Good\n\nheight <- (feet * 12) + inches\nmean(x, na.rm = TRUE)\n\n# Bad\n\nheight<-feet*12+inches\nmean(x, na.rm=TRUE)\n# Good\n\nsqrt(x^2 + y^2)\ndf$z\nx <- 1:10\ny ~ a + b\n\n# Bad\n\nsqrt(x ^ 2 + y ^ 2)\ndf $ z\nx <- 1 : 10\ny~a + b\nlist(total = a + b + c,\n     mean = (a + b + c) / n)"},{"path":"tools.html","id":"messageswarningserrors","chapter":"Tools","heading":"Messages/Warnings/Errors","text":"R messages/warnings/errors never appear submitted document. right way deal issues find cause fix underlying problem. Students sometimes use “hacks” make messages/warnings/errors disappear. common hacks involve using code chunk options like message = FALSE, warning = FALSE, results = \"hide\", include = FALSE others. Don’t , general. message/warning/error worth understanding fixing. Don’t close eyes (metaphorically) pretend problem doesn’t exist. situations, however, , matter try, can’t fix problem. cases, can use one hacks, must make code comment directly , explaining situation. exception “setup” chunk (included default every new Rmd) comes include = FALSE. chunk, explanation necessary, convention.","code":""},{"path":"tools.html","id":"how-to-use-rpubs","chapter":"Tools","heading":"How to use Rpubs","text":"Rpubs provides free hosting service R work. use :Begin creating new repository GitHub. clone computer. calling repository “rpubs_example.” , put *Rproj .gitignore file. prevent private project file uploaded GitHub.Start new R Markdown file. Go File –> New File –> R Markdown. simplicity, leave name “Untitled” hit “OK.”Save file, , “Untitled” project directory.Knit. see following.Notice blue icon upper right-hand corner reads “Publish.” Click .asked whether want publish RPubs RStudio Connect. Choose RPubs. get reminder documents publish RPubs publicly visible. Click “Publish.”take RPubs website. need create account. Follow steps prompted.Add document details. Name document. Add meaningful slug – otherwise end ugly, long address didn’t choose can’t remember. can leave Description blank simplicity exercise.Hit “Continue,” et voilá! published first document Rpubs!one important step. “rsconnect” contains files specific computer want push GitHub. Therefore, .Rproj files , want add rsconnect folder .gitignore file. Click .gitignore, add hit “Save.” see disappear GitHub top right window. don’t see changes, hit Refresh button top right corner. Since ’ve updated .gitignore file, now good time commit push changes GitHub repository.","code":""},{"path":"tools.html","id":"how-to-get-help","chapter":"Tools","heading":"How to get help","text":"best data science superpower knowing ask question. – Mara Averick","code":""},{"path":"tools.html","id":"searching-for-help-with-r","chapter":"Tools","heading":"Searching for Help with R","text":"Google best friend. question something R, someone probably question someone else probably answered online. Stack Overflow RStudio Community two best forums finding asking questions/solutions. Adding “R” /“tidyverse” keyword search helps find relevant results exact question. specific possible wording question!","code":""},{"path":"tools.html","id":"reproducible-examples","chapter":"Tools","heading":"Reproducible Examples","text":"don’t find answer question ’re still stuck, ask question forums! order get best response, sharing reproducible example community allows others easily start left . reprex package allows easily.First, install reprex package. , load package.Let’s now look problematic code. data set “murders” package “dslabs” provides murder statistics, well population counts, states. Suppose want calculate rate murders state per 100k residents (number murders/population * 10^6). However, code misspelled “population,” resulting error:use reprex, highlight code necessary packages. copy highlighted code pressing Ctrl/Cmd + c. code now clipboard. Console, type reprex() hit Enter/Return. reprex automatically creates reproducible example places clipboard.Now now go favorite R forum paste reproducible example question! example post Slack.post RStudio community.first time post RStudio Community, sure keep window open seconds pasted question window. (prevent post sent Moderation concern bot.)","code":"\ninstall.packages(\"reprex\")\nlibrary(reprex)"},{"path":"tools.html","id":"how-to-make-a-table","chapter":"Tools","heading":"How to make a table","text":"gt R package creating elegant tables. First, ’ll create gt summary table observations data. Second, ’ll run regression display outcome using gtsummary, companion package gt specializes presenting results statistical models.want learn gt check fantastic guide. Go official gt package website. See extensive guide gtsummary.Load necessary libraries.set message=FALSE code chunk avoid showing ugly notes libraries loaded.Let’s pull data use table:Create simplest table gt(), key command: Now let’s make professional. gt offers variety functions add features like these1:can add title subtitle using tab_header(): default, titles text can formatted. want formatting, must wrap character string call md(), md stands (M)ark(d). example, bolded title. can use tab_spanner() add spanner columns. c() argument takes variables spanner column cover., current table include spanner column. wish see examples spanner columns, go Chapter 4. can change column names using cols_label(): Use tab_source_note() cite source data create caption. function exclusively providing source — though ’s handy way — can used display text ’d like: Using md() , can italicize name Enos study caption: Now table structure looks good, want format numbers . Let’s add dollar signs income column using fmt_currency(). function also adds commas (want commas without dollar signs use fmt_number()). c() within fmt_currency() denotes variable formatted currency: Note line return title “Intergroup” “Contact” effect break title displayed md().","code":"\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(gt)\nx <- trains %>%\n  select(gender, income, att_end) %>%\n  slice(1:5)\nx## # A tibble: 5 x 3\n##   gender income att_end\n##   <chr>   <dbl>   <dbl>\n## 1 Female 135000      11\n## 2 Female 105000      10\n## 3 Male   135000       5\n## 4 Male   300000      11\n## 5 Male   135000       5\nx %>% \n  gt()\nx %>% \n  gt() %>%\n   tab_header(title = \"Enos Data Observations\", \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\nx %>% \n  gt()%>%\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\nx %>% \n  gt()%>%\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") %>% \n   tab_spanner(label = \"Name of Spanner Column Here\", c(gender, income))\nx %>% \n  gt()%>%\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\") %>%\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\")\nx %>% \n  gt()%>%\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") %>%\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") %>% \n  tab_source_note(\"Source: Ryan Enos\")\nx %>% \n  gt()%>%\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") %>%\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") %>% \n  tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup Contact on Exclusionary Attitudes*\"))\nx %>% \n  gt() %>%\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\")%>%\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\") %>% \n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                       Contact on Exclusionary Attitudes*\")) %>%\n    fmt_currency(columns = c(income), \n                 decimals = 0) "},{"path":"tools.html","id":"regression-tables","chapter":"Tools","heading":"Regression tables","text":"can making gt table stan_glm() regression object. Key gtsummary package tbl_regression() function.\n          1\n          \n           \n          CI = Confidence Interval\n           ","code":"\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\nfit2 <- stan_glm(att_end ~ party, data = trains, refresh = 0)\n\ntbl_regression(fit2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 2)) %>%\n  as_gt() %>%\n    tab_header(title = \"Regression of Attitudes about Immigration\", \n               subtitle = \"The Effect of Party on End Attitude\") %>%\n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                        Contact on Exclusionary Attitudes*\"))"},{"path":"functions.html","id":"functions","chapter":"Functions","heading":"Functions","text":"","code":""},{"path":"functions.html","id":"introduction-1","chapter":"Functions","heading":"11.7 Introduction","text":"function piece code packaged way makes easy reuse. Functions make easy filter(), arrange(), select(), create tibble(), seen Chapters 1 2. Functions also allow transform variables perform mathematical calculations. use functions like rnorm() runif() generate random draws distribution.Every time reference function Primer, include parentheses. call function including parentheses necessary arguments within parentheses. correct call rnorm():run function name without parentheses, R return code makes function.Functions can sorts things. sample() takes vector values returns number values randomly selected vector. can specify number random values argument size. call equivalent rolling die.Functions can also take functions arguments. example, replicate() takes expression repeats n times. replicated rolling die ten times?especially useful type function family map_* functions, purrr package, automatically loaded library(tidyverse). functions apply function every row tibble.Let’s create tibble one variable x takes three values: 3, 7, 2.easy use mutate create new variable, sq_root, square root value x.map_* functions provide another approach. map_* function takes two required arguments. First object want iterate. generally column tibble working. Second function want run row tibble.map_dbl() (pronounced “map-double”) took function sqrt() applied element x. two tricky parts use map_* functions. First, need put tilde symbol — “~” — name function want call. Without ~, get error:Second, need include period — “.” — spot variable goes. Using name variable — x case — generate error.Tilde dot (~ .) easy forget.know expected output function, can specify kind vector:map(): listmap_lgl(): logicalmap_int(): integermap_dbl(): double (numeric)map_chr(): charactermap_df(): data frameSince example returns numeric output, use map_dbl() instead map().key difference using mutate() map_* functions map_* functions designed work well lists, inputs outputs. mutate() designed atomic vectors, meaning vectors element single value.","code":"\nrnorm(n = 1)## [1] 0.33\nrnorm## function (n, mean = 0, sd = 1) \n## .Call(C_rnorm, n, mean, sd)\n## <bytecode: 0x7ffabd6837d8>\n## <environment: namespace:stats>\nsample(x = 1:6, size = 1)## [1] 1\nreplicate(10, sample(1:6, 1))##  [1] 5 4 6 1 3 5 3 4 1 3\nlibrary(tidyverse)\ntibble(x = c(4, 16, 9))## # A tibble: 3 x 1\n##       x\n##   <dbl>\n## 1     4\n## 2    16\n## 3     9\ntibble(x = c(4, 16, 9)) %>% \n  mutate(sq_root = sqrt((x)))## # A tibble: 3 x 2\n##       x sq_root\n##   <dbl>   <dbl>\n## 1     4       2\n## 2    16       4\n## 3     9       3\ntibble(x = c(4, 16, 9)) %>% \n  mutate(sq_root = map_dbl(x, ~ sqrt(.)))## # A tibble: 3 x 2\n##       x sq_root\n##   <dbl>   <dbl>\n## 1     4       2\n## 2    16       4\n## 3     9       3\ntibble(x = c(4, 16, 9)) %>% \n  mutate(sq_root = map_dbl(x, sqrt(.)))## Error: Problem with `mutate()` column `sq_root`.\n## ℹ `sq_root = map_dbl(x, sqrt(.))`.\n## x Can't convert a `tbl_df/tbl/data.frame` object to function\ntibble(x = c(4, 16, 9)) %>% \n  mutate(sq_root = map_dbl(x, ~ sqrt(x)))## Error: Problem with `mutate()` column `sq_root`.\n## ℹ `sq_root = map_dbl(x, ~sqrt(x))`.\n## x Result 1 must be a single double, not a double vector of length 3"},{"path":"functions.html","id":"list-columns-and-map-functions-1","chapter":"Functions","heading":"11.8 List-columns and map functions","text":"Recall list different atomic vector. atomic vectors, element vector one value. Lists, however, can contain vectors, even complex objects, elements.x list two elements. element numeric vector length 3. second element character vector length 2. use [[]] extract specific elements. Example:first [[]] extracts first element form list x. second `[[]]`` extracts 3rd element vector first element.number built-R functions output lists. example, ggplot objects making store plot information lists. function returns multiple values can used create list output wrapping returned object list().Notice 1x1 tibble one observation, list one element. Voila! just created list-column.function returns multiple values vector, like range() , must use list() wrapper want create list-column.list column column data list rather atomic vector. Like lists, can pipe str() examine column.can use map_* functions create list-column , much importantly, work list-column afterwards. Example:flexibility possible via use list-columns map_* functions. workflow extremely common. start empty tibble, using ID specify number rows. skeleton, step pipe adds new column, working column already exists.Let’s practice nhanes dataset primer.data package. add column dataset included quantiles height variable gender?Select relevant variables, group gender. grouping curious height distributed gender. drop rows missing data.two approaches. first, happy output tibble just two rows:Note need use map_* functions case. simple dplyr approach works fine. “trick” use list() wrap output quantile(). Use str() examine exact values.Men taller women throughout distribution, smallest individual (child) data, see 0% quantile, happens male.second case involves scenario want “lose” rows tibble. want q_height column rows, even values included repetitive. common scenario want use q_height perform calculation individual. , need map_* function.first four lines pipe cases. difference use list(quantile(height)) first map(height, ~ quantile(.) second.now, practiced using map_* functions built-R functions. Sometimes, however, R function want. happens, need create function.","code":"\nx <- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx## [[1]]\n## [1]  4 16  9\n## \n## [[2]]\n## [1] \"A\" \"Z\"\nx[[1]][3]## [1] 9\nx <- rnorm(10)\n\n# range() returns the min and max of the argument \n\nrange(x)## [1] -1.0  1.3\ntibble(col_1 = list(range(x))) ## # A tibble: 1 x 1\n##   col_1    \n##   <list>   \n## 1 <dbl [2]>\ntibble(col_1 = list(range(x))) %>%\n  str()## tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -1.02 1.33\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) %>% \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) %>% \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) %>% \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))## # A tibble: 3 x 4\n##      ID draws        max min_max  \n##   <int> <list>     <dbl> <list>   \n## 1     1 <dbl [10]> 2.38  <dbl [2]>\n## 2     2 <dbl [10]> 0.944 <dbl [2]>\n## 3     3 <dbl [10]> 1.29  <dbl [2]>\nlibrary(primer.data)\nnhanes %>%\n  select(gender, height) %>%\n  drop_na() %>% \n  group_by(gender)## # A tibble: 9,647 x 2\n## # Groups:   gender [2]\n##    gender height\n##    <chr>   <dbl>\n##  1 Male     165.\n##  2 Male     165.\n##  3 Male     165.\n##  4 Male     105.\n##  5 Female   168.\n##  6 Male     133.\n##  7 Male     131.\n##  8 Female   167.\n##  9 Female   167.\n## 10 Female   167.\n## # … with 9,637 more rows\nnhanes %>%\n  select(gender, height) %>%\n  drop_na() %>% \n  group_by(gender) %>% \n  summarise(q_height = list(quantile(height)),\n            .groups = \"drop\")## # A tibble: 2 x 2\n##   gender q_height \n##   <chr>  <list>   \n## 1 Female <dbl [5]>\n## 2 Male   <dbl [5]>\nnhanes %>%\n  select(gender, height) %>%\n  drop_na() %>% \n  group_by(gender) %>% \n  summarise(q_height = list(quantile(height)),\n            .groups = \"drop\") %>% \n  str()## tibble [2 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ gender  : chr [1:2] \"Female\" \"Male\"\n##  $ q_height:List of 2\n##   ..$ : Named num [1:5] 83.8 154.3 160.6 165.9 184.5\n##   .. ..- attr(*, \"names\")= chr [1:5] \"0%\" \"25%\" \"50%\" \"75%\" ...\n##   ..$ : Named num [1:5] 83.6 166.2 173.8 179.4 200.4\n##   .. ..- attr(*, \"names\")= chr [1:5] \"0%\" \"25%\" \"50%\" \"75%\" ...\nnhanes %>%\n  select(gender, height) %>%\n  drop_na() %>% \n  group_by(gender) %>% \n  summarize(q_height = map(height, ~ quantile(.)),\n            .groups = \"drop\")## # A tibble: 9,647 x 2\n##    gender q_height \n##    <chr>  <list>   \n##  1 Female <dbl [5]>\n##  2 Female <dbl [5]>\n##  3 Female <dbl [5]>\n##  4 Female <dbl [5]>\n##  5 Female <dbl [5]>\n##  6 Female <dbl [5]>\n##  7 Female <dbl [5]>\n##  8 Female <dbl [5]>\n##  9 Female <dbl [5]>\n## 10 Female <dbl [5]>\n## # … with 9,637 more rows"},{"path":"functions.html","id":"custom-functions","chapter":"Functions","heading":"11.9 Custom Functions","text":"many built-functions R. function composed name, list arguments body. create functions function() function.","code":""},{"path":"functions.html","id":"creating-your-own-functions","chapter":"Functions","heading":"11.9.1 Creating your own functions","text":"Assume want create function adds 1 1 together. first step write R code .code become “body” function, part curly braces. also need function definition, composed name function, call function() function, pair curly braces.Combining function definition body function completes process.just created function! function return 1 + 1 whenever called.Consider function adds number 6 value x, value want allow user provide.incorporated first formal argument. Formal arguments functions additional parameters allow user customize use function. Instead adding 1 + 1 , function takes number x user defines adds 6. Consider function two formal arguments.","code":"\n1 + 1## [1] 2\nadd_one_and_one <- function(){}\nadd_one_and_one <- function(){\n  1 + 1\n}\n\nadd_one_and_one()## [1] 2\nadd_six_to_something <- function(x){\n  x + 6\n}\n\nadd_six_to_something(x = 1)## [1] 7\nadd_x_to_y <- function(x, y) {\n  x + y\n}\n\nadd_x_to_y(1, 2)## [1] 3\nadd_x_to_y(4, 3)## [1] 7"},{"path":"functions.html","id":"anonymous-functions-with-map_-functions","chapter":"Functions","heading":"11.9.2 Anonymous functions with map_* functions","text":"can create functions perform operations “fly,” without bothering give name. nameless functions called anonymous functions.can use anonymous functions conjunction map_* family functions. probably common use anonymous functions, least Primer.can call anonymous function using ~ operator using . represent current element. Consider three approaches:three produce answer, expect. Just using mutate() best, long accomplishes goal. complex situations, especially involving simulation, often won’t. Example:Calling rnorm(), function random component, effect probably want context simple mutate(). Instead, R runs rnorm(1) , copies value generated remaining two rows tibble. get different value row, need explicitly tell R using map_* function:Note parentheses anonymous function necessary. long everything ~ works R code, anonymous function work, time replacing . value relevant row .x variable — old case.","code":"\ntibble(old = c(4, 16, 9)) %>% \n  mutate(new_1 = old + 6) %>% \n  mutate(new_2 = map_dbl(old, ~ add_six_to_something(.))) %>% \n  mutate(new_3 = map_dbl(old, ~ (. + 6)))## # A tibble: 3 x 4\n##     old new_1 new_2 new_3\n##   <dbl> <dbl> <dbl> <dbl>\n## 1     4    10    10    10\n## 2    16    22    22    22\n## 3     9    15    15    15\ntibble(ID = 1:3) %>% \n  mutate(x = rnorm(1))## # A tibble: 3 x 2\n##      ID      x\n##   <int>  <dbl>\n## 1     1 -0.282\n## 2     2 -0.282\n## 3     3 -0.282\ntibble(ID = 1:3) %>% \n  mutate(x = rnorm(1)) %>% \n  mutate(y = map_dbl(ID, ~ rnorm(1)))## # A tibble: 3 x 3\n##      ID      x      y\n##   <int>  <dbl>  <dbl>\n## 1     1 -0.282 -1.31 \n## 2     2 -0.282  0.795\n## 3     3 -0.282  0.270\ntibble(old = c(4, 16, 9)) %>% \n  mutate(new = map_dbl(old, ~ . + 1))## # A tibble: 3 x 2\n##     old   new\n##   <dbl> <dbl>\n## 1     4     5\n## 2    16    17\n## 3     9    10"},{"path":"functions.html","id":"skateboard-perfectly-formed-rear-view-mirror","chapter":"Functions","heading":"11.9.3 Skateboard >> perfectly formed rear-view mirror","text":"image — widely attributed Spotify development team — conveys important point.\nFIGURE 11.5: ultimate guide Minimum Viable Product (+great examples)\nBuild skateboard build car fancy car part. limited--functioning thing useful. also keeps spirits high.related Telescope Rule:faster make four-inch mirror six-inch mirror make six-inch mirror.","code":""},{"path":"functions.html","id":"no_na_sampler","chapter":"Functions","heading":"11.10 no_NA_sampler()","text":"Assume want sample 10 observations height nhanes tibble primer.data package. easy built function sample().One problem approach sample missing values height. can avoid manipulating vector inside call sample().works, , first, ugly code. , second, hard extend constraints. example, assume want sample individuals missing values variables, just height. , really make custom function. Call function no_NA_sampler().first step function creation write code normal pipe want function . case, code look like:start nhanes, use drop_na() remove rows missing values variable, sample 10 rows random pull height. turn function, just need copy/paste pipe within body function definition:Voila! function just executes code within body. first step building function write function. write code want function execute.first version, however, “hard codes” lot options might want change. want sample 5 values height 500? case, hard code new number place “10.” better option add argument can pass whatever value want.want sample different variable height different tibble nhanes? , trick turn hard coded values arguments. argument tbl placeholder data set, n number samples want extracted data set, var variable samples studying.R know interpret something like age passed argument. double curly braces around var tell R, essence, var variable tibble created sampling input tibble tbl. can use order arguments, without naming , no_NA_sampler(), just R function:Now function want, add comments error checking.comments code seem weird? Perhaps. good comments! First, many lines comments lines code. good rule thumb. Second, comments simple report code . redundant! code tells us code . comments, instead, discussion issues related code, things don’t understand, topics revisit. like diary. Good programmers keep good diaries.","code":"\nsample(nhanes$height, size = 10)##  [1] 159 119 170 171 181 136  99 169 153 175\nsample(nhanes$height[! is.na(nhanes$height)], size = 10)##  [1] 163 141 158 160 169 183 117 158  86 176\nnhanes %>% \n  drop_na() %>%\n  slice_sample(n = 10) %>% \n  pull(height)##  [1] 157 166 171 160 156 152 166 150 175 160\nno_NA_sampler <- function(){\n  nhanes %>% \n    drop_na() %>%\n    slice_sample(n = 10) %>% \n    pull(height)\n}\n\nno_NA_sampler()##  [1] 162 165 159 156 159 160 157 171 164 156\nno_NA_sampler <- function(n){\n  nhanes %>% \n    drop_na() %>%\n    slice_sample(n = n) %>% \n    pull(height)\n}\n\nno_NA_sampler(n = 2)## [1] 164 170\nno_NA_sampler(n = 25)##  [1] 156 172 166 168 166 164 164 161 159 156 163 164 168 170 155 177 173 166 169\n## [20] 173 174 157 159 157 159\nno_NA_sampler <- function(tbl, var, n){\n  tbl %>% \n    drop_na() %>%\n    slice_sample(n = n) %>% \n    pull({{var}})\n}\n\nno_NA_sampler(tbl = nhanes, var = height, n = 2)## [1] 164 163\nno_NA_sampler(trains, age, 5)## [1] 45 42 41 31 44\nno_NA_sampler <- function(tbl, var, n){\n  \n  # Function for grabbing `n` samples from a variable `var` which lives in a\n  # tibble `tbl`. \n  \n  # I could not figure out how to check to see if `var` actually lives in the\n  # tibble in my error checking. Also, I don't like that I need to use\n  # is_double() as the check on `n` even though I want `n` to be an integer.\n  \n  stopifnot(is_tibble(tbl))\n  stopifnot(is_double(n))\n\n  tbl %>% \n    drop_na() %>%\n    \n    # What happens if n is \"too large\"? That is, I need to think harder about a)\n    # whether or not I am sampling with or without replacement and b) which I\n    # should be doing.\n    \n    slice_sample(n = n) %>% \n    pull({{var}})\n}"},{"path":"functions.html","id":"prediction-game","chapter":"Functions","heading":"11.11 Prediction Game","text":"Let’s play prediction game. Consider kenya tibble primer.data.game pick random value rv13, number people live vicinity polling station. guess number. guess number. winner Prediction Game person whose guess closest random value selected. Example:Run code R Console try . works! also sloppy disorganized. first step writing good code write bad code.don’t want play Prediction Game just . want play thousands times. Copy/pasting code thousand times stupid. Instead, need function. Just place working code within function definition, Voila!function definition , changes. Yet, creating function, can now easily run many times.problem version want prediction_game() return message winner. Right now, returns nothing. just prints winner. Let’s change , also allow guesses passed argument, along tibble variable. can leave n hard coded 1 since, definition, Prediction Game attempt guess one number, least now. , need use return() function , executed, causes function finish return whatever value within paratheses.general, want store results tibble, makes later analysis plotting easier.wins game play 1,000 times?hardly surprising 500 wins often 600 since mean rv13 539.23. mean seems like pretty good guess! best guess.test whether mean median better guess, use created prediction_game function guesses 442 (median) 539 (mean) plot results.mean bad prediction. best prediction (surprisingly?) median, 442.","code":"\nkenya## # A tibble: 1,672 x 9\n##    block poll_station treatment poverty distance pop_density mean_age reg_byrv13\n##    <chr> <chr>        <fct>       <dbl>    <dbl>       <dbl>    <dbl>      <dbl>\n##  1 KWAL… 007/001      control     0.247     22.0    0.00296      39.6    0.00358\n##  2 KWAL… 007/004      local + …   0.329     25.1    0.000888     43.8    0.0742 \n##  3 KWAL… 007/009      local       0.263     27.8    0.00184      34.7    0.00691\n##  4 KWAL… 007/011      local + …   0.429     27.2    0.000270     44.6    0.26   \n##  5 KWAL… 007/017      local + …   0.341     19.3    0.000544     39.0    0.0228 \n##  6 KWAL… 007/018      local       0.204     24.0    0.00798      37.1    0.00243\n##  7 KWAL… 007/019      SMS         0.272     25.1    0.00167      39.2    0.00487\n##  8 KWAL… 007/020      control     0.316     23.8    0.000538     36.3    0      \n##  9 KWAL… 007/022      canvass     0.396     20.5    0.000216     42.9    0.00575\n## 10 KWAL… 007/023      local + …   0.398     14.5    0.000148     39.8    0.0360 \n## # … with 1,662 more rows, and 1 more variable: rv13 <dbl>\nyour_guess <- 500\nmy_guess <- 600\n\nsampled_value <- no_NA_sampler(kenya, rv13, n = 1) \n\nyour_error <- abs(your_guess - sampled_value)\nmy_error <- abs(my_guess - sampled_value)\n\nif(your_error < my_error) cat(\"You win!\")## You win!\nif(your_error > my_error) cat(\"I win!\")\nprediction_game <- function(){\n  your_guess <- 500\n  my_guess <- 600\n  \n  sampled_value <- no_NA_sampler(kenya, rv13, n = 1) \n  \n  your_error <- abs(your_guess - sampled_value)\n  my_error <- abs(my_guess - sampled_value)\n  \n  if(your_error < my_error) cat(\"You win!\")\n  if(your_error > my_error) cat(\"I win!\")\n}\nreplicate(3, prediction_game())## You win!You win!You win!## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\nprediction_game <- function(guesses, tbl, var){\n  \n  # Check to make sure that guesses is a vector of doubles of length 2.\n  \n  stopifnot(all(is_double(guesses)))\n  stopifnot(length(guesses) == 2)\n  \n  # This tells the function that the \"guess\" inputted first in the \n  # guesses is \"your\" guess, whereas the second input is \"my\" guess.\n  \n  your_guess <- guesses[1]\n  my_guess <- guesses[2]\n  \n  # Use the function no_NA_sampler to draw a sample from a data set\n  # of our choosing, with a {{var}} and n.\n  \n  sampled_value <- no_NA_sampler(tbl, {{var}}, n = 1) \n  \n  # Subtract the sampled value obtained from no_NA_sampler from \n  # both of our guesses. \n  \n  your_error <- abs(your_guess - sampled_value)\n  my_error <- abs(my_guess - sampled_value)\n  \n  # If the difference between your guess and the sampled value is \n  # less than the difference between my guess and the sampled value\n  # (meaning that your guess was closer to the truth), the function\n  # returns the message \"Guess, your_guess, wins!\".\n  \n  if(your_error < my_error){ \n    return(paste(\"Guess\", your_guess, \"wins!\"))\n  }\n  \n  # If your error exceeds my error (meaning that your guess was\n  # further than the truth than mine), the function prints the \n  # message \"Guess, my_guess, wins!\" \n  \n  if(your_error > my_error){ \n    return(paste(\"Guess\", my_guess, \"wins!\"))\n  }\n  \n  # If we guess the same number, and our error rates are therefore\n  # identical, we return the message \"A tie!\". \n  \n  if(your_error == my_error){ \n    return(\"A tie!\")\n  }\n\n}\nreplicate(5, prediction_game(guesses = c(500, 600), kenya, rv13))## [1] \"Guess 500 wins!\" \"Guess 500 wins!\" \"Guess 600 wins!\" \"Guess 500 wins!\"\n## [5] \"Guess 500 wins!\"\ntibble(ID = 1:3) %>% \n  mutate(result = map_chr(ID, ~ \n                            prediction_game(guesses = c(500, 600),\n                                            kenya, \n                                            rv13)))## # A tibble: 3 x 2\n##      ID result         \n##   <int> <chr>          \n## 1     1 Guess 500 wins!\n## 2     2 Guess 500 wins!\n## 3     3 Guess 500 wins!\ntibble(ID = 1:1000) %>% \n  mutate(result = map_chr(ID, ~ \n                            prediction_game(guesses = c(500, 600),\n                                            kenya, \n                                            rv13))) %>% \n  ggplot(aes(result)) +\n    geom_bar()\ntibble(ID = 1:1000) %>% \n  mutate(result = map_chr(ID, \n                          ~ prediction_game(c(442, 539),\n                                            kenya,\n                                            rv13))) %>% \n  ggplot(aes(result)) +\n    geom_bar()"},{"path":"functions.html","id":"playing-within-a-tibble","chapter":"Functions","heading":"11.11.0.1 Playing within a tibble","text":"cases, convenient play portions Prediction Game within tibble. Imagine trying guess biggest value 10 random samples.can now manipulate result column see prediction better. Using structure , subtract guesses variable guessing; case, biggest value 10 random samples.Run test 1,000 times.Empirically, see 900 much better guess 800. Instead calling function run 1,000 times, just performed step within row tibble 1,000 rows. approaches work. best choice depends context problem.","code":"\ntibble(ID = 1:3, guess_1 = 800, guess_2 = 900) %>% \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10)))## # A tibble: 3 x 4\n##      ID guess_1 guess_2 result    \n##   <int>   <dbl>   <dbl> <list>    \n## 1     1     800     900 <dbl [10]>\n## 2     2     800     900 <dbl [10]>\n## 3     3     800     900 <dbl [10]>\ntibble(ID = 1:3, guess_1 = 800, guess_2 = 900) %>% \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) %>% \n  mutate(biggest = map_dbl(result, ~ max(.))) %>% \n  mutate(error_1 = abs(guess_1 - biggest)) %>% \n  mutate(error_2 = abs(guess_2 - biggest)) %>% \n  mutate(winner = case_when(error_1 < error_2 ~ \"Guess one wins!\",\n                            error_1 > error_2 ~ \"Guess two wins!\",\n                            TRUE ~ \"A tie!\"))## # A tibble: 3 x 8\n##      ID guess_1 guess_2 result     biggest error_1 error_2 winner         \n##   <int>   <dbl>   <dbl> <list>       <dbl>   <dbl>   <dbl> <chr>          \n## 1     1     800     900 <dbl [10]>    1382     582     482 Guess two wins!\n## 2     2     800     900 <dbl [10]>    2246    1446    1346 Guess two wins!\n## 3     3     800     900 <dbl [10]>    1422     622     522 Guess two wins!\ntibble(ID = 1:1000, guess_1 = 800, guess_2 = 900) %>% \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) %>% \n  mutate(biggest = map_dbl(result, ~ max(.))) %>% \n  mutate(error_1 = abs(guess_1 - biggest)) %>% \n  mutate(error_2 = abs(guess_2 - biggest)) %>% \n  mutate(winner = case_when(error_1 < error_2 ~ \"Guess one wins!\",\n                            error_1 > error_2 ~ \"Guess two wins!\",\n                            TRUE ~ \"A tie!\")) %>% \n  ggplot(aes(winner)) +\n    geom_bar()"},{"path":"functions.html","id":"summary-12","chapter":"Functions","heading":"11.12 Summary","text":"first step writing good code write bad code.Tilde dot (~ .) easy forget.first step building function write function. write code want function execute.","code":""},{"path":"functions.html","id":"lists-and-list-columns","chapter":"Functions","heading":"11.12.1 Lists and list-columns","text":"list different atomic vector. Atomic vectors familiar us: element vector one value, thus atomic vector column data set, observation gets single value. Lists, however, can contain vectors, complex objects, elements.various ways create lists. common use list() function “wrap” object. map() always returns list.can take list column , applying anonymous function map(), create another list column. similar taking tibble piping function, like mutate(), returns new tibble work .can also use map_* functions take list column input return atomic vector – column single value per observation – output.function returns multiple values vector, must use list() wrapper want create list-column.","code":""},{"path":"functions.html","id":"writing-functions","chapter":"Functions","heading":"11.12.2 Writing functions","text":"Optimize usefulness adding formal arguments needed. function gives option n may helpful function allows us enter options data set, variable, n value.Give arguments sensible names.default, function returns result last line body. Use return() override default.starting function, remember smaller steps easier trying build everything one motion. general: start writing body, test body basic function, add formal arguments.Use double curly braces around vars, since R know interpret variable names passed argument. double curly braces tell R var variable tibble.","code":""},{"path":"functions.html","id":"distributions-1","chapter":"Functions","heading":"11.12.3 Distributions","text":"word “distribution” can mean two things. First, object — mathematical formula, imaginary urn — can draw values. Second, list values.two important aspects distribution center variability.median often stable measure center mean. mad (scaled median absolute deviation) often stable measure variation standard deviation.Outliers cause lack stability. distribution without outliers, mean/median mad/sd close value matter much ones use.","code":""},{"path":"maps.html","id":"maps","chapter":"Maps","heading":"Maps","text":"order make maps, first need data.2","code":""},{"path":"maps.html","id":"tidycensus","chapter":"Maps","heading":"Tidycensus","text":"need tidyverse tidycensus packages.Note use tidycensus, ’ll need Census API key. can request key . key, use census_api_key() function:run census_api_key() function option install = TRUE, save API key .Renviron don’t run census_api_key() every time want get data Census. However, . can examine/edit .Renviron file directly usethis::edit_r_environ().two main sources getting data : American Community Survey (ACS) Decennial Census. sources available data.census.gov.click data.census.gov., brought homepage search bar. can type topic interest.example, search “resident population,” bring page various sources. bolded title, see Survey/Program , tells data source. focusing ACS Decennial Census sources.get data Decennial Census, use get_decennial(). key arguments geography, variables, year.geography determines unit analysis (.e. “geography” data. example, use “state,” many geographies use, “us” entire country, “county” counties, .geography determines unit analysis (.e. “geography” data. example, use “state,” many geographies use, “us” entire country, “county” counties, .variables selects Census variables want. know variable dealing , two options.variables selects Census variables want. know variable dealing , two options.first way: look data.census.gov website. addition Survey/Program, also see bolded term Tables. tell name variable. However, variable names little tricky. example, variable name population “BO1003.”second way: use load_variables() function tidycensus generate tibble variable names (described ). two key arguments load_variables() year dataset.\nyear, must use year end-year Decennial Census. dataset argument, can either use “sf1” “sf3.”year last argument get_decennial() using. get_decennial() can obtain data 1990, 2000, 2010 Census. can find year variable Years .Now better understanding arguments function get_decennial(), let’s practice using !Consider following code.output tibble four columns:GEOID part FIPS code, short Federal Information Processing Standard. ’s standardized way identify states, counties, census tracts, etc. instance ’s two characters wide. specific get Census boundaries, longer code becomes.NAME generic name get_decennial() gives unit selected geography; , state names. Note 52 observations; “state” geography includes District Columbia Puerto Rico, along 50 states.variable name variable selected.value value variable selected (, population).default, get_decennial() stack variables top select one, identifying variable column. let’s say wanted know proportion population state lives rural areas. select two variables (total population rural population) receive tibble 104 observations, state appearing per variable. may helpful way receive data, depending purposes. (faceting, may want data long format like , ’ll see .) can request data wide format instead using option output = \"wide\"., created tibble states rows total population (“P001001”) rural population (“P002005”) columns. plot proportion state’s population lives rural areas now simple application tidyverse functions know love. First, let’s create variable rural population proportion order states variable:Maine Vermont rural Washington D.C. entirely urban. wanted sense proportion rural residents varied geographically? need map!","code":"\nlibrary(tidyverse)\nlibrary(tidycensus)\ncensus_api_key(\"API KEY\")  # You must replace API KEY with your actual key.\nlibrary(tidyverse)\n\npop <- get_decennial(geography = \"state\",\n                     variables = \"P001001\",\n                     year = 2010)\n\nglimpse(pop)## Rows: 52\n## Columns: 4\n## $ GEOID    <chr> \"01\", \"02\", \"04\", \"05\", \"06\", \"22\", \"21\", \"08\", \"09\", \"10\", \"…\n## $ NAME     <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Lo…\n## $ variable <chr> \"P001001\", \"P001001\", \"P001001\", \"P001001\", \"P001001\", \"P0010…\n## $ value    <dbl> 4.8e+06, 7.1e+05, 6.4e+06, 2.9e+06, 3.7e+07, 4.5e+06, 4.3e+06…\nrural <- get_decennial(geography = \"state\",\n                       variables = c(\"P001001\", \"P002005\"),\n                       year = 2010,\n                       output = \"wide\")\nglimpse(rural)## Rows: 52\n## Columns: 4\n## $ GEOID   <chr> \"01\", \"02\", \"04\", \"05\", \"06\", \"22\", \"21\", \"08\", \"09\", \"10\", \"1…\n## $ NAME    <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Lou…\n## $ P001001 <dbl> 4.8e+06, 7.1e+05, 6.4e+06, 2.9e+06, 3.7e+07, 4.5e+06, 4.3e+06,…\n## $ P002005 <dbl> 1957932, 241338, 651358, 1278329, 1880350, 1215567, 1806024, 6…\nrural %>%\n  mutate(prop_rural = P002005/P001001) %>% \n  ggplot(aes(x = prop_rural, y = fct_reorder(NAME, prop_rural))) +\n    geom_point() +\n    labs(title = \"Rural Population in US States in 2010\",\n         subtitle = \"Maine and Vermont are the most rural states\",\n         caption = \"Source: US Census\",\n         x = \"Rural Population Proportion\",\n         y = NULL)"},{"path":"maps.html","id":"conceptual-introduction-to-mapping","chapter":"Maps","heading":"Conceptual introduction to mapping","text":"two underlying important pieces information spatial data: coordinates object coordinates relate physical location Earth, also known coordinate reference system CRS.coordinates familiar geography. CRS uses three-dimensional model earth define specific locations surface grid. object can defined relation longitude (East/West) latitude (North/South).gets complicated attempting create projection. projection translation three-dimensional grid onto two-dimensional plane. animation demonstrates process.Thus, CRS determines geometric object look displayed two-dimensional screen. rarely need specify CRS working tidycensus, good know concept ever work spatial data.","code":""},{"path":"maps.html","id":"vector-versus-spatial-data","chapter":"Maps","heading":"Vector versus spatial data","text":"Spatial data defined CRS can either vector raster data. Vector data based points can connected form lines polygons. located within coordinate reference system. example road map.Raster data, however, values within grid system, satellite imagery. Primer, dealing vector data, format get data tidycensus package.","code":""},{"path":"maps.html","id":"sf-vs-sp","chapter":"Maps","heading":"sf vs sp","text":"older package, sp, lets user handle vector raster data. book focus vector data sf package. main differences sp sf packages store CRS information. sp uses spatial sub classes, sf stores data data frames, allowing interact dplyr methods ’ve learned far.","code":""},{"path":"maps.html","id":"shapefiles","chapter":"Maps","heading":"Shapefiles","text":"R can handle importing different kinds file formats spatial data, including KML geojson. ’ll focus shapefiles, created Esri 1990s. Though refer “shapefile” singular, ’s actually collection least three basic files:.shp - lists shape vertices.shx - index offsets.dbf - relationship file geometry attributes (data)files must present directory named (except file extension) import correctly. Thankfully, tidycensus grab geometric information Census shapefile .","code":""},{"path":"maps.html","id":"mapping-with-tidycensus-and-geom_sf","chapter":"Maps","heading":"Mapping with tidycensus and geom_sf()","text":"order start mapping R, need get little data tidycensus package. particular, need set geometry = TRUE.similar tibble created . However, two key differences. First, now funky “multipolygon” column called geometry. list column containing information ggplot() needs create map. Second, rural longer tibble.class() function tells us , uh, “class” object. rural class “sf,” special kind tibble includes information plotting. reason, never use as_tibble() object class “sf” since strips object key attributes needs make plotting easier.order create map using ggplot(), need new geom: geom_sf(). works much like geoms seen , geom_point() geom_line(), except spatial data. particular, designed work objects class “sf.” Example:boundaries state, including Alaska Hawaii. problems. ggplot2 best fit everything one image, taxing system. can’t see particular state well, map zoomed far . Also, colors didn’t fill data., let’s create new map geom_sf() fill prop_rural. ’ll filter Alaska, Hawaii, Puerto Rico now.Now something usable! already lot ’d want map—notably, states shaded based variable interest, helping us see patterns data. use bit makeover, ’ll give next section.","code":"\nrural <- get_decennial(geography = \"state\",\n                       variables = c(\"P001001\", \"P002005\"),\n                       year = 2010,\n                       output = \"wide\",\n                       geometry = TRUE) \n\nglimpse(rural)## Rows: 52\n## Columns: 5\n## $ GEOID    <chr> \"01\", \"02\", \"04\", \"05\", \"06\", \"22\", \"21\", \"08\", \"09\", \"10\", \"…\n## $ NAME     <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Lo…\n## $ P001001  <dbl> 4.8e+06, 7.1e+05, 6.4e+06, 2.9e+06, 3.7e+07, 4.5e+06, 4.3e+06…\n## $ P002005  <dbl> 1957932, 241338, 651358, 1278329, 1880350, 1215567, 1806024, …\n## $ geometry <MULTIPOLYGON [°]> MULTIPOLYGON (((-85 31, -85..., MULTIPOLYGON (((…\nclass(rural)## [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nrural %>%\n  ggplot() +\n  geom_sf()\nrural %>%\n  filter(! NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")) %>%\n  ggplot(aes(fill = P002005 / P001001)) +\n  geom_sf()"},{"path":"maps.html","id":"making-maps-pretty","chapter":"Maps","heading":"Making maps pretty","text":"ways can aesthetically improve map:Make fill colors easier distinguishMake darker colors map onto higher values prop_ruralRemove gray backgroundGive legend informative title add title captionA great function providing fill colors maps scale_fill_viridis_c(). different color palettes can selected option argument, easily distinguishable displayed black white people common forms colorblindness. can also reverse default order colors direction = -1 option. function continuous variables prop_rural; discrete variable, can use analogous scale_fill_viridis_d().’ll also use theme_void(), great theme maps gets rid gray background. Finally, ’ll use labs() give legend title “Percent Rural” (multiply values variable 100) add overall title caption.map, clear rural states concentrated Great Plains, South, parts New England, (South)west Northeast less rural.","code":"\nrural %>%\n  filter(! NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")) %>%\n  ggplot(aes(fill = 100 * P002005 / P001001)) +\n    geom_sf() + \n    scale_fill_viridis_c(option = \"plasma\",\n                         direction = -1) +\n    labs(title = \"Rural geography of the United States\",\n         caption = \"Source: Census 2010\",\n         fill = \"Percent Rural\") +\n    theme_void()"},{"path":"maps.html","id":"adding-back-alaska-and-hawaii","chapter":"Maps","heading":"Adding back Alaska and Hawaii","text":"Alaska Hawaii? want display map without zoom , can take advantage argument get_decennial(), shift_geo = TRUE:Now, Alaska Hawaii can displayed near lower 48 states. option removes Puerto Rico tibble altogether, good choice want show data Puerto Rico.","code":"\nrural_shifted <- get_decennial(geography = \"state\",\n                               variables = c(\"P001001\", \"P002005\"),\n                               year = 2010,\n                               output = \"wide\",\n                               geometry = TRUE,\n                               shift_geo = TRUE) %>%\n  rename(state = NAME) %>%\n  mutate(prop_rural = P002005/P001001)\nrural_shifted %>%\n  ggplot(aes(fill = prop_rural * 100)) +\n  geom_sf() + \n  scale_fill_viridis_c(option = \"plasma\",\n                       direction = -1) +\n  labs(title = \"Rural geography of the United States\",\n       caption = \"Source: Census 2010\",\n       fill = \"Percent Rural\") +\n  theme_void()"},{"path":"maps.html","id":"faceting-maps","chapter":"Maps","heading":"Faceting maps","text":"powerful tool ggplot2 use maps faceting. Let’s grab data ACS population Harris County, Texas census tracts race:code similar ’ve used , except retrieving data American Community Survey using get_acs() instead decennial census. new features worth pointing :year get_acs() last year five year sample. Thus, data 2014–2018. can choose years 2009–2018.Since geography “tract,” specifying state county.obtaining data long format, makes faceting easier.added summary_var, “B02001_001,” total population. ’ll see, appears separate column, helpful us. (exercise, try going back code created rural see long format summary_var.)Let’s take look harris:similar ’ve seen . Note now moe summary_moe columns, stand “margin error.” , unlike decennial census, ACS survey thus values get estimates true value.3","code":"\nracevars <- c(White = \"B02001_002\", \n              Black = \"B02001_003\", \n              Asian = \"B02001_005\",\n              Hispanic = \"B03003_003\")\nharris <- get_acs(geography = \"tract\",\n                  variables = racevars, \n                  year = 2018,\n                  state = \"TX\",\n                  county = \"Harris County\",\n                  geometry = TRUE,\n                  summary_var = \"B02001_001\") \nglimpse(harris)## Rows: 3,144\n## Columns: 8\n## $ GEOID       <chr> \"48201100000\", \"48201100000\", \"48201100000\", \"48201100000\"…\n## $ NAME        <chr> \"Census Tract 1000, Harris County, Texas\", \"Census Tract 1…\n## $ variable    <chr> \"White\", \"Black\", \"Asian\", \"Hispanic\", \"White\", \"Black\", \"…\n## $ estimate    <dbl> 3426, 1045, 230, 892, 2936, 3591, 7, 2119, 2973, 885, 0, 3…\n## $ moe         <dbl> 390, 308, 106, 241, 1358, 2196, 14, 1013, 430, 242, 13, 47…\n## $ summary_est <dbl> 5063, 5063, 5063, 5063, 6820, 6820, 6820, 6820, 4403, 4403…\n## $ summary_moe <dbl> 478, 478, 478, 478, 3685, 3685, 3685, 3685, 502, 502, 502,…\n## $ geometry    <MULTIPOLYGON [°]> MULTIPOLYGON (((-95 30, -95..., MULTIPOLYGON …"},{"path":"maps.html","id":"transforming-and-mapping-the-data","chapter":"Maps","heading":"Transforming and mapping the data","text":"Now can use facet_wrap() look race variables side--side:Note easy create percentages using summary_est. also used color = Percent scale_color_viridis_c() avoid annoying borders around census tracts. Otherwise, doesn’t differ much code , yet much easier make comparisons across variables. Faceting powerful tool use maps.","code":"\nharris %>%\n  mutate(Percent = 100 * (estimate / summary_est)) %>%\n  ggplot(aes(fill = Percent, color = Percent)) +\n  facet_wrap(~ variable) +\n  geom_sf() +\n  scale_fill_viridis_c(direction = -1) +\n  scale_color_viridis_c(direction = -1) +\n  labs(title = \"Racial geography of Harris County, Texas\",\n       caption = \"Source: American Community Survey 2014-2018\") +\n  theme_void()"},{"path":"maps.html","id":"working-with-big-data","chapter":"Maps","heading":"Working with big data","text":"Instead census tract map just one city, let’s “big data” project involving every census track country, plotting percentage people two races.Start finding correct variable American Community Survey using load_variables() function. function takes two required arguments: year Census endyear ACS sample, specific dataset. Example:2018 ACS, variable ’re looking called \"B02001_008\". also need total population (\"B02001_001\") calculate percentage. total 74,134 census tracts US. Note state.name vector included base R, includes names every state US.case, use state.name make continental, vector every state US Alaska Hawaii.set size 0.003 create thin outlines around census tracts, larger make hard see tracts. also use inferno option scale_fill_viridis() different aesthetic previous plots. add theme_void() labs() .","code":"\nacs2018 <- load_variables(2018, \"acs5\")\n\nacs2018\ncontinental <- state.name[! state.name %in% c(\"Alaska\", \"Hawaii\")]\n\nraces <- get_acs(geography = \"tract\",\n                 state = continental,\n                 variables = \"B02001_008\", \n                 year = 2018,\n                 summary_var = \"B02001_001\",\n                 geometry = TRUE)\n\nraces## Simple feature collection with 72359 features and 7 fields (with 196 geometries empty)\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -120 ymin: 25 xmax: -67 ymax: 49\n## Geodetic CRS:  NAD83\n## # A tibble: 72,359 x 8\n##    GEOID    NAME                 variable estimate   moe summary_est summary_moe\n##    <chr>    <chr>                <chr>       <dbl> <dbl>       <dbl>       <dbl>\n##  1 0111703… Census Tract 303.17… B02001_…      223   117        4100         301\n##  2 0111901… Census Tract 115, S… B02001_…       11    20        4380         441\n##  3 0112101… Census Tract 109, T… B02001_…       55    69        3102         389\n##  4 0112501… Census Tract 101.02… B02001_…       16    27        3026         348\n##  5 0108900… Census Tract 7.01, … B02001_…       90    65        2617         389\n##  6 0108900… Census Tract 22, Ma… B02001_…       76    48        1921         197\n##  7 0108900… Census Tract 30, Ma… B02001_…       27    35        2589         345\n##  8 0109503… Census Tract 309.02… B02001_…      122    86        4504         379\n##  9 0109700… Census Tract 4.01, … B02001_…        0    12        1068         177\n## 10 0109700… Census Tract 4.02, … B02001_…        0    12         833         113\n## # … with 72,349 more rows, and 1 more variable: geometry <MULTIPOLYGON [°]>\nraces  %>%\n  mutate(Percent = 100 * (estimate / summary_est)) %>%\n  ggplot(aes(fill = Percent)) +\n  geom_sf(size = 0.003) +\n  scale_fill_viridis_c(direction = -1, option = \"inferno\") +\n  labs(title = \"Percent of People Who are Two or More Races by Census Tract\",\n       caption = \"Source: American Community Survey 2014-2018\") +\n  theme_void()"},{"path":"maps.html","id":"pums","chapter":"Maps","heading":"PUMS","text":"Census microdata, often referred ‘Public Use Microdata Samples’ PUMS, contains advanced census data individual people. PUMS contains data roughly 1% US population. access PUMS, use get_PUMS() function, works similar way get_decennial() get_acs().’re trouble finding PUMS variables represents , tidycensus dataset pums_variables can helpful.make US_pums contain age, sex, income data every single person PUMS. AGEP age, FINCP income, SEX sex.using every single individual PUMS make US_pums, get enormous tibble 3 million rows! might look intimidating first, can use basic dplyr functions usual.One trade-using PUMS data compared aggregated data get state public use microdata area (PUMA) individual microdata. PUMAs Census geographies contain least 100,000 people entirely within single state. built census tracts counties may may similar recognized geographic boundaries. New York City, instance, PUMAs closely aligned Community Districts. , interested pulling data block groups, census tracts, small areas, can’t use PUMS data. get_pums() always return SERIALNO, SPORDER, WGTP, PWGTP, ST. SERIALNO SPORDER variables uniquely identify observations, WGTP PWGTP housing-unit person weights, ST state code.setting recode = TRUE, create new _label columns recode numerical values represent.","code":"\nglimpse(pums_variables)## Rows: 31,759\n## Columns: 12\n## $ survey     <chr> \"acs1\", \"acs1\", \"acs1\", \"acs1\", \"acs1\", \"acs1\", \"acs1\", \"ac…\n## $ year       <chr> \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"20…\n## $ var_code   <chr> \"RT\", \"RT\", \"SERIALNO\", \"DIVISION\", \"DIVISION\", \"DIVISION\",…\n## $ var_label  <chr> \"Record Type\", \"Record Type\", \"Housing unit/GQ person seria…\n## $ data_type  <chr> \"chr\", \"chr\", \"chr\", \"chr\", \"chr\", \"chr\", \"chr\", \"chr\", \"ch…\n## $ level      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ val_min    <chr> \"H\", \"P\", \"2017000000001\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6…\n## $ val_max    <chr> \"H\", \"P\", \"2017999999999\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6…\n## $ val_label  <chr> \"Housing Record or Group Quarters Unit\", \"Person Record\", \"…\n## $ recode     <lgl> TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n## $ val_length <int> 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2…\n## $ val_na     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\nUS_pums <- get_pums(variables = c(\"AGEP\", \"FINCP\", \"SEX\"),\n                    state = state.name,\n                    recode = TRUE,\n                    survey = \"acs1\")"},{"path":"maps.html","id":"plotting-with-pums","chapter":"Maps","heading":"Plotting with PUMS","text":"PUMS useful? Well, allows us create new interesting variables.Let’s try making map PUMS. Let’s say wanted make map percentage population states Northwest seniors, define percent population 65 . Although ’s variable tell us directly, can use PUMS construct . want data detailed possible, map data PUMAs. define states northwest Washington, Idaho, Oregon.First, want put custom PUMS estimates map. , let’s use tigris package download PUMA boundaries states northwest sf object.Now, use get_pums() get data. select PUMA AGEP, stand PUMA age respectively.Now, use dplyr:group_by() dplyr:summarize() make modified version nw_pums new variables. total_pop represents total population PUMA, pct_Senior represents fraction population PUMA ’s 65 (>64).Working PUMS data can little tricky, get mapping left_join() nw_Senior back nw_pums, mapping \"STATEFP10\" ST, \"PUMACE10\" \"PUMA\". confusing quirk PUMS data need worry ., use geom_sf() make plot. use scale_fill_viridis_b(option = \"magma\") theme_void() customize look map, use labels = scales::label_percent(1)) handy trick convert pct_Senior’s fractions percentages. Add labs() ’re done!","code":"\nnw_states <- c(\"OR\", \"WA\", \"ID\")\nnw_pumas <- map(nw_states, \n                tigris::pumas, \n                class = \"sf\", \n                cb = TRUE) %>%\n    reduce(rbind)\nnw_pums <- get_pums(variables = c(\"PUMA\", \"AGEP\"),\n                    state = nw_states,\n                    recode = TRUE,\n                    survey = \"acs1\",\n                    year = 2018)\nnw_Senior <- nw_pums %>%\n    group_by(ST, PUMA) %>%\n    summarize(total_pop = sum(PWGTP), \n              pct_Senior = sum(PWGTP[AGEP > 64]) / total_pop,\n              .groups = \"drop\")\nnw_final <- nw_pumas %>%\n    left_join(nw_Senior, by = c(\"STATEFP10\" = \"ST\", \"PUMACE10\" = \"PUMA\")) \nnw_final %>% \n  ggplot(aes(fill = pct_Senior)) +\n    geom_sf() +\n    scale_fill_viridis_b(name = NULL,\n        option = \"magma\",\n        labels = scales::label_percent(1)) +\n    labs(title = \"Percentage of population that are Seniors\",\n         caption = \"Source: American Community Survey 2014-2018\") +\n    theme_void()"},{"path":"maps.html","id":"want-to-explore-further","chapter":"Maps","heading":"Want to explore further?","text":"Take look tidycensus website.shapefiles place tidycensus, can read using st_read() sf package, join data using dplyr functions, map geom_sf() shown .\nmay look using coord_sf() trouble displaying data.\nmay look using coord_sf() trouble displaying data.Want add interactivity maps? Check leaflet package. ’s good introduction using leaflet tidycensus.Practice skills Andrew Tran’s case study slides, can replicate graphic Washington Post. Note: involves packages haven’t shown book, follow along step step able see used.Downloading feature geometry Census website. cache shapefiles use future sessions, set options(tigris_use_cache = TRUE).","code":""},{"path":"ipums.html","id":"ipums","chapter":"IPUMS","heading":"IPUMS","text":"IPUMS, Integrated Public Use Microdata Series, great source big data. IPUMS includes Current Population Survey (CPS) U.S. Census well health housing data. IPUMS system allows create subset massive repositories collected data fits needs interests. initial word caution new system requests filled immediately, start early!","code":""},{"path":"ipums.html","id":"getting-started-2","chapter":"IPUMS","heading":"Getting Started","text":"use tidyverse ipumsr packages.Start IPUMS home page select survey wish explore via ‘visit site’ option.see prompt login register top widow:used IPUMS , may proceed login move select data. However, new user, need apply access. register tab ask standard account creation information. (use Harvard college email indicate usage accordingly.) handful data use agreements required agree process note specifically regard citations. applied, need await confirmation account log fully.received email confirmation logged properly, able begin data selection process. first option need consider specify samples (see ‘select samples’ button), time period covered data. want pay close attention intervals step well note whether pull large sample entire available set (relevant census applications). can use drop menus /search feature locate include variables need may relevant query.can see household person tabs (highlighted) provide drop common variables repository respective classifications. add variable ‘cart’ simply select + icon, available either corresponding explanation simply isn’t available.satisfied variables intervals selected sample, can review revise selections cart menu prior requesting ‘pull.’ Shown cart review window shows. X indicates existence data given time period. Note one variable data, indicated ... across time period review columns.satisfied sample specifications, can proceed create data extract page. provide handful final options text window describe sample ’ve created. treat like commit message GitHub - brief meaningful.Submitting request automatically bring request history page associated account. note requests permanently available , make sure promptly download information upon receipt.Upon receiving confirmation email request, return window. need first download data via Download.DAT, need save DDI link (via ‘save ’) location .dat.gz file. select R command file. last step unpack / un-zip .dat.gz file .gz suffix removed. IPUMS download instructions recommend 7-zip don’t already file decompression software hand.R command file link show text file containing roughly following steps unpack data extraction:successful unpacking proper saving DDI file result ability execute code R command file shown . point can access ‘Big Data’ standard object. example extract U.S. census imported background. Another useful feature DDI opening link browser shows information variable abbreviations extract broadly.","code":"\nlibrary(tidyverse)\nlibrary(ipumsr)\nif (!require(\"ipumsr\")) stop(\"Reading IPUMS data into R requires the ipumsr package. It can be installed using the following command: install.packages('ipumsr')\")\n\n# this read_ipums_ddi and read_ipums_micro seem to require BOTH the .xml and\n# .dat files to run... strange behavior, but it seems to be implicit in the latter IPUMSR::* function\n# this is an ongoing point of research\n\nddi <- read_ipums_ddi(\"path_to_your_file.xml\")\ndata <- read_ipums_micro(ddi)## Use of data from IPUMS USA is subject to conditions including that users should\n## cite the data appropriately. Use command `ipums_conditions()` for more details.\nglimpse(census_tbl)## Rows: 12,800,619\n## Columns: 40\n## $ YEAR     <int> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2…\n## $ SAMPLE   <int+lbl> 201601, 201601, 201601, 201601, 201601, 201601, 201601, 2…\n## $ SERIAL   <dbl> 1, 1, 2, 3, 3, 3, 3, 4, 4, 5, 5, 6, 6, 7, 8, 8, 8, 8, 8, 9, 9…\n## $ CBSERIAL <dbl> 64, 64, 80, 107, 107, 107, 107, 134, 134, 180, 180, 300, 300,…\n## $ HHWT     <dbl> 97, 97, 95, 159, 159, 159, 159, 122, 122, 20, 20, 226, 226, 1…\n## $ CLUSTER  <dbl> 2e+12, 2e+12, 2e+12, 2e+12, 2e+12, 2e+12, 2e+12, 2e+12, 2e+12…\n## $ STRATA   <dbl> 70001, 70001, 90001, 30201, 30201, 30201, 30201, 60001, 60001…\n## $ GQ       <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ PERNUM   <dbl> 1, 2, 1, 1, 2, 3, 4, 1, 2, 1, 2, 1, 2, 1, 1, 2, 3, 4, 5, 1, 2…\n## $ PERWT    <dbl> 98, 89, 95, 160, 154, 184, 260, 122, 113, 20, 18, 226, 155, 1…\n## $ FAMSIZE  <int+lbl> 2, 2, 1, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 1, 5, 5, 5, 5, 5, …\n## $ SEX      <int+lbl> 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, …\n## $ AGE      <int+lbl> 84, 84, 78, 46, 52, 20, 16, 66, 68, 64, 63, 25, 23, 35, 2…\n## $ MARST    <int+lbl> 1, 1, 4, 1, 1, 6, 6, 1, 1, 1, 1, 6, 6, 4, 1, 1, 6, 6, 6, …\n## $ MARRNO   <int+lbl> 1, 1, 2, 1, 3, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, …\n## $ RACE     <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, …\n## $ RACED    <int+lbl> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 10…\n## $ HISPAN   <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ HISPAND  <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ CITIZEN  <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ YRIMMIG  <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ YRSUSA1  <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ HCOVANY  <int+lbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n## $ EDUC     <int+lbl> 7, 10, 7, 8, 6, 6, 4, 6, 10, 6, 6, 7, 8, 10, 7, 6, 1, 0, …\n## $ EDUCD    <int+lbl> 71, 101, 71, 81, 65, 63, 40, 63, 101, 65, 65, 71, 81, 101…\n## $ SCHLTYPE <int+lbl> 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 0, 2, …\n## $ EMPSTAT  <int+lbl> 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 0, 0, 0, …\n## $ EMPSTATD <int+lbl> 30, 30, 10, 10, 30, 30, 30, 30, 30, 10, 30, 10, 10, 10, 1…\n## $ OCC      <dbl+lbl>    0,    0, 5700, 1550,    0,    0,    0, 5700,    0, 620…\n## $ OCC2010  <int+lbl> 9920, 9920, 5700, 1550, 9920, 9920, 9920, 5700, 9920, 620…\n## $ IND1990  <int+lbl> 0, 0, 831, 882, 0, 0, 0, 633, 0, 60, 772, 831, 820, 842, …\n## $ INDNAICS <chr> \"0\", \"0\", \"622\", \"5413\", \"0\", \"0\", \"0\", \"443142\", \"0\", \"23\", …\n## $ UHRSWORK <int+lbl> 0, 0, 40, 40, 0, 0, 0, 0, 0, 40, 0, 24, 40, 40, 35, 27, 0…\n## $ INCTOT   <dbl+lbl>   22510,   14100,   45800,   65000,       0,       0,    …\n## $ FTOTINC  <dbl+lbl>  36610,  36610,  45800,  65000,  65000,  65000,  65000,  …\n## $ INCWAGE  <dbl+lbl>      0,      0,  27300,  65000,      0,      0,      0,  …\n## $ INCSS    <dbl+lbl>  8100, 14100, 18500,     0,     0,     0,     0,  3900,  …\n## $ INCWELFR <dbl+lbl>     0,     0,     0,     0,     0,     0,     0,     0,  …\n## $ INCRETIR <dbl+lbl>  10100,      0,      0,      0,      0,      0,      0,  …\n## $ POVERTY  <dbl+lbl> 254, 254, 402, 261, 261, 261, 261, 421, 421, 501, 501, 15…"},{"path":"ipums.html","id":"additional-notes","chapter":"IPUMS","heading":"Additional Notes","text":"data 12,800,619 rows (big data!), may need take steps crash machine sampling data initially working FAS cloud system.data 12,800,619 rows (big data!), may need take steps crash machine sampling data initially working FAS cloud system.may also encounter problems pushing large files GitHub. See large file storage options Git LFS.may also encounter problems pushing large files GitHub. See large file storage options Git LFS.variables ‘haven labeled’ see haven CRAN files information .variables ‘haven labeled’ see haven CRAN files information .","code":""},{"path":"ipums.html","id":"haven-labelled-variables","chapter":"IPUMS","heading":"Haven Labelled Variables","text":"look closely glimpse census data notice columns haven_labelled classification. feature tends useful different software applications Stata, working R need recast variables something useful convenient. first haven variables see SAMPLE, OCC(occupation), IND(industry). thousands unique positions recode, ideally want able automate .create labeled variable, see haven::labelled(), manually recode existing case, employ dplyr::recode(). find labels given OCC IND variables aren’t particularly useful - case tells set occupation/industry terms survey takers given choose rather actual term. IPUMS thankfully provides us key file make integers something useful. Using occupation example case, IPUMS provide access precise occupation string format - though takes bit work. can find conversion ‘crosswalk’ files IPUMS provides via website, (also linked IPUMS crosswalk page) recent crosswalk files provided Census Bureau . files often formatted inconveniently, can different year--year, intentional converting preferred format.said, cases may want keep integer levels variable irrespective labels attached. case, workaround much easier! Simply call as_factor() labeled variable ’re able manipulate data normally.much IPUMS offerings just U.S. population statistics. can see extract M.E.P.S. (Medical Expenditure Panel Survey) International repositories look like. Always bear mind individual access account, can create almost configuration need. names certainly available interpretations different, process import new tibble transform weird variables preferred format largely .International data also available.","code":"\nclass(census_tbl$OCC)[1]\n\n# returns: \"haven_labelled\"\n\nex2 <- census_tbl %>%\n  select(OCC) %>%\n  mutate(occ = as_factor(OCC))\n\nclass(ex2$occ)\n\n# returns: \"factor\"\nddi_meps <- read_ipums_ddi(\"IPUMS/extracts/meps/meps_00003.xml\")\nmeps_tbl <- read_ipums_micro(ddi_meps)## Use of data from IPUMS MEPS is subject to conditions including that users\n## should cite the data appropriately. Use command `ipums_conditions()` for more\n## details.\nglimpse(meps_tbl)## Rows: 752,639\n## Columns: 50\n## $ YEAR       <dbl> 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996,…\n## $ PERNUM     <dbl> 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2,…\n## $ DUID       <dbl> 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 7, 7,…\n## $ PID        <chr> \"018\", \"025\", \"032\", \"049\", \"015\", \"022\", \"018\", \"025\", \"03…\n## $ MEPSID     <chr> \"100002018\", \"100002025\", \"100002032\", \"100002049\", \"100003…\n## $ PANEL      <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ PSUANN     <dbl+lbl> 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 8, 8, 1…\n## $ STRATANN   <dbl+lbl>  12,  12,  12,  12,  74,  74,  90,  90,  90,  90,  90, …\n## $ PSUPLD     <dbl+lbl> 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2…\n## $ STRATAPLD  <dbl+lbl>  23,  23,  23,  23,  43,  43,  44,  44,  44,  44,  44, …\n## $ PANELYR    <int> 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996,…\n## $ RELYR      <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ PERWEIGHT  <dbl> 15024, 14976, 18256, 12598, 6594, 11319, 7674, 4372, 9295, …\n## $ SAQWEIGHT  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ DIABWEIGHT <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ AGE        <dbl+lbl> 31, 31,  7,  3, 74, 73, 54, 48, 27, 18, 16, 80, 39, 34,…\n## $ SEX        <int+lbl> 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1…\n## $ MARSTAT    <int+lbl> 10, 10, 0, 0, 10, 10, 10, 10, 50, 50, 50, 20, 10, 10, 0…\n## $ REGIONMEPS <int+lbl> 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4…\n## $ FAMSIZE    <int+lbl> 4, 4, 4, 4, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 99, 99,…\n## $ RACEA      <int+lbl> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, …\n## $ YRSINUS    <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ INTERVLANG <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ EDUC       <int+lbl> 500, 401, 102, 998, 500, 109, 302, 302, 401, 202, 201, …\n## $ STUDENT    <int+lbl> 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8…\n## $ INCTOT     <dbl+lbl>  24778,  25298,      0,      0,  94546,  35796,  29000,…\n## $ FTOTVAL    <dbl+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ INCWAGE    <dbl+lbl>  24440,  24960,      0,      0,      0,      0,  29000,…\n## $ INCBUS     <dbl+lbl>     0,     0,     0,     0, 56086,     0,     0,     0,…\n## $ INCUNEMP   <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ FOODSTYN   <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ POVLEV     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ FILESTATUS <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ FILETAXFRM <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ HEALTH     <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ USUALPL    <int+lbl> 2, 2, 2, 2, 2, 2, 1, 2, 9, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2…\n## $ HINOTCOV   <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n## $ HIPRIVATE  <int+lbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2…\n## $ HIMCARE    <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1…\n## $ CANCEREV   <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ SMOKENOW   <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ EXPTOT     <dbl> 507, 124, 264, 7410, 2942, 3571, 290, 1238, 94, 55, 0, 7090…\n## $ CHGTOT     <dbl> 555, 75, 252, 7927, 4712, 2890, 290, 1279, 97, 113, 0, 8426…\n## $ EREXPTOT   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ ERFCHGT    <int> 0, 0, 142, 0, 0, 0, 0, 463, 97, 0, 0, 299, 1700, 0, 0, 0, 0…\n## $ HPTOTDIS   <int> 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n## $ HPTOTNIGHT <int+lbl> 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0…\n## $ RXPRMEDSNO <int> 0, 2, 2, 10, 19, 44, 0, 5, 0, 0, 0, 6, 60, 11, 1, 0, 6, 0, …\n## $ RXEXPTOT   <int> 0, 49, 12, 217, 595, 1783, 0, 91, 0, 0, 0, 176, 995, 259, 6…\n## $ VSEXPTOT   <int> 0, 0, 0, 0, 0, 0, 290, 0, 0, 0, 0, 251, 0, 0, 0, 0, 305, 29…\nddi_intntl <- read_ipums_ddi(\"IPUMS/extracts/international/ipumsi_00001.xml\")\nint_tbl <- read_ipums_micro(ddi_intntl)## Use of data from IPUMS-International is subject to conditions including that\n## users should cite the data appropriately. Use command `ipums_conditions()` for\n## more details.\nglimpse(int_tbl)## Rows: 12,193,602\n## Columns: 30\n## $ COUNTRY    <int+lbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n## $ YEAR       <int> 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981, 1981,…\n## $ SAMPLE     <int+lbl> 40198101, 40198101, 40198101, 40198101, 40198101, 40198…\n## $ SERIAL     <dbl> 1000, 2000, 2000, 3000, 3000, 4000, 5000, 5000, 5000, 6000,…\n## $ HHWT       <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n## $ URBAN      <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ REGIONW    <int+lbl> 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44,…\n## $ BUILTYR    <int+lbl> 1980, 1944, 1944, 1980, 1980, 1918, 1960, 1960, 1960, 1…\n## $ HHTYPE     <int+lbl> 1, 2, 2, 2, 2, 1, 3, 3, 3, 2, 2, 3, 3, 3, 3, 6, 6, 6, 6…\n## $ NFAMS      <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ NCOUPLES   <int+lbl> 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ PERNUM     <dbl> 1, 1, 2, 1, 2, 1, 1, 2, 3, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1,…\n## $ PERWT      <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n## $ MARST      <int+lbl> 4, 2, 2, 2, 2, 4, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1…\n## $ MARSTD     <int+lbl> 400, 210, 210, 210, 210, 400, 210, 210, 100, 210, 210, …\n## $ BPLCOUNTRY <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ CITIZEN    <int+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ YRIMM      <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ RELIGION   <int+lbl> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6…\n## $ RELIGIOND  <int+lbl> 6001, 6001, 6001, 6001, 6001, 6001, 6001, 6106, 6106, 6…\n## $ RACE       <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ SCHOOL     <int+lbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1…\n## $ LIT        <int+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n## $ EDATTAIN   <int+lbl> 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 3, 3, 0, 0, 3, 2, 3, 0…\n## $ EDATTAIND  <int+lbl> 221, 221, 221, 321, 221, 221, 321, 221, 321, 321, 221, …\n## $ EEDATTAIN  <int+lbl> 30, 30, 30, 40, 30, 30, 40, 30, 40, 40, 30, 40, 40, 0, …\n## $ EMPSTAT    <int+lbl> 3, 3, 3, 1, 3, 3, 2, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3…\n## $ EMPSTATD   <int+lbl> 343, 343, 310, 100, 343, 343, 200, 100, 100, 343, 310, …\n## $ LABFORCE   <int+lbl> 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 9, 9, 1, 1, 2, 9…\n## $ INCTOT     <dbl+lbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…"},{"path":"ipums.html","id":"what-can-you-do-with-all-of-this","chapter":"IPUMS","heading":"What can you do with all of this?","text":"entirely ! Maybe idea health care system policy change affects people different areas. Maybe want know different demographic segments country changed fared time. Maybe want know educational achievement changed last 15 years. can answered, least approximated data can access free.examples:","code":""},{"path":"animation.html","id":"animation","chapter":"Animation","heading":"Animation","text":"gganimate package creating animated ggplots. provides range new functionality can added plot object order customize change time.Key features gganimate:transitions: want data changeviews: want viewpoint changeshadows: want animation memoryMany thanks Alboukadel Kassambara allowing us use tutorial section.","code":""},{"path":"animation.html","id":"set-up-1","chapter":"Animation","heading":"Set Up","text":"Load required packages set default ggplot2 theme theme_bw():","code":"\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(gganimate)\ntheme_set(theme_bw())\nhead(gapminder)## # A tibble: 6 x 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786."},{"path":"animation.html","id":"transition-through-distinct-states-in-time","chapter":"Animation","heading":"Transition through distinct states in time","text":"Begin static plot:","code":"\np <- ggplot(gapminder,\n            aes(x = gdpPercap, y=lifeExp, size = pop, colour = country)) +\n      geom_point(show.legend = FALSE, alpha = 0.7) +\n      scale_color_viridis_d() +\n      scale_size(range = c(2, 12)) +\n      scale_x_log10() +\n      labs(x = \"GDP per capita\", y = \"Life expectancy\")\np"},{"path":"animation.html","id":"basics","chapter":"Animation","heading":"Basics","text":"Key R function: transition_time(). transition length states set correspond actual time difference .Label variables: frame_time. Gives time current frame corresponds .","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\")"},{"path":"animation.html","id":"create-facets-by-continent","chapter":"Animation","heading":"11.12.4 Create facets by continent","text":"","code":"\np + facet_wrap(~continent) +\n  transition_time(year) +\n  labs(title = \"Year: {frame_time}\")"},{"path":"animation.html","id":"let-the-view-follow-the-data-in-each-frame","chapter":"Animation","heading":"11.12.5 Let the view follow the data in each frame","text":"","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  view_follow(fixed_y = TRUE)"},{"path":"animation.html","id":"show-preceding-frames-with-gradual-falloff","chapter":"Animation","heading":"11.12.6 Show preceding frames with gradual falloff","text":"shadow meant draw small wake data showing latest frames current. can choose gradually diminish size /opacity shadow. length wake given absolute frames make animation susceptible changes framerate. Instead given proportion total length animation.","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  shadow_wake(wake_length = 0.1, alpha = FALSE)"},{"path":"animation.html","id":"show-the-original-data-as-background-marks","chapter":"Animation","heading":"11.12.7 Show the original data as background marks","text":"shadow lets show raw data behind current frame. past /future raw data can shown styled want.","code":"\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  shadow_mark(alpha = 0.3, size = 0.5)"},{"path":"animation.html","id":"reveal-data-along-a-given-dimension","chapter":"Animation","heading":"Reveal data along a given dimension","text":"transition allows let data gradually appear, based given time dimension. Start static plot:","code":"\np <- ggplot(airquality,\n            aes(Day, Temp, group = Month, color = factor(Month))) +\n      geom_line() +\n      scale_color_viridis_d() +\n      labs(x = \"Day of Month\", y = \"Temperature\") +\n      theme(legend.position = \"top\")\np"},{"path":"animation.html","id":"let-data-gradually-appear","chapter":"Animation","heading":"11.12.8 Let data gradually appear","text":"Reveal day (x-axis)Show points:Points can kept giving unique group:","code":"\np + transition_reveal(Day)\np + \n  geom_point() +\n  transition_reveal(Day)\np + \n  geom_point(aes(group = seq_along(Day))) +\n  transition_reveal(Day)"},{"path":"animation.html","id":"transition-between-several-distinct-stages-of-the-data","chapter":"Animation","heading":"Transition between several distinct stages of the data","text":"Data preparation:Create bar plot mean temperature:transition_states():enter_grow() + enter_fade()","code":"\nmean.temp <- airquality %>%\n  group_by(Month) %>%\n  summarise(Temp = mean(Temp), .groups = \"drop_last\")\nmean.temp## # A tibble: 5 x 2\n##   Month  Temp\n##   <int> <dbl>\n## 1     5  65.5\n## 2     6  79.1\n## 3     7  83.9\n## 4     8  84.0\n## 5     9  76.9\np <- ggplot(mean.temp, \n            aes(Month, Temp, fill = Temp)) +\n      geom_col() +\n      scale_fill_distiller(palette = \"Reds\", direction = 1) +\n      theme_minimal() +\n      theme(panel.grid = element_blank(),\n            panel.grid.major.y = element_line(color = \"white\"),\n            panel.ontop = TRUE)\np\np + transition_states(Month, wrap = FALSE) +\n  shadow_mark()\np + transition_states(Month, wrap = FALSE) +\n  shadow_mark() +\n  enter_grow() +\n  enter_fade()"},{"path":"animation.html","id":"save-your-animation","chapter":"Animation","heading":"Save your animation","text":"code create animations can take long time run. created animation, ’ll want save somewhere can display without run code.key function use anim_save(), similar saving static plots using ggsave(). save animation gif. first argument filename want give animation second animation object, animation object called p wanted save file called “p.gif,” save like :don’t supply second argument, anim_save() default saving recent animation rendered. anim_save(\"animation.gif\") save recent animation “animation.gif.”don’t want save gif current directory, can specify directory using path argument. Let’s say subdirectory working directory called “gifs.” can thus save “animation.gif” “gifs” anim_save(\"animation.gif\", path = \"gifs\").created gif, can post online. can post Facebook selecting “Photo/Video” Facebook status Twitter clicking photo icon.","code":"\nanim_save(\"p.gif\", p)"},{"path":"set-up-for-working-on-the-primer.html","id":"set-up-for-working-on-the-primer","chapter":"Set Up for Working on The Primer","heading":"Set Up for Working on The Primer","text":"document provides guide setting R/RStudio work Primer, book , PPDBS/primer, associated tutorial data packages: PPDBS/primer.tutorials PPDBS/primer.data. three steps:first part ensures knowledge computer settings successful. luck, need .first part ensures knowledge computer settings successful. luck, need .second part involves making connection true repos PPBDS Github organization Github account computer. may end dozens times since, whenever something gets messed , easiest solution often just nuke orbit start . , weeks, won’t .second part involves making connection true repos PPBDS Github organization Github account computer. may end dozens times since, whenever something gets messed , easiest solution often just nuke orbit start . , weeks, won’t .third part involves daily workflow merges pull requests. steps many times day.third part involves daily workflow merges pull requests. steps many times day.","code":""},{"path":"set-up-for-working-on-the-primer.html","id":"computer-set-up","chapter":"Set Up for Working on The Primer","heading":"11.13 Computer Set Up","text":"Install latest released versions R RStudio. Install usethis package.Read Getting Started Tools sections Primer make sure Git/Github working. Read (watch videos ) Getting Used R, RStudio, R Markdown Chester Ismay Patrick C. Kennedy. Check RStudio Essentials Videos. relevant us “Writing code RStudio”, “Projects RStudio” “Github RStudio”. best reference R/RStudio/Git/Github issues always Happy Git GitHub useR.Make sure Git/Github connections good. gone key chapters Happy Git R — — may already OK. (, even ), need run usethis::git_sitrep().left end output.first part — Git config — seems messed , execute:second part seems messed , try:read Github credentials. , restart R run git_sitrep() make sure things look like mine, less.Install renv package. can read renv package .critical understand details renv works. big picture creates set libraries used just project whose versions kept sync .point, tools need contribute. never done pull request, however, need learn . Start reading help page. Read whole thing! Don’t just skim . important concepts professional-level workflow. usethis package mostly providing wrappers around underlying git commands. want understand happening lower level, read , optional., luck, steps .Prove () set working submittimg pull request simply adds name top one TODO.txt files. (See .)","code":"> library(usethis)   \n> git_sitrep()    \nGit config (global)   \n● Name: 'David Kane'   \n● Email: 'dave.kane@gmail.com'   \n● Vaccinated: FALSE   \nℹ See `?git_vaccinate` to learn more   \nℹ Defaulting to https Git protocol   \n● Default Git protocol: 'https'   \nGitHub   \n● Default GitHub host: 'https://github.com'   \n● Personal access token for 'https://github.com': '<discovered>'   \n● GitHub user: 'davidkane9'   \n● Token scopes: 'delete_repo, gist, notifications, repo, user, workflow'   \n● Email(s): 'dave.kane@gmail.com (primary)', 'dkane@fas.harvard.edu'   \n...   \nuse_git_config(user.name = \"David Kane\", user.email = \"dave.kane@gmail.com\")\nusethis::create_github_token()"},{"path":"set-up-for-working-on-the-primer.html","id":"project-set-up","chapter":"Set Up for Working on The Primer","heading":"11.14 Project Set Up","text":"need steps least one time. likely, however, dozens times. things working, great! start working, can try diagnose problem. , can’t, nuke orbit scenario, means start deleting current version package two places: computer (just put R Studio project directory Trash) Github account (just delete repo going Settings going bottom page).Key steps:Fork/download target repo:repo working book. working PPBDS/primer.data PPBDS/primer.tutorials, need create_from_github() using repos. must change destdir location computer. Indeed, professionals generally several different RStudio sessions open, working different R project/package, connected Github repo.education, worth reading help page create_from_github(). fork protocal arguments may really necessary , obviously, place project location computer projects live. command first forks copy PPBDS/primer Github account clone/downloads fork computer.may seem like overkill, , Pro Git explains, (essentially) large projects organized. luck, issue command . , always connected, fork true repos, live github/com/PPBDS. Also, note , something ever gets totally messed computer, can just delete project folder computer repo Github account start . (made changes don’t want lose, just save files changes one side move back recreated project.)Note command automatically put new RStudio session primer (primer.tutorials primer.data) RStudio project resides computerThe next step get renv setup running package versions everyone else. Run :install packages need directory project. (effect main library R packages.) Restart R session. , means now two separate installations , example, ggplot2. One default place R sessions default pointed . (different project without renv directory, can run .libPaths() see .) second place ggplot2 installed renv directory lives project.Note , part, won’t anything renv initial use. use error = TRUE code chunk, also need renv.ignore = TRUE code chunk, get annoying warning renv can’t parse code chunk.However, three renv commands might issue:renv::status() just reports anything messed . won’t hurt anything.renv::restore() looks renv.lock file installs packages specifies. need make change renv.lock, e.g., upgrade version ggplot2 add new package.renv::snapshot() issued know . changes renv.lock file, something , usually, . common case use need add new package project.Create branch work :Make sure branch name sensible. , command need issue , least current workflow. always “” branch, never default (master) branch. can check upper right corner git panel R Studio.professional settings, often work several different branches . , comfortable, feel free create one branch, use , delete . Never work default branch, however. , use multiple branches, careful .","code":"\nlibrary(usethis)  \ncreate_from_github(\"PPBDS/primer\",   \n                    fork = TRUE,   \n                    destdir = \"/Users/davidkane/Desktop/projects/\",   \n                    protocol = \"https\")  \nlibrary(renv)\nrenv::restore()\npr_init(branch = \"chapter-9\")"},{"path":"set-up-for-working-on-the-primer.html","id":"daily-work","chapter":"Set Up for Working on The Primer","heading":"11.15 Daily Work","text":"Pull regularly:Issue command time. make sure repo computer updated latest changes made book. word “upstream” associated repos PPBDS. word “origin” associated fork Github account. , general, don’t need worry . Just pull every time sit . (Just clicking pull button enough. pulls repo, changes made. pull PPBDS/primer, et al.) issue command multiple times day.Make changes file editing. Knit make sure changes work. Commit message. Push repo Github account. .point, ready push PPBDS organization. However, can’t directly. Instead, must submit pull request (PR). part larger project, commands slightly different done , usually just clicking pull (blue) push (green) arrows Git pane RStudio.Issue pull requests every days, depending much work done /whether people waiting something done.command bundles bunch git commands (hand) one handy step. command everything needed create “pull request” — request accept changes proposing repo PPBDS/primer — opens web page show . done! must PRESS green button web page, sometimes twice. , PR actually created. pr_push() just everything . “pr” pr_push() stands pull request.leave aside now issues associated back--forth discussions might around pull request. probably just accept . changes go repos PPBDS distributed everyone else run pr_merge_main().leave aside now issues associated back--forth discussions might around pull request. probably just accept . changes go repos PPBDS distributed everyone else run pr_merge_main().can now continue . need wait deal pull request. need fork/clone/download . don’t need create new branch, although many people , branch name describes working now. just keep editing files, knitting, committing. feel completed another chunk work, just run pr_push() .can now continue . need wait deal pull request. need fork/clone/download . don’t need create new branch, although many people , branch name describes working now. just keep editing files, knitting, committing. feel completed another chunk work, just run pr_push() .Read usethis setup help page least , perhaps week two working within framework. lots good stuff!Read usethis setup help page least , perhaps week two working within framework. lots good stuff!","code":"\npr_merge_main()\npr_push()"},{"path":"set-up-for-working-on-the-primer.html","id":"common-problems","chapter":"Set Up for Working on The Primer","heading":"11.16 Common Problems","text":"immediate aftermath creation process, blue/green arrows (Git panel) pulling/pushing may grayed . sign connection computer forked repo “settled .” (sure cause even right terminology.) think just issuing first pr_merge_main() fixes . , always goes away. , however, can’t pull/push repo. doesn’t really matter, however, since key commands need pr_merge_main() pr_push(), always work immediately.immediate aftermath creation process, blue/green arrows (Git panel) pulling/pushing may grayed . sign connection computer forked repo “settled .” (sure cause even right terminology.) think just issuing first pr_merge_main() fixes . , always goes away. , however, can’t pull/push repo. doesn’t really matter, however, since key commands need pr_merge_main() pr_push(), always work immediately.running pr_merge_main(), often see bunch files Git tab top right corner Rstudio marked M (Modified), including files know edit. files updated “truth” — PPBDS/primer — since last pr_merge_main(). Since pulled directly PPBDS/primer repo, forked repo sees changes people made thinks made . easily fixed, however — just commit changes forked repo. (Strangely, seems always happen. don’t see effect, don’t worry.)running pr_merge_main(), often see bunch files Git tab top right corner Rstudio marked M (Modified), including files know edit. files updated “truth” — PPBDS/primer — since last pr_merge_main(). Since pulled directly PPBDS/primer repo, forked repo sees changes people made thinks made . easily fixed, however — just commit changes forked repo. (Strangely, seems always happen. don’t see effect, don’t worry.)Always run pr_merge_main() committing file. Otherwise, may create lots merge conflicts. happens, save copy file(s) personally editing side. , nuke orbit, following instructions . Repeat Project Set process. move file(s) hand new repo, commit/push normal.Always run pr_merge_main() committing file. Otherwise, may create lots merge conflicts. happens, save copy file(s) personally editing side. , nuke orbit, following instructions . Repeat Project Set process. move file(s) hand new repo, commit/push normal.submit pull request merge work PPBDS repo, won’t always smiles sunshine — every , ’ll run merge conflicts. arise, two parties work file separately submit conflicting changes. makes hard GitHub “merge” version version. happens, find multiple adjacent “>,” “<,” “=” signs document — show conflicts occur. background merge conflicts, read .submit pull request merge work PPBDS repo, won’t always smiles sunshine — every , ’ll run merge conflicts. arise, two parties work file separately submit conflicting changes. makes hard GitHub “merge” version version. happens, find multiple adjacent “>,” “<,” “=” signs document — show conflicts occur. background merge conflicts, read .see -mentioned conflicts document, submit pull request. mess things . Instead, first, go document, make sure weird conflict indicators (<, >, =) removed. Second, decide goes space. might stuff wrote. might stuff. might combination two decide . Whatever happens, making affirmative choice appear file location. merge conflicts fixed, run pr_push() .pr_push() can tricky. First, note , accepted (p)ull (r)equest submitted, PR still open. can see Github. fact, can see closed/completed pull requests well. , one PR still open, submit another pr_push(), just added current PR. OK! don’t need separate.even open PR, pr_push() can tricky. key thing remember must press green button Github new PR created. Normally, easy. Running pr_push() automatically (perhaps run pr_view()) puts browser brings correct Github page. Press button – presto! – created PR. , sometimes, web page different. actually sends back old pull request. happens, need click “Pull Request” tab . take new page, green button labeled “Compare & Pull Request.” Press button.end needing install new package — rare — just install type renv::status() confirm renv aware change. , type renv::snapshot(). update renv.lock file include new package. just commit/push new version renv.lock, shares information everyone else project. Never commit/push modified renv.lock unless know changed.end needing install new package — rare — just install type renv::status() confirm renv aware change. , type renv::snapshot(). update renv.lock file include new package. just commit/push new version renv.lock, shares information everyone else project. Never commit/push modified renv.lock unless know changed.careful committing garbage files like “.DS_Store,” file created sometimes. commit changes understand. vast majority cases PRs involve one two files.careful committing garbage files like “.DS_Store,” file created sometimes. commit changes understand. vast majority cases PRs involve one two files.","code":""},{"path":"set-up-for-working-on-the-primer.html","id":"style-guide-1","chapter":"Set Up for Working on The Primer","heading":"11.17 Style Guide","text":"Never use just single # using chapter title. first subpart uses ##. 5 8 subparts chapter. Within subpart, may sub-subparts, indicated ###. 3 10 . may use #### like.Never use just single # using chapter title. first subpart uses ##. 5 8 subparts chapter. Within subpart, may sub-subparts, indicated ###. 3 10 . may use #### like.Section headings (Chapter titles) sentence case (first word capitalized, unless something always capitalized) rather title case (words except small words like “” “” capitalized). Chapter titles title case. Headings end period.Section headings (Chapter titles) sentence case (first word capitalized, unless something always capitalized) rather title case (words except small words like “” “” capitalized). Chapter titles title case. Headings end period.Never hard code stuff like “tibble 336,776 rows 19 columns.” happens update data? Instead, calculate numbers fly, “r scales::comma(x)” whenever x number thousands greater. Example: “tibble ‘r scales::comma(nrow(x))’ rows ‘r ncol(x)’ columns.”Never hard code stuff like “tibble 336,776 rows 19 columns.” happens update data? Instead, calculate numbers fly, “r scales::comma(x)” whenever x number thousands greater. Example: “tibble ‘r scales::comma(nrow(x))’ rows ‘r ncol(x)’ columns.”“” writing book.“” writing book.Package names bold: ggplot2 package graphics. general, reserve bolding package names. Use italics emphasis contexts.Package names bold: ggplot2 package graphics. general, reserve bolding package names. Use italics emphasis contexts.R code, anything might type console, always within backticks. Example: mtcars built-dataset.R code, anything might type console, always within backticks. Example: mtcars built-dataset.Function names always include parentheses: write pivot_wider(), pivot_wider.Function names always include parentheses: write pivot_wider(), pivot_wider.Add lots memes videos cartoons.Add lots memes videos cartoons.use code chunk names messes building book limits bookdown.use code chunk names messes building book limits bookdown.Make ample use comments, placed handy CMD-Shift-/ shortcut. notes everyone else working chapter, future .Make ample use comments, placed handy CMD-Shift-/ shortcut. notes everyone else working chapter, future .tables created gt package.tables created gt package.images gifs loaded knitr::include_graphics().images gifs loaded knitr::include_graphics().code chunk options allowed include = FALSE, echo = FALSE, fig.cap = “cap” message = FALSE loading packages like ggplot2 since prevents messages printing .code chunk options allowed include = FALSE, echo = FALSE, fig.cap = “cap” message = FALSE loading packages like ggplot2 since prevents messages printing .Interim data sets called x something sensible situation, like ch7 data set working Chapter 7. use names like data df, R commands.Interim data sets called x something sensible situation, like ch7 data set working Chapter 7. use names like data df, R commands.Students sometimes tentative. Don’t ! Edit aggressively. don’t like , delete . (disagree decision, can always get text back Github.) Move things around. Make chapter , keeping style chapters. Note 90% prose written . Cut anything don’t like.Students sometimes tentative. Don’t ! Edit aggressively. don’t like , delete . (disagree decision, can always get text back Github.) Move things around. Make chapter , keeping style chapters. Note 90% prose written . Cut anything don’t like.make mp4, can convert .gif using https://convertio.co/mp4-gif.make mp4, can convert .gif using https://convertio.co/mp4-gif.Everything Bayesian. confidence interval regression means 95% chance true value lies within interval. Use Rubin Causal Model potential outcomes define precisely “true value” talking . .Everything Bayesian. confidence interval regression means 95% chance true value lies within interval. Use Rubin Causal Model potential outcomes define precisely “true value” talking . .","code":""},{"path":"set-up-for-working-on-the-primer.html","id":"stray-thoughts","chapter":"Set Up for Working on The Primer","heading":"11.17.1 Stray thoughts","text":"Every chapter 5+ begins problem, decision must make. often toy, highly stylized problems. decisions realistic. , structure, problems parallel real problems people face, actual decisions must make.problem specified end “preamble,” untitled part chapter title first subpart. Example Chapter 8:person arrives Boston commuter station. thing know political party. old ? Two people arrive: Democrat Republican. odds Democrat 10% older Republican?different person arrives station. know nothing . attitude toward immigration exposed Spanish-speakers platform? ? certain ?actual problem someone might face? ! like problems. first requires creation predictive model. second necessitates causal model. rest chapter teaches reader create models. end chapter harkens back questions beginning.Might nice put meat story ? Perhaps. ideal world, “decision” faced complex just playing prediction game. Begin decision. real world problem trying solve? costs benefits different approaches? unknown thing trying estimate? Sampling, might : many people call? estimating one parameter — like vote share ballots come — might : much bet election outcome?data might directly connected problem. example, might running Senate campaign trying decide spend money . Spanish-speakers---train-platform data set directly related problem, isn’t unrelated. Indeed, first theme “validity” directly related issue: data relevant problem want solve?","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
