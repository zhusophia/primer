<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Two Parameters | Preceptor’s Primer for Bayesian Data Science</title>
<meta name="author" content="David Kane">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.5.3/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script><script src="libs/plotly-binding-4.9.2.1/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet">
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script><script src="libs/rglWebGL-binding-0.103.5/rglWebGL.js"></script><link href="libs/rglwidgetClass-0.103.5/rgl.css" rel="stylesheet">
<script src="libs/rglwidgetClass-0.103.5/rglClass.min.js"></script><script src="libs/CanvasMatrix4-0.103.5/CanvasMatrix.min.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Preceptor’s Primer for Bayesian Data Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="preamble.html">Preamble</a></li>
<li><a class="" href="shopping-week.html">Shopping Week</a></li>
<li><a class="" href="visualization.html"><span class="header-section-number">1</span> Visualization</a></li>
<li><a class="" href="wrangling.html"><span class="header-section-number">2</span> Wrangling</a></li>
<li><a class="" href="functions.html"><span class="header-section-number">3</span> Functions</a></li>
<li><a class="" href="rubin-causal-model.html"><span class="header-section-number">4</span> Rubin Causal Model</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">5</span> Probability</a></li>
<li><a class="" href="one-parameter.html"><span class="header-section-number">6</span> One Parameter</a></li>
<li><a class="active" href="two-parameters.html"><span class="header-section-number">7</span> Two Parameters</a></li>
<li><a class="" href="three-parameters.html"><span class="header-section-number">8</span> Three Parameters</a></li>
<li><a class="" href="four-parameters.html"><span class="header-section-number">9</span> Four Parameters</a></li>
<li><a class="" href="five-parameters.html"><span class="header-section-number">10</span> Five Parameters</a></li>
<li><a class="" href="n-parameters.html"><span class="header-section-number">11</span> N Parameters</a></li>
<li><a class="" href="case-studies.html"><span class="header-section-number">12</span> Case Studies</a></li>
<li><a class="" href="tools.html">Tools</a></li>
<li><a class="" href="shiny.html">Shiny</a></li>
<li><a class="" href="maps.html">Maps</a></li>
<li><a class="" href="ipums.html">IPUMS</a></li>
<li><a class="" href="animation.html">Animation</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/PPBDS/primer">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="two-parameters" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Two Parameters<a class="anchor" aria-label="anchor" href="#two-parameters"><i class="fas fa-link"></i></a>
</h1>
<!-- Get axis labels and titles to use \sigma and \mu -->
<!-- Show posterior_interval() -->
<p>In Chapter <a href="one-parameter.html#one-parameter">6</a>, we learned how to do inference. We created a joint distribution of the models under consideration and the data which might be observed. Once we observed the data, we went from the joint distribution to the conditional distribution of possible models given the data which we did, in fact, observe. That conditional distribution, suitably normalized, was our posterior probability distribution over the space of possible models. With that distribution, we can answer any question we might (reasonably) ask.</p>
<!-- DK: Need a gif here. -->
<p><em>But what a pain in the ass that whole process was!</em> Do professionals actually go through all those steps every time they work on a data science problem? No! Don’t be absurd. Instead, professionals use standard tools which, in an automated fashion, take care of those steps, taking us directly from assumptions and data to the posterior:</p>
<p><span class="math display">\[\text{Prob}(\text{models} | \text{data} = \text{data we observed})\]</span></p>
<p>Even then, however, the relative likelihood of different models is not that important. Models are invisible, mental entities with no more physical presence than unicorns or leprechauns. In the world itself, we make and test predictions. How many red beads will come from the urn the next time we use our shovel? People with better models make better predictions. That is what matters.</p>
<p>In addition to a change in tools, there are two more differences in this chapter. First, Chapter <a href="one-parameter.html#one-parameter">6</a> used models with just one parameter: the number of red beads, which we can also transform into a parameter, <span class="math inline">\(p\)</span>, the number of red beads divided by 1,000. The model in Chapter <a href="one-parameter.html#one-parameter">6</a> was binomial, and there is only one unknown parameter <span class="math inline">\(p\)</span> in such models. In this chapter, we have two unknown parameters: the mean <span class="math inline">\(\mu\)</span> height in the US and the standard deviation, <span class="math inline">\(\sigma\)</span>, of the normally distributed error term.</p>
<p>Second, Chapter <a href="one-parameter.html#one-parameter">6</a> dealt with a limited set of specific models: 1,001 of them, to be precise. The procedure used was just what we saw in Chapter <a href="probability.html#probability">5</a>. In this chapter, on the other hand, we have continuous parameters.</p>
<p>The purpose of this chapter is to demonstrate, slowly and thoroughly, how professionals do data science.</p>
<div class="inline-figure"><img src="07-two-parameters/images/learn_the%20ways.gif" width="100%"></div>
<p>Data science is ultimately a moral act, so we will use the four <a href="https://en.wikipedia.org/wiki/Cardinal_virtues">Cardinal Virtues</a> — Wisdom, Justice, Courage and Temperance — to organize our approach.</p>
<div id="wisdom-1" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Wisdom<a class="anchor" aria-label="anchor" href="#wisdom-1"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="other/images/Wisdom.jpg" width="100%"></div>
<p>What decision do we face? The reason for making models is not, primarily that making models is fun, although it is! The reason is that we face a decision. We must decide between X or Y. We must choose from A, B and C. We must set D to a specific numeric value. Confronted by a decision, we should make a model of the world to help us.</p>
<p>In any textbook, it will be tough to avoid the “toy problem” trap. The real world is complex. Any substantive decision problem includes a great deal of complexity and requires even more context. We do not have the time to get into that level of detail now, although Chapter <a href="case-studies.html#case-studies">12</a> provides some useful case studies. So, we simplify. We are going to create a model of height for adult men. We will then use that model to answer four questions:</p>
<ul>
<li><p>What is the average height of men?</p></li>
<li><p>What is the probability that the next man we meet will be taller than 180 centimeters?</p></li>
<li><p>What is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?</p></li>
<li><p>What is our posterior probability distribution for the height of the 3rd tallest man out of the next 100 we meet?</p></li>
</ul>
<!-- DK: This is awkward. Why not just ppd everywhere? --><p>The first and fourth questions require a full scale posterior probability distribution as an answer. The middle two questions have a single number as their answer, but we first create a posterior probability distribution in order to derive that number.</p>
<p>Before starting that process, however, we need to look at the data we have.</p>
<div id="eda-for-nhanes" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">7.1.1</span> EDA for <code>nhanes</code><a class="anchor" aria-label="anchor" href="#eda-for-nhanes"><i class="fas fa-link"></i></a>
</h3>
<!-- DK: This EDA is not very good and does not build naturally to where we want to be. -->
<p>Let’s look at the <code>nhanes</code> dataset from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention and covering children and adults in the United States.</p>
<div class="sourceCode" id="cb627"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">primer.data</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/skimr">skimr</a></span><span class="op">)</span>
<span class="fu">glimpse</span><span class="op">(</span><span class="va">nhanes</span><span class="op">)</span></code></pre></div>
<pre><code>## Rows: 10,000
## Columns: 15
## $ survey         &lt;int&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, …
## $ gender         &lt;chr&gt; "Male", "Male", "Male", "Male", "Female", "Male", "Mal…
## $ age            &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 5…
## $ race           &lt;chr&gt; "White", "White", "White", "Other", "White", "White", …
## $ education      &lt;fct&gt; High School, High School, High School, NA, Some Colleg…
## $ hh_income      &lt;fct&gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, 35…
## $ weight         &lt;dbl&gt; 87, 87, 87, 17, 87, 30, 35, 76, 76, 76, 68, 78, 75, 39…
## $ height         &lt;dbl&gt; 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170,…
## $ bmi            &lt;dbl&gt; 32, 32, 32, 15, 31, 17, 21, 27, 27, 27, 24, 24, 26, 19…
## $ pulse          &lt;int&gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80…
## $ diabetes       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ general_health &lt;int&gt; 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, N…
## $ depressed      &lt;fct&gt; Several, Several, Several, NA, Several, NA, NA, None, …
## $ pregnancies    &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA…
## $ sleep          &lt;int&gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA…</code></pre>
<p><code>nhanes</code> include 15 variables, including physical attributes like weight and height. Let’s restrict our attention to a subset, focusing on age, gender and height.</p>
<div class="sourceCode" id="cb629"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nhanes</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">age</span>, <span class="va">gender</span>, <span class="va">height</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 10,000 x 3
##      age gender height
##    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;
##  1    34 Male     165.
##  2    34 Male     165.
##  3    34 Male     165.
##  4     4 Male     105.
##  5    49 Female   168.
##  6     9 Male     133.
##  7     8 Male     131.
##  8    45 Female   167.
##  9    45 Female   167.
## 10    45 Female   167.
## # … with 9,990 more rows</code></pre>
<p>Look at a random sample of our data:</p>
<div class="sourceCode" id="cb631"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nhanes</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">age</span>, <span class="va">gender</span>, <span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">sample_n</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 3
##     age gender height
##   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;
## 1    56 Female   155.
## 2    33 Female   167.
## 3    80 Female   158.
## 4    29 Female   161.
## 5    40 Male     171.</code></pre>
<p>Notice how there is a decimal in the <code>height</code> column of <code>ch7</code>. This is because <code>height</code> is a <code>&lt;dbl&gt;</code> and not an <code>&lt;int&gt;</code>.</p>
<p>Let’s also run <code>glimpse()</code> on our new data.</p>
<div class="sourceCode" id="cb633"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nhanes</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">age</span>, <span class="va">gender</span>, <span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">glimpse</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## Rows: 10,000
## Columns: 3
## $ age    &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9…
## $ gender &lt;chr&gt; "Male", "Male", "Male", "Male", "Female", "Male", "Male", "Fem…
## $ height &lt;dbl&gt; 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170, 182, 16…</code></pre>
<p>Be on the lookout for anything suspicious. Are there any NA’s in your data set? What types of data are the columns, i.e. why is <code>age</code> characterized as integer instead of double? Are there more females than males?</p>
<p><strong>You can never look at your data too closely.</strong></p>
<p>In addition to <code>glimpse()</code>, we can run <code><a href="https://docs.ropensci.org/skimr/reference/skim.html">skim()</a></code>, from the <strong>skimr</strong> package, to calculate some summary statistics.</p>
<div class="sourceCode" id="cb635"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nhanes</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">age</span>, <span class="va">gender</span>, <span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://docs.ropensci.org/skimr/reference/skim.html">skim</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:unnamed-chunk-495">TABLE 7.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">10000</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table></div>
<p><strong>Variable type: character</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">gender</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr></tbody>
</table></div>
<p><strong>Variable type: numeric</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">37</td>
<td align="right">22</td>
<td align="right">0</td>
<td align="right">17</td>
<td align="right">36</td>
<td align="right">54</td>
<td align="right">80</td>
<td align="left">▇▇▇▆▅</td>
</tr>
<tr class="even">
<td align="left">height</td>
<td align="right">353</td>
<td align="right">0.96</td>
<td align="right">162</td>
<td align="right">20</td>
<td align="right">84</td>
<td align="right">157</td>
<td align="right">166</td>
<td align="right">174</td>
<td align="right">200</td>
<td align="left">▁▁▁▇▂</td>
</tr>
</tbody>
</table></div>
<p>Interesting! There are 353 missing values of height in our subset of data. Just using <code>glimpse()</code> does not show us that. Let’s filter out the NA’s using <code>drop_na()</code>. This will delete the rows in which the value of any variable is missing. For simplicity, let’s only consider adults.</p>
<div class="sourceCode" id="cb636"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ch7</span> <span class="op">&lt;-</span> <span class="va">nhanes</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">age</span>, <span class="va">gender</span>, <span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">age</span> <span class="op">&gt;=</span> <span class="fl">18</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">drop_na</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p>Plot your data. <code>geom_density()</code> is a smooth version of <code>geom_histogram()</code>. With <code>geom_density()</code>, the y-axis is scaled so that the area under the curve equals 1.</p>
<!-- DK: Delete females and make a better plot. Add discussion about 95% interval. Connect to posterior_predict later. More or less: posterior_predict makes the same histogram as the raw data. Teach that! -->
<div class="sourceCode" id="cb637"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ch7</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">height</span>, color <span class="op">=</span> <span class="va">gender</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">geom_density</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Height"</span>,
       title <span class="op">=</span> <span class="st">"Height by Gender in NHANES Dataset"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-497-1.png" width="100%"></div>
<p>We can see the the most probable heights for both genders and that men are generally taller than women.</p>
<p>Let’s focus on a subset of the <code>nhanes</code> data, designed to answer our questions. And, instead of using all the data, let’s just use 50 randomly selected observations. (The <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> function ensures that the same 50 observations are selected every time this code is run.) Using just 50 observations sets the stage for our discussion of <em>coverage</em> in Section <a href="two-parameters.html#coverage">7.5</a>.</p>
<!-- DK: Make the above flow more smoothly. -->
<div class="sourceCode" id="cb638"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ch7_all</span> <span class="op">&lt;-</span> <span class="va">nhanes</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">gender</span> <span class="op">==</span> <span class="st">"Male"</span>, <span class="va">age</span> <span class="op">&gt;=</span> <span class="fl">18</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">drop_na</span><span class="op">(</span><span class="op">)</span> 

<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">9</span><span class="op">)</span>
<span class="va">ch7</span> <span class="op">&lt;-</span> <span class="va">ch7_all</span> <span class="op">%&gt;%</span> 
  <span class="fu">sample_n</span><span class="op">(</span><span class="fl">50</span><span class="op">)</span></code></pre></div>
<p>Will the data we have — which is only for a sample of adult American men more than a decade ago — allow us to answer our questions, however roughly?</p>
<p>That is where Wisdom comes in. In the social sciences, <em>there is never a perfect relationship between the data you have and the question you are trying to answer.</em> Data for American males in the past is not the same thing as data for American males today. Nor is it the same as the data for men in France or Mexico. Moreover, the problem hasn’t specified where on Earth we are, nor who we are near. Walking by a basketball tournament will generate different answers than walking around Times Square.</p>
<p>Yet, this data is relevant. Right? It is certainly better than nothing. <em>Using not-perfect data is generally better than using no data at all.</em></p>
<p>Is not-perfect data <em>always</em> better? No! If your problem is estimating the median height of 5th grade girls in Tokyo, we doubt that our data is at all relevant. Wisdom recognizes the danger of using non-relevant data to build a model and then mistakenly using that model in a way which will only make the situation worse. If the data won’t help, don’t use the data, don’t build a model. Better to just use your common sense and experience. Or find better data.</p>
<!-- DK: More on population. -->
<p>The statistical term “population” is often relevant here. Recall from Chapter <a href="one-parameter.html#one-parameter">6</a> that the “population” is the set of units which we are interested in. <em>To perform inference, there must be a population which includes both the data that we have and the data that we want to have.</em> The population is all the rows in the ideal Preceptor Table.</p>
<p>Wisdom asks: <em>Is there really a single population which includes both the data that we have and the data we need to answer our questions?</em> If there is, then we can proceed. If not, we must find another approach.</p>
<p>The other aspect of Wisdom is ethics. Just because we <em>can</em> make a model does not mean we <em>should</em> make that model. Models can be used for evil and, if at all possible, we should do no evil. Fortunately, it is hard to generate many ethical worries about height models. If, instead, we were modeling criminality, the ethics become much more complex . . .</p>
</div>
</div>
<div id="justice-1" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Justice<a class="anchor" aria-label="anchor" href="#justice-1"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="other/images/Justice.jpg" width="100%"></div>
<p>Having looked at our data and decided that it is “close enough” to our questions that creating a model will help us come up with better answers, we move on to Justice. A mathematical model and a Preceptor Table are the two components of Justice.</p>
<p>Mathematical knowledge is the least important skill for a data scientist.</p>
<p>However, a little mathematical notation will make our modeling assumptions clear, will bring some precision to our approach. In this case:</p>
<p><span class="math display">\[ y_i =  \mu + \epsilon_i \]</span>
with <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>. <span class="math inline">\(y_i\)</span> is the height of male <span class="math inline">\(i\)</span>. <span class="math inline">\(\mu\)</span> is the average height of all males in the population. <span class="math inline">\(\epsilon_i\)</span> is the “error term,” the difference between the height of male <span class="math inline">\(i\)</span> and the average height of all males. <span class="math inline">\(\epsilon_i\)</span> is, by assumption, normally distributed with a mean of 0 and a standard deviation of <span class="math inline">\(\sigma\)</span>.</p>
<p>This is the simplest model we can construct. Note:</p>
<ul>
<li>The model has two unknown parameters: <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. Before we can do anything else we need to estimate the values of these parameters. Can we ever know their exact value? No! Perfection lies only in God’s own R code. But, by using a Bayesian approach similar to what we used in Chapters <a href="probability.html#probability">5</a> and <a href="one-parameter.html#one-parameter">6</a>, we will be able to create a <em>posterior probability distribution</em> for each parameter.</li>
</ul>
<!-- DK: Box quote. --><ul>
<li><p>The model is wrong, as are all models.</p></li>
<li><p>The parameter we most care about is <span class="math inline">\(\mu\)</span>. That is the parameter with a substantively meaningful interpretation. Not only is the meaning of <span class="math inline">\(\sigma\)</span> difficult to describe, we also don’t particular care about its value. Parameters like <span class="math inline">\(\sigma\)</span> in this context are <em>nuisance</em> or <em>auxiliary</em> parameters. We still estimate their posterior distributions, but we don’t really care what those posteriors look like.</p></li>
<li><p><span class="math inline">\(\mu\)</span> is not the average height of the men in the sample. We can calculate that directly. It is 175.61. No estimation required! Instead, <span class="math inline">\(\mu\)</span> is the average height of men in the <em>population</em>. Recall from the discussions in Chapter <a href="one-parameter.html#one-parameter">6</a> that the population is the universe of people/units/whatever about which we seek to draw conclusions. On some level, this seems simple. On a deeper level, it is very subtle. For example, if we are walking around Copenhagen, then the population we really care about, in order to answer our three questions, is the set of adult men which we might meet today. This is not the same as the population of adult men in the US in 2010. But is it close enough? Is it better than nothing? We want to assume that both men from <code>nhanes</code> (the data we have) and men we meet in Copenhagen today (the data we want to have) are drawn from the same <em>population</em>. Each case is a different and the details matter.</p></li>
<li><p><span class="math inline">\(\sigma\)</span> is an estimate for the standard deviation of the errors, i.e., variability in height after accounting for the mean.</p></li>
</ul>
<!-- You can show students the sample estimate of that with something like this (or sd(resid(fit_obj)) --><!-- DK: Should we discuss what a superpopulation is? --><p>Consider:</p>
<p><span class="math display">\[\text{outcome} = \text{model} + \text{what is not in the model}\]</span>
In this case, the <em>outcome</em> is the height of an individual male. This variable, also called the “response,” is what we are trying to understand and/or explain and/or predict. The <em>model</em> is our creation, a mixture of data and parameters, an attempt to capture the underlying structure in the world which generates the outcome.</p>
<p>What is the difference between the <em>outcome</em> and the <em>model</em>? By definition, it is <em>what is not in the model</em>, all the blooming and buzzing complexity of the real world. The model will always be incomplete in that it won’t capture everything. Whatever the model misses is thrown into the error term.</p>
<p>The Preceptor Table for this problem is almost identical to the one we saw in Chapter <a href="rubin-causal-model.html#rubin-causal-model">4</a>:</p>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#jghbgqokwz .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#jghbgqokwz .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#jghbgqokwz .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#jghbgqokwz .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#jghbgqokwz .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#jghbgqokwz .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#jghbgqokwz .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#jghbgqokwz .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#jghbgqokwz .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#jghbgqokwz .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#jghbgqokwz .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#jghbgqokwz .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#jghbgqokwz .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#jghbgqokwz .gt_from_md > :first-child {
  margin-top: 0;
}

#jghbgqokwz .gt_from_md > :last-child {
  margin-bottom: 0;
}

#jghbgqokwz .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#jghbgqokwz .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#jghbgqokwz .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#jghbgqokwz .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#jghbgqokwz .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#jghbgqokwz .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#jghbgqokwz .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#jghbgqokwz .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#jghbgqokwz .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#jghbgqokwz .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#jghbgqokwz .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#jghbgqokwz .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#jghbgqokwz .gt_left {
  text-align: left;
}

#jghbgqokwz .gt_center {
  text-align: center;
}

#jghbgqokwz .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#jghbgqokwz .gt_font_normal {
  font-weight: normal;
}

#jghbgqokwz .gt_font_bold {
  font-weight: bold;
}

#jghbgqokwz .gt_font_italic {
  font-style: italic;
}

#jghbgqokwz .gt_super {
  font-size: 65%;
}

#jghbgqokwz .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="jghbgqokwz" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><div class="inline-table"><table class="gt_table">
<thead class="gt_col_headings">
<tr>
<th class="gt_col_heading gt_center gt_columns_bottom_border" rowspan="2" colspan="1" style="text-align: left; vertical-align: middle;">ID</th>
      <th class="gt_center gt_columns_top_border gt_column_spanner_outer" rowspan="1" colspan="1">
        <span class="gt_column_spanner">Outcome</span>
      </th>
    </tr>
<tr>
<th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Heights (cm)</th>
    </tr>
</thead>
<tbody class="gt_table_body">
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">1</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">2</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">...</td>
      <td class="gt_row gt_center">...</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">473</td>
      <td class="gt_row gt_center">172</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">474</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">...</td>
      <td class="gt_row gt_center">...</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">3,258</td>
      <td class="gt_row gt_center">?</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">3,259</td>
      <td class="gt_row gt_center">162</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">...</td>
      <td class="gt_row gt_center">...</td>
    </tr>
<tr>
<td class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #000000;">N</td>
      <td class="gt_row gt_center">?</td>
    </tr>
</tbody>
</table></div></div>
<p> </p>
<p>Since this is not a causal model, there is only one potential outcome — which is to say, only one outcome: an individual’s height.</p>
<!-- DK: To set the stage for the next chapter, might show an equation a math equation in which mu is a parameter multiplied times an columns of 1s. Maybe? -->
</div>
<div id="courage-1" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Courage<a class="anchor" aria-label="anchor" href="#courage-1"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="other/images/Courage.jpg" width="100%"></div>
<p>In data science, we deal with words, math, and code, but the most important of these is code. We need Courage to create the model, to take the leap of faith that we can make our ideas real.</p>
<div id="stan_glm-1" class="section level4" number="7.3.0.1">
<h4>
<span class="header-section-number">7.3.0.1</span> stan_glm<a class="anchor" aria-label="anchor" href="#stan_glm-1"><i class="fas fa-link"></i></a>
</h4>
<p>Bayesian models are not hard to create in R. The <strong>rstanarm</strong> package provides the tools we need, most importantly the function <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code>.</p>
<div class="sourceCode" id="cb639"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mc-stan.org/rstanarm/">rstanarm</a></span><span class="op">)</span></code></pre></div>
<p>The first argument in <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code> is <code>data</code>, which in our case is the filtered <code>ch7</code> tibble which contains 50 observations. The only other mandatory argument is the <code>formula</code> that we want to use to build the model. In this case, since we have no predictor variables, our formula is <code>height ~ 1</code>.</p>
<div class="sourceCode" id="cb640"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">ch7</span>, 
                    formula <span class="op">=</span> <span class="va">height</span> <span class="op">~</span> <span class="fl">1</span>, 
                    family <span class="op">=</span> <span class="va">gaussian</span>, 
                    refresh <span class="op">=</span> <span class="fl">0</span>,
                    seed <span class="op">=</span> <span class="fl">9</span><span class="op">)</span></code></pre></div>
<p>Details:</p>
<ul>
<li><p>This may take time. Bayesian models, especially ones with large amounts of data, can take longer than we might like. Indeed, computational limits were the main reason why Bayesian approaches were — and, to some extent, still are — little used. When creating your own models, you will often want to use the <code>cache = TRUE</code> code chunk option. This saves the result of the model so that you don’t recalculate it every time you knit your document.</p></li>
<li><p>The <code>data</code> argument, like all such usage in R, is for the input data.</p></li>
<li><p>If you don’t set <code>refresh = 0</code>, the model will puke out many lines of confusing output. You can learn more about that output by reading the help page for <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code>. The output provides details on the fitting process as it runs as well as diagnostics about the final result. All of those details are beyond the scope of this book.</p></li>
<li><p>You should always assign the result of the call of <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code> to an object, as we do above. By convention, the name of that object will often include the word “fit” to indicate that it is a <em>fitted</em> model object.</p></li>
<li><p>There is a direct connection between the mathematical form of the model created under Justice and the code we use to fit the model under Courage. <code>height ~ 1</code> is the code equivalent of <span class="math inline">\(y_i = \mu\)</span>.</p></li>
<li><p>The default value for <code>family</code> is <code>gaussian</code>, so we did not need to include it in the call above. From the Justice section, the assumption that <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> is equivalent to using <code>gaussian</code>. If <span class="math inline">\(\epsilon_i\)</span> had a different distribution, we would need to use a different <code>family</code>. We saw an example of such a situation at the end of Chapter <a href="one-parameter.html#one-parameter">6</a> when we performed the urn analysis using <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code> and setting <code>family = binomial</code>.</p></li>
<li><p>Although you can use the standalone function <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> in order to make your code reproducible, it is more convenient to use the <code>seed</code> argument within <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code>. Even though the fitting process, unavoidably, contains a random component, you will get the exact same answer if we set <code>seed = 9</code> and rerun this code. It does not matter what number you use for the seed.</p></li>
</ul>
<div id="printed-model" class="section level5" number="7.3.0.1.1">
<h5>
<span class="header-section-number">7.3.0.1.1</span> Printed model<a class="anchor" aria-label="anchor" href="#printed-model"><i class="fas fa-link"></i></a>
</h5>
<p>There are several ways to examine the fitted model. The simplest is to print it. Recall that just typing <code>x</code> at the prompt is the same as writing <code><a href="https://docs.ropensci.org/skimr/reference/print.html">print(x)</a></code>.</p>
<div class="sourceCode" id="cb641"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit_obj</span></code></pre></div>
<pre><code>## stan_glm
##  family:       gaussian [identity]
##  formula:      height ~ 1
##  observations: 50
##  predictors:   1
## ------
##             Median MAD_SD
## (Intercept) 175.6    1.2 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 8.5    0.9   
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<p>The first line is telling us which model we used, in our case a <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code>.</p>
<p>The second line tells us this model is using a Gaussian, or normal, distribution. We discussed this distribution in Section <a href="wrangling.html#normal">2.9.5</a>. We typically use this default unless we are working with a lefthand variable that is extremely non-normal, e.g., something which only takes two values like 0/1 or TRUE/FALSE. Since height is (very roughly) normally distributed, the Gaussian distribution is a good choice.</p>
<p>The third line gives us back the formula we provided. We are creating a model predicting height with a constant — which is just about the simplest model you can create. Formulas in R are constructed in two parts. First, on the left side of the tilde (the “~” symbol) is the “response” or “dependent” variable, the thing which we are trying to explain. Since this is a model about <code>height</code>, <code>height</code> goes on the lefthand side. Second, we have the “explanatory” or “independent” or “predictor” variables on the righthand side of the tilde. There will often be many such variables but in this, the simplest possible model, there is only one, a single constant. (The number <code>1</code> indicates that constant. It does not mean that we think that everyone is height <code>1</code>.)</p>
<p>The fourth and fifth lines of the output tell us that we have 50 observations and that we only have one predictor (the constant). Again, the terminology is a bit confusing. What does it mean to suggest that <span class="math inline">\(\mu\)</span> is “constant?” It means that, although <span class="math inline">\(\mu\)</span>’s value is unknown, it is <em>fixed</em>. It does not change from person to person. The <code>1</code> in the formula corresponds to the parameter <span class="math inline">\(\mu\)</span> in our mathematical definition of the model.</p>
<p>We knew all this information before we fit the model. R records it in the <code>fit_obj</code> because we don’t want to forget what we did. The second half of the display gives a summary of the parameter values. We can look at just the second half with the <code>detail</code> argument:</p>
<div class="sourceCode" id="cb643"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://docs.ropensci.org/skimr/reference/print.html">print</a></span><span class="op">(</span><span class="va">fit_obj</span>, detail <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></code></pre></div>
<pre><code>##             Median MAD_SD
## (Intercept) 175.6    1.2 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 8.5    0.9</code></pre>
<p>We see the output for the two parameters of the model: “(Intercept)” and “sigma.” This can be confusing! Recall that the thing we care most about is <span class="math inline">\(\mu\)</span>, the average height in the population. If we had the ideal Preceptor Table — with a row for every adult male in the population we care about and no missing data — <span class="math inline">\(\mu\)</span> would be trivial to calculate, and with no uncertainty. But only we know that we named that parameter <span class="math inline">\(\mu\)</span>. All that R sees is the <code>1</code> in the formula. In most fields of statistics, this constant term is called the “intercept.” So, now we have three things — <span class="math inline">\(\mu\)</span> (from the math), <code>1</code> (from the code), and “(Intercept)” (from the output) — all of which refer to the exact same concept. This will not be the last time that terminology will be confusing.</p>
<p>At this point, <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code> — or rather the <code><a href="https://docs.ropensci.org/skimr/reference/print.html">print()</a></code> method for objects created with <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code> — has a problem. We have full posteriors for both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. But this is a simple printed summary. We can’t show the entire distribution. So, what are the best few numbers to provide? There is no right answer to this question! Here, the choice is to provide the “Median” of the posterior and the “MAD_SD.”</p>
<ul>
<li>Anytime you have a distribution, whether posterior probability or otherwise, the most important single number associated with it is some measure of its <em>location</em>. Where is the data? The two most common choices for this measure are the mean and median. We use the median here because posterior distributions can often be quite skewed, making the mean a less reliable measure.</li>
</ul>
<!-- DK: Is "reliable" the best word? I think ROAS uses "stable." --><ul>
<li>The second most important number for summarizing a distribution concerns its <em>spread</em>. How far is the data spread around its center? The most common measure used for this is the standard deviation. MAD SD, the scaled median absolute deviation, is another. If the variable has a normal distribution, then the standard deviation and the MAD SD will be very similar. But the MAD SD is much more robust to outliers, which is why it is used here. (Note that MAD SD is the same measure as what we have referred to as <em>mad</em> up till now. The measure is calculated with the <code><a href="https://rdrr.io/r/stats/mad.html">mad()</a></code> command in R. Terminology is confusing, as usual.)</li>
</ul>
<p>We can also change the number of digits shown:</p>
<div class="sourceCode" id="cb645"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://docs.ropensci.org/skimr/reference/print.html">print</a></span><span class="op">(</span><span class="va">fit_obj</span>, detail <span class="op">=</span> <span class="cn">FALSE</span>, digits <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<pre><code>##             Median MAD_SD
## (Intercept) 175.6    1.2 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 8.5    0.9</code></pre>
<!-- DK: Are we sure that coef() and sigma() give us the median values?  -->
<p>Now that we understand the meaning of Median and MAD_SD in the above display, we can interpret the actual numbers. The median of the intercept, 175.6, is the median of our posterior distribution for <span class="math inline">\(\mu\)</span>, the average height of all men in the population. The median of sigma, 8.5, is the median of our posterior distribution for the true <span class="math inline">\(\sigma\)</span>, which can be roughly understood as the variability in the height of men, once we account for our estimate of <span class="math inline">\(\mu\)</span>.</p>
<p>The MAD SD for each parameter is a measure of the variability of our posterior distributions for that parameter. How spread out are they? Speaking roughly, 95% of the mass of a posterior probability distribution is located within +/- 2 MAD SDs from the median. For example, we would be about 95% confident that the true value of <span class="math inline">\(\mu\)</span> is somewhere between 173.2 and 178.</p>
</div>
<div id="plotting-the-posterior-distributions" class="section level5" number="7.3.0.1.2">
<h5>
<span class="header-section-number">7.3.0.1.2</span> Plotting the posterior distributions<a class="anchor" aria-label="anchor" href="#plotting-the-posterior-distributions"><i class="fas fa-link"></i></a>
</h5>
<p>Instead of doing this math in our heads, we can display both posterior probability distributions. <em>Pictures speak where math mumbles.</em> Fortunately, getting draws from those posteriors is easy:</p>
<!-- as_tibble() is just trying to make things "pretty", for certain meanings of that word. This causes the decimals for the intercept to vanish when this chapter is knit. The decimals are present when I run it here. Worth worrying about? -->
<div class="sourceCode" id="cb647"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit_obj</span> <span class="op">%&gt;%</span> 
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 2
##    `(Intercept)` sigma
##            &lt;dbl&gt; &lt;dbl&gt;
##  1          175.  8.34
##  2          175.  8.16
##  3          174.  8.26
##  4          174.  8.25
##  5          173.  9.43
##  6          175.  9.24
##  7          174.  8.47
##  8          174.  8.26
##  9          177.  8.71
## 10          173.  9.06
## # … with 3,990 more rows</code></pre>
<p>These 4,000 rows are draws from the estimated posteriors, each in its own column. These are like the vectors which result from calling functions like <code><a href="https://rdrr.io/r/stats/Normal.html">rnorm()</a></code> or <code><a href="https://rdrr.io/r/stats/Binomial.html">rbinom()</a></code>. We can create the plot in a similar way:</p>
<!-- DK: More here. Drawing the connection to what we have done in earlier chapters. -->
<div class="sourceCode" id="cb649"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit_obj</span> <span class="op">%&gt;%</span> 
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html">rename</a></span><span class="op">(</span>mu <span class="op">=</span> <span class="va">`(Intercept)`</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, 
                   bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for Average Male Height"</span>,
         subtitle <span class="op">=</span> <span class="st">"American men average around 176 cm in height"</span>,
         x <span class="op">=</span> <span class="st">"Height in Centimeters"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">theme_classic</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-509-1.png" width="100%"></div>
<p>Although it is possible to have variable names like “(Intercept),” it is not recommended. Avoid weird names! When you are stuck with them, place them in backticks. Even better, rename them, as we do above.</p>
<p>Note that the title includes the word “Posterior” and not the complete term “Posterior Probability Distribution.” This will be our practice going forward. “Posterior” means “Posterior Distribution” and, any posterior distribution in which the sum of the range is 1 is a “posterior probability distribution.” In most or our plots, “posterior” implies “posterior probability distribution.”</p>
<p>Always go back to first principles. There is some “<em>truth</em>, an unknown number, a fact about the world. If we knew everything, if we had the ideal Preceptor Table, inference would not be necessary. Algebra would suffice. Alas, in this imperfect world, we have no choice but to be data scientist. We are always uncertain. We summarize our knowledge of unknown numbers with posterior probability distributions, or”posteriors" for short.</p>
<!-- DK: Add more discussion about what this plot means. -->
<div class="sourceCode" id="cb650"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit_obj</span> <span class="op">%&gt;%</span> 
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, 
                   bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for Standard Deviation of Male Height"</span>,
         subtitle <span class="op">=</span> <span class="st">"The standard deviation of height is around 7 to 11 cm"</span>,
         x <span class="op">=</span> <span class="st">"Sigma in Centimeters"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">theme_classic</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-510-1.png" width="100%"></div>
<p>Again, <span class="math inline">\(\sigma\)</span> is usually a nuisance parameter. We don’t really care what its value is, so we rarely plot it.</p>
<!-- DK: Discuss the meaning in more detail. -->
<!-- DK: Fun problem set exercise is to pull this data, pivot it, and then create one graphic with both distributions on the same scale. Then you could . . . -->
</div>
</div>
<div id="decomposing-the-outcome" class="section level4" number="7.3.0.2">
<h4>
<span class="header-section-number">7.3.0.2</span> Decomposing the outcome<a class="anchor" aria-label="anchor" href="#decomposing-the-outcome"><i class="fas fa-link"></i></a>
</h4>
<p>Two other important concepts in model creation are fitted values and residuals.</p>
<div class="sourceCode" id="cb651"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ch7</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>fitted_value <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">fit_obj</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>residual <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">fit_obj</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">sample_n</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 5 x 3
##   height fitted_value residual
##    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;
## 1   172.         176.   -3.91 
## 2   171.         176.   -4.41 
## 3   174.         176.   -1.11 
## 4   175.         176.   -0.311
## 5   170.         176.   -5.21</code></pre>
<p>The fitted value represents the model’s best guess at to what the true value of the outcome should be for that individual, given information about any covariates. This is a tricky concept since, after all, <em>we already know what the actual value is.</em> We commonly use “hat” notation for fitted values. <span class="math inline">\(y_i\)</span> is the actual height for man <span class="math inline">\(i\)</span>. <span class="math inline">\(\hat{y_i}\)</span> is the fitted (or predicted or modeled) height for man <span class="math inline">\(i\)</span>. It is the height we would guess if we did not know his true height. The residual, <span class="math inline">\(r_i\)</span>, is the difference between the outcome (<span class="math inline">\(y_i\)</span>) and the fitted value <span class="math inline">\(\hat{y_i}\)</span>. These definitions lead to a natural decomposition of the outcome data:</p>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-512-1.png" width="100%"></div>
<!-- DK: More details. Can we add $y_i$ and other math terms to these graphs? And do a posterior predictive check! Need to discuss how the unique values are because we only have 50 observations. Maybe show all observations? -->
<!-- DK: Is there other stuff to do within Courage? Might show how to make a regression table. Should discuss how there is only one model and that we have different views of that model by looking at the printed output, the posterior probability distributions of the parameters and graphics of the fitted values and residuals.  -->
<!-- Discuss testing for the first time? Yes! Takes Courage to Not Test! Also, foreshadow the difficulties we will have in deciding what to include in the model. And transformations. Or maybe discuss transformations now? -->
</div>
</div>
<div id="temperance-1" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Temperance<a class="anchor" aria-label="anchor" href="#temperance-1"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="other/images/Temperance.jpg" width="100%"></div>
<p>We have a model. What can we do with it? Let’s answer the four questions with which we started.</p>
<div id="question-1" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">7.4.1</span> Question 1<a class="anchor" aria-label="anchor" href="#question-1"><i class="fas fa-link"></i></a>
</h3>
<!-- DK: Show how this posterior is the same as the posterior mu. They are the same thing. Discuss mapping from parameters to reality. The latter is easier and more general. The former is mostly useful in seminar rooms. -->
<ul>
<li>What is the average height of men?</li>
</ul>
<p>If we had the ideal Preceptor Table, one with a row for every man alive, and with their actual height, we could calculate this number easily. Just take the average of those 3 billion or so rows! Alas, in our actual Preceptor Table, the vast majority of heights are missing. Question marks make simple algebra impossible. So, as with any unknown number, we need to estimate a posterior probability distribution. Objects created by <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code> make this easy to do.</p>
<div class="sourceCode" id="cb653"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">newobs</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>constant <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>

<span class="va">pe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">fit_obj</span>,
                      newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span>

<span class="va">pe</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 1
##      `1`
##    &lt;dbl&gt;
##  1  175.
##  2  175.
##  3  174.
##  4  174.
##  5  173.
##  6  175.
##  7  174.
##  8  174.
##  9  177.
## 10  173.
## # … with 3,990 more rows</code></pre>
<p>We will use <code><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred()</a></code> many times. The two key arguments are <code>object</code>, the fitted model object returned by <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code>, and <code>newdata</code>, which is the tibble in which are the covariate values associated with the unit (or units) for which we want to make a forecast. (In this case, <code>newdata</code> can be any tibble because an intercept-only model does not make use of covariates. We don’t really need a variable named <code>constant</code>, but including it does no harm.) The <code>epred</code> in <code><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred()</a></code> stands for <strong>e</strong>xpected <strong>pred</strong>iction. In other words, if we pick a random adult male what do we “expect” his height to be. We also call this the <em>expected value</em>.</p>
<!-- DK: Tell the story about "mathematical hope." Maybe give more details on what the tibble is doing. Once new version comes out, replace with tibble(.rows = 1). -->
<p>We use <code>as_tibble()</code> to convert the matrix which is returned by <code><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred()</a></code>. We have a tibble with 1 column and 4,000 rows. The column, unhelpfully named <code>1</code>, is 4,000 draws from the posterior probability distribution for the expected height of a random male. Recall from earlier chapters how a posterior probability distribution and the draws from a posterior probability distribution are, more or less, the same thing. Or, rather, a posterior probability distribution, its ownself, is hard to work with. Draws from that distribution, on the other hand, are easy to manipulate. We use draws to answer our questions.</p>
<p>Converting these 4,000 draws into a posterior probability distribution is straightforward.</p>
<div class="sourceCode" id="cb655"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pe</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">`1`</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
                   bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for Average Adult Male Height"</span>,
         subtitle <span class="op">=</span> <span class="st">"Note that the plot is very similar to the one created with the parameters"</span>,
         x <span class="op">=</span> <span class="st">"Average Height in Centimeters"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span> 
    <span class="fu">scale_x_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_number.html">number_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">theme_classic</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-515-1.png" width="100%"></div>
<p>The rest of the <em>Primer</em> will be filled with graphics like this one. You will make dozens of them yourself. The fundamental structure for doing algebra is the real number. The fundamental structure for data science is the posterior probability distribution. You need to be able to create and interpret them.</p>
</div>
<div id="question-2" class="section level3" number="7.4.2">
<h3>
<span class="header-section-number">7.4.2</span> Question 2<a class="anchor" aria-label="anchor" href="#question-2"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>What is the probability that the next adult male we meet will be taller than 180 centimeters?</li>
</ul>
<p>There are two fundamentally different kinds of unknowns which we care about: <em>expected values</em> (as in the previous question) and <em>predicted values</em>. With the former, we are not interested in any specific individual. The individual value is irrelevant. With predicted values, we care, not about the average, but about this specific person. With the former, we use <code><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred()</a></code>. With the latter, the relevant function is <code><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict()</a></code>. Both functions return draws from a posterior probability distribution, but the unknown number which underlies the posterior is very different.</p>
<p>Recall the mathematics:</p>
<p><span class="math display">\[ y_i =  \mu + \epsilon_i \]</span></p>
<p>With expected values or averages, we can ignore the <span class="math inline">\(\epsilon_i\)</span> term in this formula. The expected value of <span class="math display">\[\epsilon_i\]</span> is zero since, by assumption, <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>. However, we can’t ignore <span class="math inline">\(\epsilon_i\)</span> when predicting the height for a single individual.</p>
<div class="sourceCode" id="cb656"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">newobs</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>constant <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>

<span class="va">pp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">fit_obj</span>,
                        newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span>

<span class="va">pp</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 1
##    `1`  
##    &lt;ppd&gt;
##  1 174  
##  2 174  
##  3 177  
##  4 164  
##  5 176  
##  6 177  
##  7 170  
##  8 169  
##  9 177  
## 10 175  
## # … with 3,990 more rows</code></pre>
<p>As before, it is straightforward to turn draws from the posterior probability distribution into a graphic:</p>
<div class="sourceCode" id="cb658"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pp</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">`1`</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
                   bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for Height of Random Male"</span>,
         subtitle <span class="op">=</span> <span class="st">"Uncertainty for a single individual is much greater than for the expected value"</span>,
         x <span class="op">=</span> <span class="st">"Height in Centimeters"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span> 
    <span class="fu">scale_x_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_number.html">number_format</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">theme_classic</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-517-1.png" width="100%"></div>
<p>Note:</p>
<ul>
<li><p>The posterior for an individual is much wider than the posterior for the expected value.</p></li>
<li><p>Eyeballing, seems like there is a 1 out of 3 chance that the next man we meet, or any randomly chosen man, is taller than 180 cm.</p></li>
<li><p>We can calculate the exact probability by manipulating the tibble of draws directly.</p></li>
</ul>
<div class="sourceCode" id="cb659"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pp</span><span class="op">$</span><span class="va">`1`</span> <span class="op">&gt;</span> <span class="fl">180</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">pp</span><span class="op">$</span><span class="va">`1`</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.32</code></pre>
<p>If 30% or so of the draws from the posterior probability distribution are greater than 180 cm, then there is about a 30% chance that the next individual will be taller than 180 cm.</p>
<p>Again, the key conceptual difficulty is the population. The problem we actually have involves walking around London, or wherever, today. The data we have involve America in 2010. Those are not the same things! But they are not totally different. Knowing whether the data we have is “close enough” to the problem we want to solve is at the heart of Wisdom. Yet that was the decision we made at the start of the process, the decision to create a model in the first place. Now that we have created a model, we look to the virtue of Temperance for guidance in using that model. The data we have is never a perfect match for the world we face. We need to temper our confidence and act with humility. Our forecasts will never be as good as a naive use of the model might suggest. Reality will surprise us. We need to take the model’s claims with a family-sized portion of salt.</p>
<!-- DK: More on temperance and the many ways that we should be less confident. Move material from Chapters 3 and 5 here? -->
</div>
<div id="question-3" class="section level3" number="7.4.3">
<h3>
<span class="header-section-number">7.4.3</span> Question 3<a class="anchor" aria-label="anchor" href="#question-3"><i class="fas fa-link"></i></a>
</h3>
<!-- DK: Discuss how each row of pp is like a random draw from the scenario of 4 new men. So, we can look at each row without considering any other row. Makes simulation studies much easier. -->
<ul>
<li>What is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?</li>
</ul>
<p>Bayesian models are beautiful because, via the magic of simulation, we can answer (almost!) any question. Because the question is about four random individuals, we need <code><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict()</a></code> to give us four sets of draws from four identical posterior probability distributions. Start with a new <code>newobs</code>:</p>
<div class="sourceCode" id="cb661"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">newobs</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>constant <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span>

<span class="va">newobs</span></code></pre></div>
<pre><code>## # A tibble: 4 x 1
##   constant
##      &lt;dbl&gt;
## 1        1
## 2        1
## 3        1
## 4        1</code></pre>
<p>If you need to predict X individuals, then you need a tibble with X rows, regardless of whether or not those rows are otherwise identical.</p>
<div class="sourceCode" id="cb663"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">fit_obj</span>,
                        newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu">mutate_all</span><span class="op">(</span><span class="va">as.numeric</span><span class="op">)</span></code></pre></div>
<p>Note we need to add <code>mutate_all(as.numeric)</code> to the end of the pipe. This is caused by a bug — or at least an awkwardness — whereby the variable type provided by <code><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict()</a></code> is <code>ppd</code> rather than <code>dbl</code>. Using <code>mutate_all(as.numeric)</code> makes each column of type double. Avoid working with columns of type <code>ppd</code>. Doing so will only lead to heartache.</p>
<div class="sourceCode" id="cb664"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pp</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 4
##      `1`   `2`   `3`   `4`
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  169.  187.  174.  173.
##  2  180.  175.  180.  186.
##  3  190.  194.  171.  171.
##  4  172.  179.  185.  175.
##  5  166.  187.  154.  172.
##  6  179.  167.  174.  174.
##  7  173.  177.  180.  179.
##  8  178.  173.  160.  169.
##  9  174.  184.  174.  181.
## 10  170.  166.  172.  165.
## # … with 3,990 more rows</code></pre>
<p>The next step is to calculate the number of interest. We can not, directly, draw the height of the tallest or shortest out of 4 random men. However, having drawn 4 random men, we can calculate those numbers, and the difference between them.</p>
<div class="sourceCode" id="cb666"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># First part of the code is the same as we did above.</span>

<span class="va">pp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">fit_obj</span>,
                        newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu">mutate_all</span><span class="op">(</span><span class="va">as.numeric</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  
        <span class="co"># Second part of the code requires some trickery.</span>
  
        <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>tallest <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>shortest <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
        <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>diff <span class="op">=</span> <span class="va">tallest</span> <span class="op">-</span> <span class="va">shortest</span><span class="op">)</span> 
        
<span class="va">pp</span>        </code></pre></div>
<pre><code>## # A tibble: 4,000 x 7
## # Rowwise: 
##      `1`   `2`   `3`   `4` tallest shortest  diff
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;
##  1  185.  177.  180.  173.    185.     173. 11.7 
##  2  172.  173.  180.  189.    189.     172. 16.8 
##  3  194.  180.  169.  158.    194.     158. 35.8 
##  4  165.  168.  175.  178.    178.     165. 13.4 
##  5  170.  166.  175.  173.    175.     166.  9.42
##  6  179.  171.  178.  169.    179.     169.  9.42
##  7  172.  173.  175.  188.    188.     172. 15.5 
##  8  172.  180.  170.  167.    180.     167. 12.9 
##  9  172.  169.  172.  187.    187.     169. 18.4 
## 10  175.  178.  176.  159.    178.     159. 19.0 
## # … with 3,990 more rows</code></pre>
<p>These steps serve as a template for much of the analysis we do later. It is often very hard to create a model <em>directly</em> of the thing we want to know. There is no <em>easy</em> way to create a model which estimates this height difference <em>directly</em>. It is easy, however, to create a model which allows for random draws.</p>
<p><em>Give us enough random draws, and a tibble in which to store them, and we can estimate the world.</em></p>
<p>Once we have random draws from the posterior distribution we care about, graphing the posterior probability distribution is the same-old, same-old.</p>
<div class="sourceCode" id="cb668"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pp</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">diff</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
                   bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for Max Height Difference Among Four Men"</span>,
         subtitle <span class="op">=</span> <span class="st">"The expected value for this difference would be much more narrow"</span>,
         x <span class="op">=</span> <span class="st">"Height Difference in Centimeters"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span> 
    <span class="fu">scale_x_continuous</span><span class="op">(</span>breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">50</span>, <span class="fl">10</span><span class="op">)</span>,
                       labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_number.html">number_format</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-523-1.png" width="100%"></div>
<p>There is about an 84% chance that, when meeting 4 random men, the tallest will be at least 10 cm taller than the shortest. Exact calculation:</p>
<div class="sourceCode" id="cb669"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pp</span><span class="op">$</span><span class="va">diff</span> <span class="op">&gt;</span> <span class="fl">10</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">pp</span><span class="op">$</span><span class="va">diff</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.84</code></pre>
<!-- DK: Discuss all the reasons why this might not be true. Show how to give the exact answer. -->
</div>
<div id="question-4" class="section level3" number="7.4.4">
<h3>
<span class="header-section-number">7.4.4</span> Question 4<a class="anchor" aria-label="anchor" href="#question-4"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>What is our posterior probability distribution of the height of the 3rd tallest man out of the next 100 we meet?</li>
</ul>
<p>The same approach will work for almost any question.</p>
<div class="sourceCode" id="cb671"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">newobs</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>constant <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span>

<span class="va">pp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">fit_obj</span>,
                        newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">mutate_all</span><span class="op">(</span><span class="va">as.numeric</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>third_tallest <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, 
                              decreasing <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<p>Explore the <code>pp</code> object. It has 101 columns: one hundred for the 100 individual heights and one column for the 3rd tallest among them. Having done the hard work, plotting is easy:</p>
<div class="sourceCode" id="cb672"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pp</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">third_tallest</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for Height of 3rd Tallest Man from Next 100"</span>,
         subtitle <span class="op">=</span> <span class="st">"Should we have more or less certainty about behavior in the tails?"</span>,
         x <span class="op">=</span> <span class="st">"Height in Centimeters"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span> 
    <span class="fu">scale_x_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_number.html">number_format</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> </code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-526-1.png" width="100%"></div>
<!-- DK: Need more text. Explain all the things that could be wrong with the model. Explain what is going on in different columns. Explain all the cool R code tricks. Discuss difference between predict and epred.  -->
</div>
</div>
<div id="coverage" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Coverage<a class="anchor" aria-label="anchor" href="#coverage"><i class="fas fa-link"></i></a>
</h2>
<!-- Prove that the bootstrap works, that our 95% confidence intervals provide correct coverage. We make a game and I give you a sample of like 40. Here's the 40, and you give me a 95% CI using the bootstrap tools we learned. Then I give you another 40. And another. If we do this 1,00 times, and we use the same procedure for calculating a confidence interval each time, then 950 should include the truth. That's how we know bootstraps is correct and I could only demonstrate this to you if we know what the truth is.  

Plan: create function called create_ci(), which takes a tibble with a single variable called height and returns the 95% confidence interval --- i.e., a numeric vector of length 2 --- for the 2.5th and 97.5th percentiles.  Note that you get to hard code everything.

Then, create a tibble, first column is ID. Second column is height_sample, which is created by running sample(ch7$height, size = 40, replace = FALSE). Third column is ci, which is result mutate(ci = map(height_sample, ~ create_ci(.)). Fourth column is within_ci, which is TRUE if ci includes the TRUE value and FALSE otherwise. (If you want to have two columns, one for each limit, that is fine.)
-->
<!-- Discuss how this exercise is still useful even if we begin with all our data. That is, don't sample. Just use all 5,000 people. Then, do the bootstrap to get a confidence interval. Note that the interval will be --- how much? --- smaller than the ones we got above, because we are using 116 times as much data. But it is also weird. I know exactly what the mean is! I have the entire Rubin Table! I don't need a confidence interval for the mean.  -->
<!-- That is both true, and false. If all you truly care about is the mean these 5,000 people then, it is true, you are done. But that is generally not the case! The true Rubin Table is often bigger than you might initially think. You might also be interested in data from another time period (which has occurred but which may not be available to you) or from 2021, which has not even happened yet. Your Rubin Table includes rows for all those people. They are just missing. You also care about the millions of people who are not in the 5,000. You really want the mean for the country. (Or the world?) So, you use the model that you have to estimate stuff for the data that you don't. What is your best guess for the mean in 2010 (which you can check) or in 2021 (which you can't)? How confident are you are that estimate? What is your 50/50 prediction interval? -->
<!-- Relatedly, what if I told you that your 2010 data I gave you did not include one person (or ten people or 100), which was (were) dropped at random from the data by mistake. What is your guess as to the height of that single person, or the height of the average of the 10 people or the tallest of the 10 classes? What is your confidence interval for that? Want to bet? These are all different estimands. -->
<p>You now understand how to use <code><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm()</a></code> to estimate posterior probability distributions. But we have not, yet, offered evidence that these distributions are accurate, that they “work.” Does the 95% confidence interval really include the true value 95% of the time? The technical term for this is “coverage,” derived from the idea that any X% confidence interval for an unknown value should include that unknown value, should “cover” the value, X% of the time. We can explore coverage with a simple simulation.</p>
<!-- DK: When I use n = 1000, the book fails to build, I suspect because storing a 1,000 models is too much. -->
<!-- DK: Could make a fun plot which shows, first, all 1000 confidence intervals as lines in order, with the 50 or so who misssed the truth in red. And, next to it, show the same but ordered by lower bound. Then, the misses would be 25 at the top and 25 at the bottom. -->
<div class="sourceCode" id="cb673"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Calculating key values, and giving them useful names, at the top of a block of</span>
<span class="co"># code is handy.</span>

<span class="va">true_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">ch7_all</span><span class="op">$</span><span class="va">height</span><span class="op">)</span>

<span class="co"># Always run initial versions of the simulation with n = 3. That is enough</span>
<span class="co"># iterations to make sure that the code is working. Once it is, switch n to a</span>
<span class="co"># larger number.</span>

<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span>

<span class="co"># Using set.seed() ensures that you will get the same result when you run this</span>
<span class="co"># code as we did in writing the book.</span>

<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">96</span><span class="op">)</span>

<span class="co"># Simulations like this demonstrate the power of list-columns and map functions.</span>
<span class="co"># It is always a good idea to start with a tibble that only includes an ID or</span>
<span class="co"># counter variable. Then, we add columns to this tibble as we, step-by-step,</span>
<span class="co"># conduct the simulation.</span>

<span class="va">cov_analysis</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>ID <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  
  <span class="co"># Note how sample_id is a tibble, not just a number, as we are used to in</span>
  <span class="co"># tibbles. List-columns can contain almost anything. The first argument to</span>
  <span class="co"># map() is the thing that we are iterating over. In this case, ID does not</span>
  <span class="co"># matter since we are not using its value in the function call to sample_n().</span>
  
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>sample_data <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">ID</span>, <span class="op">~</span> <span class="fu">sample_n</span><span class="op">(</span><span class="va">ch7_all</span>, <span class="fl">50</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  
  <span class="co"># It is easy to forget both the tilde and the period when using map functions.</span>
  <span class="co"># In this row, we are doing the exact same modeling as we did above. But, in</span>
  <span class="co"># this case, we are making a new model, with new data, in each row. That can</span>
  <span class="co"># be an expensive operations, especially since we are saving the fitted model</span>
  <span class="co"># each time.</span>
  
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>model <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">sample_data</span>, <span class="op">~</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">.</span>, 
                                             <span class="va">height</span> <span class="op">~</span> <span class="fl">1</span>, 
                                             refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  
  <span class="co"># For each model, we need the 95% confidence interval. posterior_interval(),</span>
  <span class="co"># when given a fitted model and a parameter, returns that confidence interval</span>
  <span class="co"># as a matrix with two columns.</span>
  
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>conf_interval <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">model</span>, 
                             <span class="op">~</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_interval.stanreg.html">posterior_interval</a></span><span class="op">(</span><span class="va">.</span>, 
                                                  pars <span class="op">=</span> <span class="st">"(Intercept)"</span>,
                                                  prob <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  
  <span class="co"># The next chunk of code is not really necessary. We don't need to pull out</span>
  <span class="co"># the lower and upper bound into separate columns and then, after doing so,</span>
  <span class="co"># check the coverage. But, when debugging, it is nice to check each step. Note</span>
  <span class="co"># the use of [] to pull values from the conf_interval matrix.</span>
  
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fu">map_dbl</span><span class="op">(</span><span class="va">conf_interval</span>, <span class="op">~</span> <span class="va">.</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>upper <span class="op">=</span> <span class="fu">map_dbl</span><span class="op">(</span><span class="va">conf_interval</span>, <span class="op">~</span> <span class="va">.</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>coverage <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">true_value</span> <span class="op">&gt;</span> <span class="va">lower</span> <span class="op">&amp;</span> 
                             <span class="va">true_value</span> <span class="op">&lt;</span> <span class="va">upper</span>,
                           <span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb674"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">cov_analysis</span><span class="op">$</span><span class="va">coverage</span><span class="op">)</span></code></pre></div>
<pre><code>##    Mode   FALSE    TRUE 
## logical       4      96</code></pre>
<p>Given randomness, coverage will only be precisely correct by luck. The key point is that, <em>about</em> 95% of the time, a 95% confidence interval includes the true value.</p>
<!-- DK: More to say? -->
</div>
<div id="outcomes" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> 0/1 Outcomes<a class="anchor" aria-label="anchor" href="#outcomes"><i class="fas fa-link"></i></a>
</h2>
<p>Variables with well-behaved, continuous ranges are the easiest to handle. We started with <code>height</code> because it was simple. Sadly, however, many variables are not like <code>height</code>. Consider <code>gender</code>, a variable in <code>nhanes</code> which takes on two possible values: “Male” and “Female.” In the same way that we would like to construct a model which explains or predicts <code>height</code>, we would like to build a model which explains or predicts <code>gender</code>. We want to answer questions like:</p>
<p><em>What is the probability that a random person who is 180 cm tall is female?</em></p>
<p><em>Wisdom</em> suggests we start by looking at the data. Because models use numbers, we need to create a new variable, <code>female</code>, which is 1 for Females and 0 for Males.</p>
<div class="sourceCode" id="cb676"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ch7_b</span> <span class="op">&lt;-</span> <span class="va">nhanes</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">age</span>, <span class="va">gender</span>, <span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>female <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">gender</span> <span class="op">==</span> <span class="st">"Female"</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">age</span> <span class="op">&gt;=</span> <span class="fl">18</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">female</span>, <span class="va">height</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">drop_na</span><span class="op">(</span><span class="op">)</span>

<span class="va">ch7_b</span></code></pre></div>
<pre><code>## # A tibble: 7,424 x 2
##    female height
##     &lt;dbl&gt;  &lt;dbl&gt;
##  1      0   165.
##  2      0   165.
##  3      0   165.
##  4      1   168.
##  5      1   167.
##  6      1   167.
##  7      1   167.
##  8      0   170.
##  9      0   182.
## 10      0   169.
## # … with 7,414 more rows</code></pre>
<div class="sourceCode" id="cb678"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ch7_b</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">height</span>, y <span class="op">=</span> <span class="va">female</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_jitter</span><span class="op">(</span>height <span class="op">=</span> <span class="fl">0.1</span>, alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Gender and Height"</span>,
       subtitle <span class="op">=</span> <span class="st">"Men are taller than women"</span>,
       x <span class="op">=</span> <span class="st">"Height (cm)"</span>,
       y <span class="op">=</span> <span class="cn">NULL</span>,
       caption <span class="op">=</span> <span class="st">"Data from NHANES"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">scale_y_continuous</span><span class="op">(</span>breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,
                     labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Male"</span>, <span class="st">"Female"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-530-1.png" width="100%"></div>
<p>Why not just fit a linear model, as we did above? Consider:</p>
<div class="sourceCode" id="cb679"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit_gender_linear</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">ch7_b</span>,
                              formula <span class="op">=</span> <span class="va">female</span> <span class="op">~</span> <span class="va">height</span>,
                              family <span class="op">=</span> <span class="va">gaussian</span>,
                              refresh <span class="op">=</span> <span class="fl">0</span>,
                              seed <span class="op">=</span> <span class="fl">82</span><span class="op">)</span> </code></pre></div>
<p>Recall that the default value for <code>family</code> is <code>gaussian</code>, so we did not need to include it here. Initially, the fitted model seems OK.</p>
<div class="sourceCode" id="cb680"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://docs.ropensci.org/skimr/reference/print.html">print</a></span><span class="op">(</span><span class="va">fit_gender_linear</span>, digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<pre><code>## stan_glm
##  family:       gaussian [identity]
##  formula:      female ~ height
##  observations: 7424
##  predictors:   2
## ------
##             Median MAD_SD
## (Intercept)  6.22   0.07 
## height      -0.03   0.00 
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 0.36   0.00  
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<p>Comparing two individuals who differ in <code>height</code> by 1 cm, we expect the taller individual to have a 3% lower probability of being female. That is not unreasonable. The problems show up at the extremes. Consider the fitted values across the range of our data.</p>
<div class="sourceCode" id="cb682"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ch7_b</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">height</span>, y <span class="op">=</span> <span class="va">female</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_jitter</span><span class="op">(</span>height <span class="op">=</span> <span class="fl">0.1</span>, alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_smooth</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">fit_gender_linear</span><span class="op">)</span><span class="op">)</span>,
              method <span class="op">=</span> <span class="st">"lm"</span>,
              formula <span class="op">=</span> <span class="va">y</span> <span class="op">~</span> <span class="va">x</span>,
              se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Gender and Height"</span>,
       subtitle <span class="op">=</span> <span class="st">"Some fitted values are impossible"</span>,
       x <span class="op">=</span> <span class="st">"Height (cm)"</span>,
       y <span class="op">=</span> <span class="cn">NULL</span>,
       caption <span class="op">=</span> <span class="st">"Data from NHANES"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">scale_y_continuous</span><span class="op">(</span>breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0</span>, <span class="fl">0.5</span>, <span class="fl">1</span>, <span class="fl">1.5</span><span class="op">)</span>,
                     labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"-50%"</span>, <span class="st">"0% (Male)"</span>, 
                                <span class="st">"50%"</span>, <span class="st">"100% (Female)"</span>,
                                <span class="st">"150%"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-533-1.png" width="100%"></div>
<p>Using 1 for Female and 0 for Male allows us to interpret fitted values as <em>the probability that a person is female or male</em>. That is a handy and natural interpretation. The problem with a linear model arises when, as in this case, the model suggests values outside 0 to 1. Such values are, by definition, impossible. People who are 190 cm tall do not have a -25% chance of being female.</p>
<p><em>Justice</em> suggests a different functional form, one which restricts fitted values to the acceptable range. Look closely at the math:</p>
<p><span class="math display">\[  p(\text{Female} = 1) = \frac{\text{exp}(\beta_0 + \beta_1 \text{height})}{1 + \text{exp}(\beta_0 + \beta_1 \text{height})} \]</span></p>
<p>This is an inverse logistic function, but don’t worry about the details. Mathematical formulas are never more than a Google search away. Instead, note how the range is restricted. Even if <span class="math inline">\(\beta_0 + \beta_1 \text{height}\)</span> is a very large number, the ratio is bound below 1. Similarly, no matter how negative <span class="math inline">\(\beta_0 + \beta_1 \text{height}\)</span> is, the ratio can never be smaller than 0. The model can not, ever, produce impossible values.</p>
<p>Whenever you have two categories as the outcome, you should use <code>family = binomial</code>.</p>
<!-- DK: Footnote to discuss that often linear regression is used and also works. Show that you get the same answer for the 180 question, but not for 200! -->
<p><em>Courage</em> allows us to use the same tools for fitting this logistic regression as we did above in fitting linear models.</p>
<div class="sourceCode" id="cb683"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fit_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/stan_glm.html">stan_glm</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">ch7_b</span>,
                  formula <span class="op">=</span> <span class="va">female</span> <span class="op">~</span> <span class="va">height</span>,
                  family <span class="op">=</span> <span class="va">binomial</span>,
                  refresh <span class="op">=</span> <span class="fl">0</span>,
                  seed <span class="op">=</span> <span class="fl">27</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb684"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://docs.ropensci.org/skimr/reference/print.html">print</a></span><span class="op">(</span><span class="va">fit_2</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></code></pre></div>
<pre><code>## stan_glm
##  family:       binomial [logit]
##  formula:      female ~ height
##  observations: 7424
##  predictors:   2
## ------
##             Median MAD_SD
## (Intercept) 43.389  0.999
## height      -0.257  0.006
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<p>One major difference between linear and logistic models is that parameters in the latter are much harder to interpret. What does it mean, substantively, that <span class="math inline">\(\beta_1\)</span> is -0.26? That is a topic for a more advanced course.</p>
<!-- DK: Footnote to discuss? -->
<p>Fortunately, parameters are not what we care about. They are epiphenomenon, unicorns of our imagination. Instead, we want answers to our questions, for which <em>Temperance</em> — and the functions in <strong>rstanarm</strong> — is our guide. Recall our question:</p>
<p><em>What is the probability that a random person who is 180 cm tall is female?</em></p>
<div class="sourceCode" id="cb686"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">newobs</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>height <span class="op">=</span> <span class="fl">180</span><span class="op">)</span>

<span class="va">pe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred</a></span><span class="op">(</span><span class="va">fit_2</span>, newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span>

<span class="va">pe</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">`1`</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
                   bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for p(female | height = 180 cm)"</span>,
         subtitle <span class="op">=</span> <span class="st">"There is a 5-6% chance that a person this tall is female"</span>,
         x <span class="op">=</span> <span class="st">"Probability"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span> 
    <span class="fu">scale_x_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-536-1.png" width="100%"></div>
<p>There is only about a 1 in 20 chance that a 180 centimeter tall person is female.</p>
<p>Note that both the x and y axes are probabilities. Whenever we create a posterior probability distribution then, by definition, the y-axis is a probability. The x-axis is the unknown number we do not know. That unknown number can be anything — the weight of the average male, the height of 3rd tallest out of 100 men, the probability that a 180 cm tall person is female. A probability is just another number. The interpretation is the same as always.</p>
<p>Another major difference with logistic models is that <code><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred()</a></code> and <code><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict()</a></code> return different types of objects. <code><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred()</a></code> returns probabilities, as above. <code><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict()</a></code>, on the other hand, returns predictions, as its name suggests. In other words, it returns zeros and ones. Consider another question:</p>
<p><em>In a group of 100 people who are 180 centimeters tall, how many will be women?</em></p>
<div class="sourceCode" id="cb687"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">newobs</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span>height <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">180</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span>

<span class="va">pp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict</a></span><span class="op">(</span><span class="va">fit_2</span>, newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">mutate_all</span><span class="op">(</span><span class="va">as.numeric</span><span class="op">)</span> 

<span class="va">pp</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 4
##      `1`   `2`   `3`   `4`
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     0     0     0     0
##  2     0     0     0     0
##  3     0     0     0     0
##  4     0     0     0     1
##  5     0     0     0     0
##  6     0     0     0     0
##  7     0     0     0     0
##  8     0     0     0     0
##  9     0     1     1     0
## 10     0     0     0     0
## # … with 3,990 more rows</code></pre>
<!-- DK: I am confused! Is this a posterior predictive distribution or a posterior predictive probability distribution? -->
<p>We show just the first 4 columns for convenience. Each column is 4,000 draws from the posterior predictive distribution for the gender of a person who is 180 cm tall. (Since all 100 people have the same height, all the columns are draws from the same distribution.)</p>
<p>We can manipulate this object on a row-by-row basis.</p>
<div class="sourceCode" id="cb689"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict</a></span><span class="op">(</span><span class="va">fit_2</span>, newdata <span class="op">=</span> <span class="va">newobs</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">mutate_all</span><span class="op">(</span><span class="va">as.numeric</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>total <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/everything.html">everything</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>

<span class="va">pp</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"1"</span>, <span class="st">"2"</span>, <span class="st">"100"</span>, <span class="st">"total"</span><span class="op">)</span><span class="op">]</span></code></pre></div>
<pre><code>## # A tibble: 4,000 x 4
## # Rowwise: 
##      `1`   `2` `100` total
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     0     0     0    10
##  2     0     0     0     6
##  3     0     1     0     9
##  4     0     0     0     5
##  5     0     0     0     6
##  6     0     0     0     5
##  7     0     0     0     4
##  8     1     0     0     5
##  9     0     0     0     1
## 10     0     0     0     8
## # … with 3,990 more rows</code></pre>
<p><code>total</code> is the number of women in each row. Manipulating draws on a row-by-row basis is very common.</p>
<!-- DK: Add more here. -->
<div class="sourceCode" id="cb691"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pp</span> <span class="op">%&gt;%</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">total</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
                   bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Posterior for Number of Women among 100 People 180 cm Tall"</span>,
         subtitle <span class="op">=</span> <span class="st">"Consistent with probability estimate above"</span>,
         x <span class="op">=</span> <span class="st">"Number of Women"</span>,
         y <span class="op">=</span> <span class="st">"Probability"</span>,
         caption <span class="op">=</span> <span class="st">"Data source: NHANES"</span><span class="op">)</span> <span class="op">+</span> 
    <span class="fu">scale_x_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_number.html">number_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org//reference/label_percent.html">percent_format</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </code></pre></div>
<div class="inline-figure"><img src="book_temp_files/figure-html/unnamed-chunk-539-1.png" width="100%"></div>
<p>That 5 or 6 women is the most likely number is very consistent with the answer to the first question. There we found that a random person who is 180 cm tall has a 5% or 6% chance of being female. So, with 100 such people, 5 or 6 seems a reasonable total. But the <em>expected value</em> from <code><a href="https://mc-stan.org/rstanarm/reference/posterior_linpred.stanreg.html">posterior_epred()</a></code>, although it does provide a sense of where the center of the predictive distribution will be, does not tell us much about the range of possible outcomes. For that, we need <code><a href="https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html">posterior_predict()</a></code>.</p>
</div>
<div id="summary-7" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> Summary<a class="anchor" aria-label="anchor" href="#summary-7"><i class="fas fa-link"></i></a>
</h2>
<!-- DK: Discuss meaning of "Posterior" in all plot titles. Show the key code examples. -->
<p>The next five chapters will follow the same process we have just completed here. We start with a decision we have to make. With luck, we will have some data to guide us. (Without data, even the best data scientist will struggle to make progress.) <em>Wisdom</em> asks us: “Is the data we have close enough to the decision we face to make using that data helpful?” Often times, the answer is “No.” Even if we do have data, and the ability to make a model, Wisdom will tap us on the shoulder and say, “Even if you can make a model, don’t forget to ask yourself if you should.” Ethics matter.</p>
<p>Once we start to build the model, <em>Justice</em> will guide us. Is the model descriptive or causal? What is the mathematical relationship between the dependent variable we are trying to explain and the independent variables we can use to explain it? What assumptions are we making about distribution of the error term?</p>
<p>Having set up the model framework, we need <em>Courage</em> to implement the model in code. Without code, all the math in the world is useless. Once we have created the model, we need to understand it. What are the posterior distributions of the unknown parameters? Do they seem sensible? How should we interpret them?</p>
<p><em>Temperance</em> guides the final step. With a model, we can finally get back to the decision which motivated the exercise in the first place. We can use the model to make statements about the world, both to confirm that the model is consistent with the world and to use the model to make predictions about numbers which we do not know.</p>
<p>Let’s practice this process another dozen or so times.</p>
<!-- ## Probability to bootstrap to Bayesian models -->
<!-- Most textbooks would, at this stage, provide a more mathematical explanation of the transition we are making from Chapter \@ref(probability) to this chapter. In both Chapters \@ref(probability) and \@ref(one-parameter) we dealt with a discrete set of possible models. We began with examples in which there were only two or three possible "true" states of the world. You were either infected or not infected. There were either zero, one or two white marbles in the bag. These examples grew more and more complex, both by increasing the number of models under consideration and by increasing the number of possible outcomes of the experiment. In the case of the urn, there were 2,401 possible models: either zero or one or two or . . . 2,400 red beads in the urn.  -->
<!-- The transition from a discrete set of possible models to an infinite set of possible models is mathematically complex but easy on the intuition. Just wave you hands, imagine lots more models, and invoke the aesthetic appeal of smoothness. In the case of height, there are an infinite number of possible models: average height of adult American men in 2010 could be 175, 175.1, 175.14, 175.148, 175.1482, and so on. There are an infinite number of possible values since height is continuous. Yet, almost miraculously, the same intuition applies.  -->
<!-- Let's use $\mu$ as the parameter for the unknown average height of all the adult men in America in 2010. This is exactly analogous to the parameter $p$ from Chapter \@ref(one-parameter), the proportion of red beans in the urn. The only difference is that there are an infinite number of values which $\mu$ might take. We restricted $p$ to only 2,401 possible values: $0$, $1/2400$, $2/2400$, ..., $2399/2400$, $1$.  -->
<!-- Although a bootstrap can create a posterior distribution, as above, there are much simpler ways to do so. The most common involves the function `stan_glm()` from the **rstanarm** library. Halfway through the book, we are now ready for our first full scale data science project. Let us be guided by the cardinal virtues. -->

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="one-parameter.html"><span class="header-section-number">6</span> One Parameter</a></div>
<div class="next"><a href="three-parameters.html"><span class="header-section-number">8</span> Three Parameters</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#two-parameters"><span class="header-section-number">7</span> Two Parameters</a></li>
<li>
<a class="nav-link" href="#wisdom-1"><span class="header-section-number">7.1</span> Wisdom</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#eda-for-nhanes"><span class="header-section-number">7.1.1</span> EDA for nhanes</a></li></ul>
</li>
<li><a class="nav-link" href="#justice-1"><span class="header-section-number">7.2</span> Justice</a></li>
<li><a class="nav-link" href="#courage-1"><span class="header-section-number">7.3</span> Courage</a></li>
<li>
<a class="nav-link" href="#temperance-1"><span class="header-section-number">7.4</span> Temperance</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#question-1"><span class="header-section-number">7.4.1</span> Question 1</a></li>
<li><a class="nav-link" href="#question-2"><span class="header-section-number">7.4.2</span> Question 2</a></li>
<li><a class="nav-link" href="#question-3"><span class="header-section-number">7.4.3</span> Question 3</a></li>
<li><a class="nav-link" href="#question-4"><span class="header-section-number">7.4.4</span> Question 4</a></li>
</ul>
</li>
<li><a class="nav-link" href="#coverage"><span class="header-section-number">7.5</span> Coverage</a></li>
<li><a class="nav-link" href="#outcomes"><span class="header-section-number">7.6</span> 0/1 Outcomes</a></li>
<li><a class="nav-link" href="#summary-7"><span class="header-section-number">7.7</span> Summary</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/PPBDS/primer/blob/master/07-two-parameters.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/PPBDS/primer/edit/master/07-two-parameters.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Preceptor’s Primer for Bayesian Data Science</strong>" was written by David Kane. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
