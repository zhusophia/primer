# Functions {#functions}




<!-- tibble(ID = 1:10) %>%  -->
<!--   mutate(height = map_dbl(ID, ~ my_sampler(nhanes, "height"))) %>%  -->
<!--   mutate(height_list = map(ID, ~ my_sampler(nhanes, "height", n = 10))) %>%  -->
<!--   mutate(avg_height = map_dbl(height_list, ~ mean(.))) -->
  
<!-- Plot histograms of average height with different n's. Can also do with standard deviation. -->

<!-- Lesson: Larger N means more stable estimates. -->

<!-- Perhaps there are three main parts to the chapter. The first is lists, list-columns, map functions and custom functions. We can cover all that with fairly simple examples. (Don't use N argument yet. Just solve the no NA problem.) Second, as discussed below, is explorations of measures of center and variability for distributions. (Means we need to extend my_sampler with n argument.) We can explore those things with our new tools. The third section plays the Guessing Game. Show them mean, median, sd, and mad.-->


<!-- Key concept to focus on is the idea of a distribution and draws therefrom. This connects to chapter 2, where we introduced distributions and to chapter 5, where we explain probability distributions. In particular, we should show various ways to measure center and variability. These are cool and important concepts. Is "center" the mean or the median or the trimmed mean or . . . Is "variability" the standard deviation, the mean absolute deviation, the standard deviation of the median absolute deviation or . . . There is nothing explicitly "statistical" here. We are just trying to connect English words to specific formulas. We might show the distributions for which these measures give the same answers and then some (mainly skewed) for which they don't. How to explore these concepts in the context of function writing? Perhaps by writing functions like my_mean() and my_median()? Or maybe focus on a function which calculates MAD SD? Then use list-columns to compare this function to other choices? -->

<!-- Lesson: If you have a really skewed distribution, then median is a more stable measure than mean, and MAD (median absolute difference -- which means absolute difference from the median) is a more stable measure than standard deviation. Results depends too much on who ended up in the sample, like Bill Gates. -->

<!-- Related: Teach random draws. We introduce random variables at the end of chapter 2. Do more with them here. For example, with a data set, calculated a mean and sd. Then do 1,000 draws from that. Then, plot the densities of the raw data and the simulated data on top of one another. -->

<!-- Standard errors, qua standard errors, are a tricky thing to teach. But what if, instead of teaching them as standard errors, we just introduce the formula as a simple way to approximate something which, when multiplied by 2, and then added/subtracted from the mean, gives you a pretty useful 95% confidence interval. Then we could do the same for MAD SD, and show how that works better for skewed variables! Here, "works better" means that, if you do it 1,000 times, it works better on average. No! Too much! -->

<!-- The main use case is the "Guessing Game." Imagine that you are I are using the kenya data set and the distance, pop_density or rv13 variables. (All of which are quite skewed. We might also experiment with the poverty variable, which is less skewed. But skewed variables make this more interesting.) -->

<!-- In the Guessing Game, you guess a number and I guess a number. The goal is to get closest to a random draw from all the values of distance (or whatever). We do this 1,000 times. Whoever gets closer more often, wins the Guessing Game.  -->

<!-- This is a useful exercise for several reasons. First, coding it up is a good practice for writing a function. Indeed, we might re-write the whole chapter to focus on this example. Second, once we have a function, which runs one iteration of the Game --- maybe it takes the data set (or maybe a vector of values?), my guess and your guess, and then returns the winner --- we can use list columns and map_functions to run it a thousand times. This is all more fun and more interesting that stupid max-minus-min functions. -->

<!-- Third, the Guessing Game gets at key statistical issues. There is a lot of meat there, all things that we will revisit many times. Some things to highlight, many of which can be put into the Guessing Game. -->

<!-- 1) Guess a formula rather than a number. Presumably, you and I get to see (all of?) the data before playing the game. In the simplest version, we look at the data and give you a number. (The mean and median both do well in this game, obviously.) But, in a more advanced form of the game, we might be allowed to provide a function. The goal of inference is to come up with a function which wins the prediction game.) -->

<!-- 2) Change the objective function. The simplest form of the Guessing Game just counts each contest separately. A more advanced for would give you a penalty which varies depending on how wrong you are. (This, obviously, is one way to think about minimizing the squared residuals.) This is very fun because there are many different penalty functions, each of which may lead to a different function winning the Guessing Game. -->

<!-- 3) The chapter climaxes with the following cool result: If you are minimizing the sum (or average) of the squared residuals, guessing the mean wins the Guessing Game. However, if you are minimizing the sum (or average) of the absolute values of the residuals, you should guess the median. -->

<!-- 4) Another form of the Guessing Game is like a casino. One person (the Casino) gives a 50% confidence interval. The other person gets to pick either inside or outside. Then there is a draw. The Casino wins if the second person can't consistently win. -->

<!-- Key question: Do we explain standard error here? Could be easy! Calculate the mean. Calculate the sd. Divide and scale by the square root of n. Voila! But, still, I think No. But we should explain everything short of standard error.  -->

<!-- group_nest() might be a useful function.  -->

<!-- Might also show how to add lots of plots to a tibble and then pick out the ones that show cool stuff. If you run 1,000 experiments, show the plots of the 5 extreme ones. -->


<!-- Useful reference, maybe -->

<!-- https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/ -->
<!-- https://aosmith.rbind.io/2018/06/05/a-closer-look-at-replicate-and-purrr/ -->


## Introduction

A function is a piece of code that is packaged in a way that makes it easy to reuse. Functions make it easy for you to `filter()`, `arrange()`, `select()`, and create a `tibble()`, as you have seen in the last few chapters. Functions also allow you to transform variables and perform a bunch of useful mathematical calculations, such as finding the `min()`, `max()`, and `quantile()`s of a dataset. Functions are also important in generating data and values, such as using `rnorm()` to randomly generate a normal distribution.

Note that every time we discuss a function, we include the parentheses. This is because you call a function by including its parentheses and any necessary arguments in those parentheses. This is a correct call of `rnorm()`:

```{r}
rnorm(1)
```

If you run the function name without its parentheses, R will return the code that makes up the function. 

```{r}
rnorm
```

Functions can do all sorts of things. Below, `sample()` takes a vector of values and spits out a number of random values from that vector. You can specify the number of random values with the argument `size`. This is the equivalent of rolling a die!

```{r}
sample(x = 1:6, size = 1)
```

Functions can also take in other functions as arguments. For example, `replicate()` takes an expression and repeats it `n` times. What if we replicated the rolling of a die ten times?

```{r}
replicate(10, sample(1:6, 1))
```

An especially useful type of function is the family of `map_*` functions. `map_*` functions come from the **purrr** package, which you will have loaded if you have loaded **tidyverse**. These functions apply the same function to every row in a tibble.

```{r, include = FALSE}
library(tidyverse)
```

Let's create a tibble with 3 values: 3, 7, and 2.

```{r}
tibble(x = c(3, 7, 2))
```

What if we wanted to take the square root of each value?

```{r}
tibble(x = c(3, 7, 2)) %>% 
  mutate(new = map_dbl(x, sqrt))
```

`map_dbl()` (pronounced "map-double") took the function `sqrt()` and applied it to each element of `x`. Note that, when we passed the function `sqrt()` to `map_dbl()`, we passed in just its name, without the closing parentheses. This code fails:

```{r error=TRUE}
tibble(x = c(3, 7, 2)) %>% 
  mutate(new = map_dbl(x, sqrt()))
```

`sqrt()` with the parentheses is a call to the function. So, the first thing R tries to do is to run the function. Doing so fails because `sqrt()` requires an argument `x`, which is empty. 


*Lesson*: When passing a function in to `map_*` functions, pass just the name.


We called these `map_*` *functions* (plural) before.  If you know the expected output of your function, you can specify that kind of vector:

- `map()`: list  
- `map_lgl()`: logical
- `map_int()`: integer
- `map_dbl()`: double (numeric)
- `map_chr()`: character
- `map_df()`: data frame

So, since our example produces numeric output, we use `map_dbl()` instead of `map()`.

What's the difference between using `mutate()` and `map_*` functions? `map_*` functions are useful because of their ability to apply functions to every single element of a list, which `mutate()` cannot handle.

Speaking of lists, `map_*` functions are known to take in list inputs, and sometimes even create list-columns depending on the specific `map_*` function.

```{r}
tibble(x = c(3, 7, 2)) %>% 
  mutate(new = map(x, sqrt))
```

Notice that we changed the `map_dbl()` to `map()`, which returns a list-column. You can use `str` to easily view the contents of a list-column.

```{r}
tibble(x = c(3, 7, 2)) %>% 
  mutate(new = map(x, sqrt)) %>%
  str()
```

Only... what exactly is a list-column? Our next section will give you a quick refresher on lists, then dive into how lists, list-columns, and `map_*` functions relate.


## Lists and list-columns

Recall that a list is different from an atomic vector. Atomic vectors are familiar to us: each element of the vector has one value, and thus if an atomic vector is a column in your dataset, each observation gets a single value.  Lists, however, can contain vectors as elements.

```{r}
x <- list(c(3, 7, 2))
x
```

The above object, x, contains a numeric vector as an element. How do we extract the first element of the list?

```{r}
x[[1]][1]
```

Right! Recall the example from Chapter 2 in which lists are a pepper jar that contain more pepper packets. Don't forget the square brackets!

Let's index through more lists to make sure it works. Remember that lists can contain a multitude of data types. First, let's create a few new lists. Pay attention to the placement of `list()`.

```{r}
list_1 <- list("Three", 7, "Two")
list_2 <- c(list(3), list("7"), list(2))

list_1 %>% str()
list_2 %>% str()
x %>% str()
```

The two objects defined above are lists. They also display the data type of each element. `list_1` is a list of length 3, with a character, a numeric, and a character. `list_2` is also a list of length 3, with a numeric, a character, and a numeric. Note that the `[1:3]` denotes a numeric vector of length 3 for `x`.

Now, let's extract the first element of each of the new lists.

```{r}
list_1[[1]][1]
list_2[[1]][1]
```

You might be wondering, how is the list `x` different from the lists `list_1` and `list_2`? Besides the contents of the lists, we wanted to demonstrate how there are various ways to create lists. You can directly input values inside `list()`, or wrap the values in `c()` first to create a vector element.

There are a number of built-in R functions that output lists. For example, the ***ggplot***s you have been making store all of the graph information in lists.

Any function that returns multiple values can be used to create a list output. Consider the following list `x` again:

```{r}
x
```

If we take the `range()` of `x`, we expect two values to be returned: the minimum value and the maximum value in the numeric vector.

```{r}
range(x)
class(range(x))
```

The output of `range(x)` is a numeric vector with two values, 2 and 7. To turn the output into a list, simply wrap the line of code with `list()`. 

```{r}
list(range(x))
class(list(range(x)))
```

What if we want to create a tibble with the values of `range(x)`?

```{r}
tibble(col_1 = list(range(x)))
```

Notice this is a 1x1 tibble with one observation, which is a list of one element. The element is a vector of the integers 2 and 7. Voila! You have just created a *list-column**.

Lesson: If a function returns multiple values as a vector, like `range()` does, you can't use it directly to obtain a list-column, but you can wrap `list()` around it in order to get the same behavior.

A list column is a column of your data which is a [list](https://adv-r.hadley.nz/vectors-chap.html#lists) rather than an atomic vector.  Like with lists, you can pipe in `str()` to read the column more easily.

```{r message=FALSE}
tibble(col_1 = list(range(x))) %>%
  str()
```

Note that this is a case where it is crucial to use `tibble()`, not `data.frame()`!  If we had used `data.frame()` in the last example, it wouldn't have worked how we wanted:

```{r error=TRUE}
data.frame(col_1 = list(range(x)))
```

Note that the output is a 2 x 1 dataframe with double observations, not one singular list. 

<!-- DK: Rest of stuff is garbage? Think of a plot, and then make it. Motivate with the guessing game. -->

<!-- Start with: kenya %>% drop_na() %>% filter(rv13 != 0) %>% sample_n(10) %>% select(rv13) %>% as_vector() %>% mean() -->

<!-- Create function which does this: my_sample(x, var). (Do not include mean() at end).  -->

<!-- ID(tibble = 1) %>%  -->
<!--   mutate(my_samp = map(ID, ~ my_sampler(kenya, 10))) %>%  -->
<!--   mutate(center = map_dbl(my_samp, ~ mean())) -->
  
<!-- ID(tibble = 1:000) %>%  -->
<!--   mutate(my_samp = map(ID, ~ my_sampler(kenya, 10))) %>%  -->
<!--   mutate(center = map_dbl(my_samp, ~ mean()))   -->

<!-- Do it for mean and for median, and the plot histogram of their overlapping densities, and show that the median is a much more stable measure. -->

Wrapping a function with `list()` is often how we will go about creating list columns.  Let's practice with the `kenya` dataset. How could we add a column to the dataset that included the quantiles of the `poverty` variable?

First, we load the necessary **primer.data** library, select the relevant variables, and group by `block`. We are grouping because we are curious as to how the poverty rates look when aggregated by block, rather than as individual observations.

```{r}
library(primer.data)
kenya %>%
  select(block, poverty) %>%
  group_by(block)
```

Next, we will create a list-column by wrapping `quantile()` with `list()`. `quantile()` naturally produces a numeric vector of the quantiles of `poverty`, and wrapping with `list()` will capture this numeric vector as a list.

```{r}
library(primer.data)
kenya %>%
  select(block, poverty) %>%
  group_by(block) %>%
  mutate(poverty_quantile = list(quantile(poverty)))
```

Or let's say that we wanted 1) to subset the dataset to the those with `treatment == "control"`, 2) group by `block`, and 3) get a `summary()` of the `distance` variable by continent:

```{r, message=FALSE}
kenya %>%
  select(treatment, block, distance) %>%
  filter(treatment == "control") %>%
  group_by(block) %>%
  summarize(dist_summary = list(summary(distance)),
            .groups = "drop")
```

### Using `map_*` functions to create list-columns

For this example, we will use the `nhanes` dataset from the **primer.data** package to demonstrate how to use `map_*` functions to create list columns.

First, let's wrangle the data so each observation is grouped by age and gender.  We'll create a list column `all_weights` that consists of every subject's weight, grouped by age and gender.

```{r}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight), 
            .groups = "drop")
```

Now that we have a list column, we can use it as the input to `map()`, outputting another list column.  Let's say we wanted a new list column, `log_weights`, which takes the logarithmic value of each weight observation.

```{r}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight),
            .groups = "drop") %>%
  mutate(log_weights = map(all_weights, log))
```

Note that we took the list column `all_weights` and, by applying an anonymous function to it with `map()`, created another list column `log_weights`.  This is a very common process.  It is similar to taking a tibble and piping it into a `dplyr` function (such as `mutate()`) which gives you a new tibble that you can work with.

If we try to replace the `map()` function in the last line with a simple `mutate()`, we will get an error.

```{r, error = TRUE}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight),
            .groups = "drop") %>%
  mutate(log_weights = log(all_weights))
```

Again, this is because the `mutate()` function is ill-equipped to handle lists. We could have avoided creating the list-columns in the first place and found the log weight of each observation, but then we would be losing out on the grouping ability of lists. Do you see how `map_*` functions are equipped to handle inputs different from `mutate()`?

You can also use `map_*` functions to take a list column as an input and return an atomic vector -- a column with a single value per observation -- as an output.  For instance, let's say we now wanted the mean of the log weights:

```{r}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight),
            .groups = "drop") %>%
  mutate(log_weights = map(all_weights, log),
         mean_log_weights = map_dbl(log_weights, mean, na.rm = TRUE))
```

Here, we also see that the `map_*` functions have the `...` argument, which allows `na.rm = TRUE` to be passed along to `mean()`.
What if we wanted to extract the top five log weights per row?

```{r}
nhanes %>%
  group_by(age, gender) %>%
  summarize(all_weights = list(weight),
            .groups = "drop") %>%
  mutate(log_weights = map(all_weights, log),
         sorted_log_weights = map(log_weights, 
                                  ~ sort(., decreasing = TRUE)),
         top5_log_weights = map(sorted_log_weights, ~ .[1:5]))
```

See how we chained the `map_*` functions:

1) `all_weights` was used as the input to `map()` to create `log_weights`
2) `log_weights` was used as the input to `map()` to create `sorted_log_weights`
3) `sorted_log_weights` was used as the input to `map()` to create `top5_log_weights`

Until now, we have practiced using map functions and built-in R functions such as `log()` and `sort()` to create list columns. Next, we will show you how to write your very own functions!

## Custom Functions

### Anonymous functions with `map_*` functions

We can create functions that do operations "on the fly" without bothering to give them a name. These nameless functions are called [anonymous functions.](https://coolbutuseless.github.io/2019/03/13/anonymous-functions-in-r-part-1/)

You can use anonymous functions in conjunction with the `map_*` family of functions. They're commonly used to conduct mathematical operations repeatedly.

You can call an anonymous function using a `~` operator and then using a `.` to represent the current element.

```{r}
tibble(old = c(3, 7, 2)) %>% 
  mutate(new = map_dbl(old, ~ (. + 1)))
```

Note that the parentheses are not necessary. As long as everything after the `~` works as R code, the anonymous function should work, each time replace the `.` with the value of the `.x` variable --- which is `old` in this case --- with its value in that row.

```{r}
tibble(old = c(3, 7, 2)) %>% 
  mutate(new = map_dbl(old, ~ . + 1))
```

The `~ ` shorthand is very convenient once you get used to it.  Let's see it in an example. We'll use the `weather` dataset in the `nycflights13` package. Let's say that we want to return a list-column with all the recorded temperatures in a day in Celsius and basic summary statistics.

First, let's wrangle the data so each observation is a day rather than an hour. We will do so by selecting for the necessary variables and grouping by `origin`, `year`, `month`, and `day`.

```{r echo=FALSE}
library(nycflights13)
```

```{r eval=FALSE}
weather %>%
  select(origin, year, month, day, temp) %>%
  group_by(origin, year, month, day)
```

Next, we'll create a list column `temps_F` that consists of all the temperatures recorded that day at a particular origin. Note that the temperatures are Fahrenheit by default.

```{r, message=FALSE}
weather %>%
  select(origin, year, month, day, temp) %>%
  group_by(origin, year, month, day) %>%
  summarize(temps_F = list(c(temp)),
            .groups = "drop")
```

Now that we have a list-column, we can use it as the input to `map()`, outputting another list-column.  Let's say we wanted a new list-column, `temps_C`, which records the temperature in Celsius.

The formula to convert from Fahrenheit to Celsius is $\frac{(F-32)*5}{9}$. Let's "eyeball" the results using `str()`.

```{r, message=FALSE}
weather %>%
  select(origin, year, month, day, temp) %>%
  group_by(origin, year, month, day) %>%
  summarize(temps_F = list(c(temp)),
            .groups = "drop") %>%
  mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9))
```

Let's "eyeball" the first few results using `str()`.

```{r, message=FALSE}
weather %>%
  select(origin, year, month, day, temp) %>%
  group_by(origin, year, month, day) %>%
  summarize(temps_F = list(c(temp)), 
            .groups = "drop") %>%
  mutate(temps_C = map(temps_F, ~ (. - 32) * 5/9)) %>%
  ungroup() %>%
  slice(1:5) %>%
  str()
```

That was an anonymous function! As you can see, anonymous functions used in a `map_*` function context cannot get too complex. The next section will teach you how to write your own custom functions with flexibility.

### Creating your own functions

There are plenty of built-in functions in R, such as the ones mentioned above. You can also create your own custom functions, which may look something like this:

```{r eval=FALSE}
add_one_and_one <- function() {
  1 + 1
}
add_one_and_one()
```

You just created a function! This function will return `1+1` whenever called. 

What if we wanted to leave some mystery in the function? That is to say, we want to add the number 6 to a value `x`, that the user provides for us. 

```{r}
add_six_to_something <- function(x){
  x + 6
}
add_six_to_something(1)
```

Congratulations! You have incorporated your first **formal argument**. Formal arguments in functions are additional parameters that allow the user to customize the use of your function. Instead of adding 1+1 over and over again, your function takes in a number `x` that the user defines and adds 6. Now let's drive it home and make a function with *two* formal arguments.

```{r}
add_x_to_y <- function(x,y) {
  x + y
}

add_x_to_y(1,2)
add_x_to_y(4,3)
```

Great work! Now let's create an example that we will apply to the `kenya` dataset.

What if we were interested in taking the difference between the highest poverty rate and the lowest poverty rate? First, develop some working code for interactive use, using a representative input. Use the `poverty` variable. Built-in R functions that will be useful: `min()`, `max()`, `range()`.

```{r}
min(kenya$poverty)
max(kenya$poverty)
range(kenya$poverty)

max(kenya$poverty) - min(kenya$poverty)
range(kenya$poverty)[2] - range(kenya$poverty)[1]
diff(range(kenya$poverty))
```

Internalize this "answer" because our informal testing relies on you noticing departures from this.

### Skateboard >> perfectly formed rear-view mirror

This image --- widely attributed to the Spotify development team --- conveys an important point.

```{r echo = FALSE, out.width = "60%", fig.align='center', fig.cap = "From [Your ultimate guide to Minimum Viable Product (+great examples)](https://blog.fastmonkeys.com/2014/06/18/minimum-viable-product-your-ultimate-guide-to-mvp-great-examples/)"}
knitr::include_graphics("03-functions/images/mvp.jpg")
```

Build that skateboard before you build the car or some fancy car part. A limited-but-functioning thing is very useful. It also keeps spirits high.

This is related to the valuable Telescope Rule:

> It is faster to make a four-inch mirror and then a six-inch mirror than it is to make a six-inch mirror.

### Turn the working interactive code into a function

Add NO new functionality! Just write your very first R function.

```{r}
max_minus_min <- function(x) max(x) - min(x)
max_minus_min(kenya$poverty)
```

Check that you're getting the same answer as you did with your interactive code. Test it eyeball-to-metrically at this point.

One thing to note about functions is that functions have selective memory. In fact, functions only "remember" what happens within the function itself.

For example, I have saved value `x` as 1.

```{r}
x <- 1
```

Defining `x` inside a function will not change its value outside of the function.

```{r}
function(x) x <- 2

x
```

### Test on new inputs

Pick some new artificial inputs where you know (at least approximately) what your function should return.

```{r}
max_minus_min(1:10)
max_minus_min(runif(1000))
```

I know that 10 minus 1 is 9. I know that random uniform [0, 1] variates will be between 0 and 1. Therefore max - min should be less than 1. If I take LOTS of them, max - min should be pretty close to 1.

It is intentional that I tested on integer input as well as floating point. Likewise, I like to use valid-but-random data for this sort of check.

#### Test on real data but *different* real data

Back to the real world now. Two additional quantitative variables are lying around: `distance` and `rv13`. Let's have a go.

```{r}
max_minus_min(kenya$distance)
max_minus_min(kenya$rv13)
```

Either check these results "by hand" or apply the "does that even make sense?" test.

#### Test on weird stuff

Now we try to break our function. Don't get truly diabolical (yet). Just make the kind of mistakes you can imagine making at 2am when, 3 years from now, you rediscover this useful function you wrote. Give your function inputs it's not expecting.

```{r error=TRUE}
max_minus_min(kenya)

# Hey, sometimes things "just work" on data.frames!

max_minus_min(kenya$treatment)

# Factors are kind of like integer vectors, no?

max_minus_min("eggplants are purple")

# I have no excuse for this one
```

How happy are you with those error messages? You must imagine that some entire __script__ has failed and that you were hoping to just `source()` it without re-reading it. If a colleague or future you encountered these errors, do you run screaming from the room? How hard is it to pinpoint the usage problem?


#### I will scare you now

Here are some great examples where the function __should break but it does not.__

```{r}
max_minus_min(kenya[c('poverty', 'distance', 'rv13')])
max_minus_min(c(TRUE, TRUE, FALSE, TRUE, TRUE))
```

In both cases, R's eagerness to make sense of our requests is unfortunately successful. In the first case, a tibble containing just the quantitative variables is eventually coerced into numeric vector. We can compute max minus min, even though it makes absolutely no sense at all. In the second case, a logical vector is converted to zeroes and ones, which might merit an error or at least a warning.

### Check the validity of arguments

For functions that will be used again -- which is not all of them! -- it is good to check the validity of arguments. This implements a rule from the Unix philosophy:

> Rule of Repair: When you must fail, fail noisily and as soon as possible.

#### `stopifnot()`

`stopifnot()` is the entry level solution. We use it here to make sure the input `x` is a numeric vector.

```{r error=TRUE}
mmm <- function(x) {
  stopifnot(is.numeric(x))
  max(x) - min(x)
}
mmm(kenya)
mmm(kenya$treatment)
mmm("eggplants are purple")
mmm(kenya[c('poverty', 'distance', 'rv13')])
mmm(c(TRUE, TRUE, FALSE, TRUE, TRUE))
```

And we see that it catches all of the self-inflicted damage we would like to avoid.

#### if then stop

`stopifnot()` doesn't provide a very good error message. The next approach is very widely used. Put your validity check inside an `if()` statement and call `stop()` yourself, with a custom error message, in the body.

```{r error=TRUE}
mmm2 <- function(x) {
  if(!is.numeric(x)) {
    stop('I am so sorry, but this function only works for numeric input!\n',
         'You have provided an object of class: ', class(x)[1])
  }
  max(x) - min(x)
}
mmm2(kenya)
```


**Non-programming uses for assertions**

Another good use of this pattern is to leave checks behind in data analytical scripts. If we were loading from file (vs. a stable data package), we might want to formalize our expectations about the number of rows and columns, the names and flavors of the variables, etc. This would alert us if the data suddenly changed, which can be a useful wake-up call in scripts that you re-run *ad nauseam* on auto-pilot or non-interactively.

In addition to a gratuitous apology, the error also contains two more pieces of helpful info:

* *Which* function threw the error.
* Hints on how to fix things: expected class of input vs actual class.

If it is easy to do so, we highly recommend this template: "you gave me THIS, but I need THAT".

The tidyverse style guide has a very useful [chapter on how to construct error messages](https://style.tidyverse.org/error-messages.html).
  
## Argument Specifications and Default Values

<!-- DK: Explain that slice() gets you a vector. -->

In this section, we create a new function, generalize it, and learn more technical details about R functions.

Our goal is to write a function that draws samples from datasets found in `PPBDS/data`. 

We will begin by creating a minimally viable function, `starter_sampler()`, which isolates a sample of variable `height` from the dataset `nhanes`. We start by selecting the variable we want, `height`, and then performing sample_n with a value of 1 to get one sample. Finally, we slice to isolate the result. 

```{r}
start_sampler <- function() {
  nhanes %>% 
    select(height) %>% 
    sample_n(1) %>% 
    slice()
}
```

```{r}
start_sampler()
```

That's interesting! However, it isn't very adaptable, is it? Let's add a formal argument `n`, which allows us to sample any number of `height`s from `nhanes`. 

```{r}
multiple_sampler <- function(n) {
  nhanes %>% 
    select(height) %>% 
    sample_n(n) %>% 
    slice()
}
```

Now, when we call our new function, `multiple_sampler`, we want to enter an argument for `n`. Let's try calling `n` = 10 to get 10 resulting heights.

```{r}
multiple_sampler(n = 10)
```

Great! Though we have inserted some flexibility in our function, it still isn't as useful as possible. What if we want to sample a variable other than `height`? Like last time, we will write a new function which adds the formal argument `var` for variable. This way, we can look at other variables within our dataset. 

```{r}
flexible_sampler <- function(var, n){
  nhanes %>% 
    select(var) %>% 
    sample_n(n) %>% 
    slice()
}
```

Let's test the variable `weight` instead of height. Recall that when we want a character variable, we place the variable inside quotes:

```{r}
flexible_sampler(var = "weight", n = 10)
```
Let's take it one step further. We will now completely optimize our function by inserting one more formal argument, `x`, which will allow us to sample from any dataset, not just `nhanes`.

<!-- DK: No slice(), I think. -->

```{r}
my_sampler <- function(x, var, n){
  x %>% 
    select(var) %>% 
    sample_n(n) %>% 
    slice()
}
```

Test our new function using the `kenya` dataset as `x`, `mean_age` as `var`, and 10 as our `n`. Feel free to test any number of datasets, variables, and n values using this sampler!

```{r}
my_sampler(x = kenya, var = "mean_age", n = 10)
```
 
Our function is nearly perfect! One issue we may run into, however, is an NA value in our results. How can we avoid this? Recall our very valuable friend `drop_na`, which removes any values of `NA` from our results. After selecting the variable we want to inspect, we should call `drop_NA`:

```{r}
my_sampler <- function(x, var, n){
  x %>% 
    select(var) %>% 
    drop_na() %>% 
    sample_n(n) %>% 
    slice()
}
```

We have now created our very first flexible, useful, customizable function! 

### Argument names: freedom and conventions

Understand the importance of argument names. We can name my arguments almost anything we like. Proof:

```{r}
multiple_sampler <- function(bob_ross) {
  nhanes %>% 
    select(height) %>% 
    sample_n(bob_ross) %>% 
    slice()
}
  
multiple_sampler(bob_ross = 3)
```

While we *can* name our argument after a famous painter, it's usually a bad idea. Take all opportunities to make things more self-explanatory via meaningful names.

If you are going to pass the arguments of your function as arguments of a built-in function, consider copying the argument names. Unless you have a good reason to do your own thing (some argument names are bad!), be consistent with the existing function. Again, the reason is to reduce your cognitive load. 

We took this detour so you could see there is no *structural* relationship between our argument (`n`) and that of `sample()` (which also includes `n`). The similarity or equivalence of the names __accomplishes nothing__ as far as R is concerned; it is solely for the benefit of humans reading, writing, and using the code. Which is very important!

### Default values: freedom to NOT specify the arguments

What happens if we call our function but neglect to specify the probabilities?

```{r error=TRUE}
my_sampler()
```

Oops! At the moment, this causes a fatal error. It can be nice to provide some reasonable default values for certain arguments. Setting `n = 1` is a reasonable default value.

```{r}
multiple_sampler <- function(n = 1) {
  nhanes %>% 
    select(height) %>% 
    sample_n(n) %>% 
    slice()
}
```

Again we check how the function works by specifying `n` and not specifying `n`.

```{r}
multiple_sampler()
multiple_sampler(10)
```

Note that you do not necessarily need to supply the argument name. R tries its best to understand you based on the order you provide your argument values in. However, it is usually best practice to specify the argument name.


  
## Formal testing

In this section, we tackle `NA`s, the special argument `...` and formal testing.

### Use testthat for formal unit tests

Until now, we've relied on informal tests such as trying to break our function with silly inputs. If you are going to use a function a lot, especially if it is part of a package, it is wise to use formal unit tests.

The [testthat][testthat-web] package ([CRAN][testthat-cran]; [GitHub][testthat-github]) provides excellent facilities for this, with a distinct emphasis on automated unit testing of entire packages. However, we can take it out for a test drive even with our one measly function.

We will construct a test with `test_that()` and, within it, we put one or more *expectations* that check actual against expected results. You simply harden your informal, interactive tests into formal unit tests. Here are some examples of tests and indicative expectations.

Let's keep our very first function around as a baseline.

```{r}
add_dice <- function(n = 1) {
  stopifnot(is.numeric(n))
  stopifnot(n >= 0)
  sum(sample(1:6, n, replace = TRUE))
}
```

```{r, message=FALSE}
library(testthat)

test_that('invalid args are detected', {
  expect_error(add_dice("desserts are great!"))
  expect_error(add_dice(kenya))
})
```

No news is good news! Let's see what test failure would look like. Let's revert to a version of our function that does not check for a positive `n` value and test it. We can watch it fail.

```{r error=TRUE}
add_dice <- function(n = 2) {
  stopifnot(is.numeric(n))
  sum(sample(1:6, n, replace = TRUE))
}

show_failure(add_dice(-1))
```

We recommend you use unit tests to monitor the behavior of functions you (or others) will use often. If your tests cover the function's important behavior, then you can edit the internals freely. You'll rest easy in the knowledge that, if you broke anything important, the tests will fail and alert you to the problem. 

### What a function returns

By default, a function returns the result of the last line of the body. We are just letting that happen with the line `sum(sample(1:6, n, replace = TRUE))`. However, there is an explicit function for this: `return()`. We could just as easily make this the last line of my function's body:

```{r echo = FALSE}
add_dice <- function(n = 2) {
  stopifnot(is.numeric(n))
  stopifnot(n >= 0)
  sum(sample(1:6, n, replace = TRUE))
}
```

```{r eval=FALSE}
return(sum(sample(1:6, n, replace = TRUE)))
```

You absolutely must use `return()` if you want to return early based on some condition, i.e. before execution gets to the last line of the body. Otherwise, you can decide your own conventions about when you use `return()` and when you don't.

Right now, running `add_dice(5)` gives us an integer. What if we want the output to be a list of each roll outcome?

```{r}
show_dice <- function(n = 2) {
  stopifnot(is.numeric(n))
  stopifnot(n >= 0)
  return(list(sample(1:6, n, replace = TRUE)))
}
```

```{r}
show_dice(5)
```

Note that to achieve the above, we took out `sum()` and swapped it with `list()`. The new function, `show_dice()`, returns a list of `n` dice thrown.




## Part Two: Distributions?

Last chapter, we learned about distributions. The most important of these distributions is, of course, the normal distribution. 

This bell-shaped distribution is defined by two parameters: (1) the *mean* $\mu$ (spoken as "mu") which locates the center of the distribution and (2) the *standard deviation* $\sigma$ (spoken as "sigma") which determines the variation of values around that center. 

As we learned in Chapter 2, using `rbinom()`, a smaller sample does not tend to resemble the normal distribution. Larger samples, on the other hand, do tend to mirror the distinctive bell-shaped curve of normal distributions. 

<!-- Insert my_sampler examples using 10 samples, then 100, then 1000 -->

## Part Three: Guessing Game?

<!-- MF: Does the median versus mean discussion belong here? -->

Another interesting consideration in our models is the fact that the mean (or average) of our data might not be the most useful metric. Consider if we were looking at 1000 observations of income for citizens in the United States. Now, imagine that Bill Gates is one of those 1000 observations. The mean of our samples would be *much* higher than the actual figure of average income because of the significant outlier. 

What is the better metric, then? The median. The median, unlike the mean, is not skewed so greatly by outliers. Therefore, for the purposes of our data science observations, we want to look at the median value.

## Summary

<!-- Add summaries for all major sections of the chapter. This is just for the function section and needs more clarification. -->

<!-- MF: Should the summary include enough information that a student could get away with not reading the chapter? Or should it only be a tool of review for students who have read the chapter? -->

### Lists and list-columns

The introduction of the chapter was centered on a discussion of lists and list-columns. Key points from this section include:

- A list is different from an atomic vector. Atomic vectors are familiar to us: each element of the vector has one value, and thus if an atomic vector is a column in your dataset, each observation gets a single value.  Lists, however, can contain vectors as elements.
- There are various ways to create lists. We can directly input values inside `list()`, or wrap the values in `c()` first to create a vector element.
- Any function that returns multiple values can be used to create a list output. To turn the output into a list, simply wrap the line of code with `list()`.
- We can take a list column and, by applying an anonymous function to it with `map()`, create another list column. This is similar to taking a tibble and piping it into a `dplyr` function (such as `mutate()`) which gives you a new tibble that you can work with.
- You can also use `map_*` functions to take a list column as an input and return an atomic vector -- a column with a single value per observation -- as an output. 

### Functions

This chapter, we focused on functions. 

In particular, when building a function, you want to remember these key lessons:
- Optimize usefulness by adding more formal arguments when needed. A function that only gives an option for `n` may not be as helpful as a function that allows us to enter options for a dataset, variable, and n value. 
- As there is no *structural* relationship between our argument names (`n`) and that of `sample()` (which also includes `n`), ensure that argument names are sensible. This helps not only your readers, but also you. In short: using "bob_ross" to denote the number of samples you want... is a bad idea. 
- Make sure we account for possible values of NA in our functions. This can be accomplished by placing something like `drop_na()` as another line of code within the function's body, or by adding an `na.rm =` argument to our function. We might even enforce our preferred default -- but at least we're giving the user a way to control the behavior around `NA`s.
- By default, a function returns the result of the last line of the body.

### Distributions
