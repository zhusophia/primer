# Functions {#functions}

<!-- Key concept to focus on is the idea of a distribution and draws therefrom. This connects to chapter 2, where we introduced distributions and to chapter 5, where we explain probability distributions. In particular, we should show various ways to measure center and variability. These are cool and important concepts. Is "center" the mean or the median or the trimmed mean or . . . Is "variability" the standard deviation, the mean absolute deviation, the standard deviation of the median absolute deviation or . . . There is nothing explicitly "statistical" here. We are just trying to connect English words to specific formulas. We might show the distributions for which these measures give the same answers and then some (mainly skewed) for which they don't. How to explore these concepts in the context of function writing? Perhaps by writing functions like my_mean() and my_median()? Or maybe focus on a function which calculates MAD SD? Then use list-columns to compare this function to other choices? -->

<!-- Lesson: If you have a really skewed distribution, then median is a more stable measure than mean, and MAD (median absolute difference -- which means absolute difference from the median) is a more stable measure than standard deviation. Results depends too much on who ended up in the sample, like Bill Gates. -->

<!-- Related: Teach random draws. We introduce random variables at the end of chapter 2. Do more with them here. For example, with a data set, calculated a mean and sd. Then do 1,000 draws from that. Then, plot the densities of the raw data and the simulated data on top of one another. -->

<!-- Standard errors, qua standard errors, are a tricky thing to teach. But what if, instead of teaching them as standard errors, we just introduce the formula as a simple way to approximate something which, when multiplied by 2, and then added/subtracted from the mean, gives you a pretty useful 95% confidence interval. Then we could do the same for MAD SD, and show how that works better for skewed variables! Here, "works better" means that, if you do it 1,000 times, it works better on average. No! Too much! -->

<!-- The main use case is the "Guessing Game." Imagine that you are I are using the kenya data set and the distance, pop_density or rv13 variables. (All of which are quite skewed. We might also experiment with the poverty variable, which is less skewed. But skewed variables make this more interesting.) -->

<!-- In the Guessing Game, you guess a number and I guess a number. The goal is to get closest to a random draw from all the values of distance (or whatever). We do this 1,000 times. Whoever gets closer more often, wins the Guessing Game.  -->

<!-- This is a useful exercise for several reasons. First, coding it up is a good practice for writing a function. Indeed, we might re-write the whole chapter to focus on this example. Second, once we have a function, which runs one iteration of the Game --- maybe it takes the data set (or maybe a vector of values?), my guess and your guess, and then returns the winner --- we can use list columns and map_functions to run it a thousand times. This is all more fun and more interesting that stupid max-minus-min functions. -->

<!-- Third, the Guessing Game gets at key statistical issues. There is a lot of meat there, all things that we will revisit many times. Some things to highlight, many of which can be put into the Guessing Game. -->

<!-- 1) Guess a formula rather than a number. Presumably, you and I get to see (all of?) the data before playing the game. In the simplest version, we look at the data and give you a number. (The mean and median both do well in this game, obviously.) But, in a more advanced form of the game, we might be allowed to provide a function. The goal of inference is to come up with a function which wins the prediction game.) -->

<!-- 2) Change the objective function. The simplest form of the Guessing Game just counts each contest separately. A more advanced for would give you a penalty which varies depending on how wrong you are. (This, obviously, is one way to think about minimizing the squared residuals.) This is very fun because there are many different penalty functions, each of which may lead to a different function winning the Guessing Game. -->

<!-- 3) The chapter climaxes with the following cool result: If you are minimizing the sum (or average) of the squared residuals, guessing the mean wins the Guessing Game. However, if you are minimizing the sum (or average) of the absolute values of the residuals, you should guess the median. -->

<!-- 4) Another form of the Guessing Game is like a casino. One person (the Casino) gives a 50% confidence interval. The other person gets to pick either inside or outside. Then there is a draw. The Casino wins if the second person can't consistently win. -->

<!-- Key question: Do we explain standard error here? Could be easy! Calculate the mean. Calculate the sd. Divide and scale by the square root of n. Voila! But, still, I think No. But we should explain everything short of standard error.  -->

<!-- group_nest() might be a useful function.  -->

<!-- Might also show how to add lots of plots to a tibble and then pick out the ones that show cool stuff. If you run 1,000 experiments, show the plots of the 5 extreme ones. -->


## Introduction

A function is a piece of code that is packaged in a way that makes it easy to reuse. Functions make it easy for you to `filter()`, `arrange()`, `select()`, and create a `tibble()`, as you have seen in Chapters \@ref(visualization) and \@ref(wrangling). Functions also allow you to transform variables and perform mathematical calculations. We use functions like `rnorm()` and `runif()` to generate random draws from a distribution.

Note that every time we discuss a function, we include the parentheses. This is because you call a function by including its parentheses and any necessary arguments within those parentheses. This is a correct call of `rnorm()`:

```{r}
rnorm(n = 1)
```

If you run the function name without its parentheses, R will return the code that makes up the function. 

```{r}
rnorm
```

Functions can do all sorts of things. `sample()` takes a vector of values and returns a number of values randonly selected from that vector. You can specify the number of random values with the argument `size`. This call is the equivalent of rolling a die.

```{r}
sample(x = 1:6, size = 1)
```

Functions can also take in other functions as arguments. For example, `replicate()` takes an expression and repeats it `n` times. What if we replicated the rolling of a die ten times?

```{r}
replicate(10, sample(1:6, 1))
```

An especially useful type of function is the family of `map_*` functions. `map_*` functions come from the **purrr** package, which is automatically loaded with `library(tidyverse)`. These functions apply the same function to every row in a tibble.

```{r, message = FALSE}
library(tidyverse)
```

Let's create a tibble with one variable `x` which takes on three values: 3, 7, and 2.

```{r}
tibble(x = c(3, 7, 2))
```

It is easy to use mutate to create a new variable, `sq_root`, which is the square root of each value of x.

```{r}
tibble(x = c(3, 7, 2)) %>% 
  mutate(sq_root = sqrt((x)))
```

`map_*` functions provide another approach. A `map_*` function takes two required arguments. First is the object over which you want to iterate. This will generally be a column in the tibble you are working in. Second is the function which you want to run for each row in the tibble.

```{r}
tibble(x = c(3, 7, 2)) %>% 
  mutate(sq_root = map_dbl(x, ~ sqrt(.)))
```

`map_dbl()` (pronounced "map-double") took the function `sqrt()` and applied it to each element of `x`. There are two tricky parts to the use of map_* functions. First, you need to put the tilde symbol --- the "~" --- before the call to the function. Second, you need to include a period --- the "." --- in the spot where the variable goes.

We called these `map_*` functions (plural) before.  If you know the expected output of your function, you can specify that kind of vector:

- `map()`: list  
- `map_lgl()`: logical
- `map_int()`: integer
- `map_dbl()`: double (numeric)
- `map_chr()`: character
- `map_df()`: data frame

Since our example produces numeric output, we use `map_dbl()` instead of `map()`.

What's the difference between using `mutate()` and `map_*` functions? `map_*` functions are useful because of their ability to apply functions to every single element of a list, which `mutate()` cannot handle.

## List-columns and map functions

Recall that a list is different from an atomic vector. In atomic vectors, each element of the vector has one value.  Lists, however, can contain vectors, and even more complex objects, as elements.

```{r}
x <- list(c(3, 7, 2))
x
```

The above object, x, contains a numeric vector as an element. How do we extract the first element of the list?

```{r}
x[[1]][1]
```

<!-- BG: I removed the "recall pepper packet analogy" because it no longer exists in Ch. 2 -->

There are a number of built-in R functions that output lists. For example, the ***ggplot***s you have been making store all of the plot information in lists.

Any function that returns multiple values can be used to create a list output by wrapping those values with `list()`.

```{r}
x <- rnorm(10)

# range() returns the min and max of the argument 

range(x)

tibble(col_1 = list(range(x))) 
```

Notice this is a 1x1 tibble with one observation, which is a list of one element. Voila! You have just created a *list-column**.

*If a function returns multiple values as a vector, like `range()` does, you must use `list()` as a wrapper if you want to create a list-column.*

A list column is a column of your data which is a [list](https://adv-r.hadley.nz/vectors-chap.html#lists) rather than an atomic vector.  Like with lists, you can pipe in `str()` to read the column more easily.

```{r message=FALSE}
tibble(col_1 = list(range(x))) %>%
  str()
```


Let's practice with the `nhanes` dataset. How could we add a column to the dataset that included the quantiles of the `height` variable for each `gender`?

First, we load the necessary **primer.data** library.

```{r}
library(primer.data)
```

Then, select the relevant variables, and group by `gender`. We are grouping because we are curious as to how `height` is distributed in between `gender`. We drop any rows with missing data.

```{r}
nhanes %>%
  select(gender, height) %>%
  drop_na() %>% 
  group_by(gender)
```

<!-- BG: Here it says we use list() to wrap quantile(), but map() does that in the code below. Is it supposed to use list() here? -->

Next, we will create a list-column by wrapping `quantile()` with `list()`. `quantile()` naturally produces a numeric vector of the quantiles of `height`, and wrapping with `list()` will capture this numeric vector as a list.

```{r}
tmp <- nhanes %>%
  select(gender, height) %>%
  drop_na() %>% 
  group_by(gender) %>% 
  mutate(height_quantiles = map(height, ~ quantile(.)))

tmp
```

Take a look at the values for `height_quantiles`:

```{r}
tmp %>% 
  select(gender, height_quantiles) %>% 
  slice(1,5) %>% 
  str()
```

<!-- DK: This is confusing! Cut it? Or explain better what a list column actually looks like? One or the other! -->

Men are taller than women, except at the very bottom of the height distribution, which includes children.

We can use `map_*` functions to both create a list-column and then, much more importantly, work with that list-column afterwards. Example:


```{r}
tibble(ID = 1:3) %>% 
  mutate(draws = map(ID, ~ rnorm(10))) %>% 
  mutate(max = map_dbl(draws, ~ max(.))) %>% 
  mutate(range = map(draws, ~ range(.)))
```

This flexibility is only possible via the use of list-columns and  `map_*` functions.

Until now, we have practiced using `map_*` functions with built-in R functions. Next, we will show you how to write your very own functions!

## Custom Functions

### Anonymous functions with `map_*` functions

We can create functions that do operations "on the fly" without bothering to give them a name. These nameless functions are called [anonymous functions.](https://coolbutuseless.github.io/2019/03/13/anonymous-functions-in-r-part-1/)

You can use anonymous functions in conjunction with the `map_*` family of functions. They're commonly used to conduct mathematical operations repeatedly.

You can call an anonymous function using a `~` operator and then using a `.` to represent the current element.

```{r}
tibble(old = c(3, 7, 2)) %>% 
  mutate(new = map_dbl(old, ~ (. + 1)))
```

Note that the parentheses are not necessary. As long as everything after the `~` works as R code, the anonymous function should work, each time replace the `.` with the value of the relevant value of the `.x` variable --- which is `old` in this case --- with its value in that row.

```{r}
tibble(old = c(3, 7, 2)) %>% 
  mutate(new = map_dbl(old, ~ . + 1))
```

### Creating your own functions

There are plenty of built-in functions in R, such as the ones mentioned above. You can also create your own custom functions, which may look something like this:

```{r}
add_one_and_one <- function(){
  1 + 1
}

add_one_and_one()
```

You just created a function! This function will return `1 + 1` whenever called. 
What if we wanted to leave some mystery in the function? That is to say, we want to add the number 6 to a value `x`, that the user provides for us. 

```{r}
add_six_to_something <- function(x){
  x + 6
}

add_six_to_something(x = 1)
```

Congratulations! You have incorporated your first **formal argument**. Formal arguments in functions are additional parameters that allow the user to customize the use of your function. Instead of adding `1 + 1` over and over again, your function takes in a number `x` that the user defines and adds 6. Now let's drive it home and make a function with *two* formal arguments.

```{r}
add_x_to_y <- function(x, y) {
  x + y
}

add_x_to_y(1, 2)
add_x_to_y(4, 3)
```


### Skateboard >> perfectly formed rear-view mirror

This image --- widely attributed to the Spotify development team --- conveys an important point.

```{r echo = FALSE, out.width = "60%", fig.align='center', fig.cap = "From [Your ultimate guide to Minimum Viable Product (+great examples)](https://blog.fastmonkeys.com/2014/06/18/minimum-viable-product-your-ultimate-guide-to-mvp-great-examples/)"}
knitr::include_graphics("03-functions/images/mvp.jpg")
```

Build that skateboard before you build the car or some fancy car part. A limited-but-functioning thing is very useful. It also keeps spirits high.

This is related to the valuable Telescope Rule:

> It is faster to make a four-inch mirror and then a six-inch mirror than it is to make a six-inch mirror.


## `no_NA_sampler()`

Assume that we want to sample 10 observations for `height` from the `nhanes` tibble from the **primer.data** package. That is easy to do with the built in function `sample()`.

```{r}
sample(nhanes$height, size = 10)
```

One problem with this approach is that it will sample missing values of `height`. We can avoid that by manipulating the vector inside of the call to `sample()`.

```{r}
sample(nhanes$height[! is.na(nhanes$height)], size = 10)
```

That works, but, first, it is ugly code. And, second, it is hard to extend when we have more constraints. For example, assume we only want to sample from individuals who have no missing values for any variables, not just `height`. To do that, we really ought to make a custom function. Call that function `no_NA_sampler()`.

The first step in function creation is to write code in a normal pipe which does what you want the function to do. In this case, that code would look like:

```{r}
nhanes %>% 
  drop_na() %>%
  sample_n(10) %>% 
  pull(height)
```

We start with `nhanes`, use `drop_na()` to remove rows with missing values for any variable, sample 10 rows at random and then pull out `height`. To turn this into a function, we just need to copy/paste this pipe within the body of our function definition:

```{r}
no_NA_sampler <- function(){
  nhanes %>% 
    drop_na() %>%
    sample_n(10) %>% 
    pull(height)
}

no_NA_sampler()
```

Voila! A function just executes the code within its body. The first step in building a function is not to write the function. It is to write the code which you want the function to execute.

The first version, however, "hard codes" a lot of options which we might want to change. What if we want to sample 5 values of height or 500? In that case, we could hard code a new number in place of "10". A better option would be to add an argument so that we can pass in whatever value we want.

```{r}
no_NA_sampler <- function(n){
  nhanes %>% 
    drop_na() %>%
    sample_n(n) %>% 
    pull(height)
}

no_NA_sampler(n = 2)
no_NA_sampler(n = 25)
```

What if we want to sample from a different variable than `height` or from a different tibble than `nhanes`? Again, the trick is to turn hard coded values into arguments. The argument `tbl` is a placeholder for a data set, `n` for the number of samples you want extracted from your data set, and `var` for the variable in the samples that we are studying.

```{r}
no_NA_sampler <- function(tbl, var, n){
  tbl %>% 
    drop_na() %>%
    sample_n(n) %>% 
    pull({{var}})
}

no_NA_sampler(nhanes, height, n = 2)
no_NA_sampler(trains, age, n = 5)
```

R does not know how to interpret something like `age` when it is passed in an argument. The double curly braces around `var` tell R, in essence, that `var` is a variable in the tibble created from sampling from our imputed data set (`tbl`). 

<!-- DK: I realize that the above is a lousy explanation. Feel free to change it completely. -->

Now that we have the function doing what we want, we should add some comments and some error checking.

```{r}
no_NA_sampler <- function(tbl, var, n){
  
  # Function for grabbing `n` samples from a variable `var` which lives in a
  # tibble `tbl`. 
  
  # I could not figure out how to check to see if `var` actually lives in tibble
  # in my error checking. Also, I don't like that I need to use is_double() as
  # the check on `n` even though I want `n` to be an integer.
  
  stopifnot(is_tibble(tbl))
  stopifnot(is_double(n))

  tbl %>% 
    drop_na() %>%
    
    # What happens if n is "too large"? That is, I need to think harder about a)
    # whether or not I am sampling with or without replacement and b) which I
    # should be doing.
    
    sample_n(size = n) %>% 
    pull({{var}})
}
```

## Prediction Game

Let's play a prediction game. Consider the `kenya` tibble from **primer.data**.

```{r}
kenya
```

The game is that we will pick a random value of `rv13`, which is the number of people who live in the vicinity of a polling station. You guess a number. I guess a number. The winner of the Prediction Game is the person whose guess is closest to the random value selected. Example:

```{r}
your_guess <- 500
my_guess <- 600

sampled_value <- no_NA_sampler(kenya, rv13, n = 1) 

your_error <- abs(your_guess - sampled_value)
my_error <- abs(my_guess - sampled_value)

if(your_error < my_error) cat("You win!")
if(your_error > my_error) cat("I win!")
```


Run this code in your R Console to try it out. It works! It is also sloppy and disorganized. *But the first step in writing good code is to write bad code*.

We don't want to play the Prediction Game just once. We want to do it thousands of times. Copy/pasting this code a thousand times would be stupid. Instead, we need a function. Just place the working code within a function definition, and Voila!

```{r}
prediction_game <- function(){
  your_guess <- 500
  my_guess <- 600
  
  sampled_value <- no_NA_sampler(kenya, rv13, n = 1) 
  
  your_error <- abs(your_guess - sampled_value)
  my_error <- abs(my_guess - sampled_value)
  
  if(your_error < my_error) cat("You win!")
  if(your_error > my_error) cat("I win!")
}
```

Other than the function definition itself, there are no changes. Yet, by creating a function, we can now easily run this multiple times.


```{r}
replicate(3, prediction_game())
```

The problem with this version is that we want `prediction_game()` to *return* a message about the winner. Right now, it returns nothing. It just prints the winner. Let's change that, and also allow for guesses to be passed in as an argument, along with the tibble and variable. We can leave `n` hard coded as 1 since, by definition, the Prediction Game is an attempt to guess one number, at least for now.

<!-- DK: Add some code comments, especially {{var}} -->



```{r}
prediction_game <- function(guesses, tbl, var){
  
  # This is telling our function to stop if we do not select two guesses 
  # in the format of c(guess 1, guess 2).
  
  stopifnot(all(is_double(guesses)))
  stopifnot(length(guesses) == 2)
  
  # This tells the function that the "guess" inputted first in the 
  # guesses is "your" guess, whereas the second input is "my" guess.
  
  your_guess <- guesses[1]
  my_guess <- guesses[2]
  
  # Use the function no_NA_sampler to draw a sample from a data set
  # of our choosing, with a {{var}} and n.
  
  sampled_value <- no_NA_sampler(tbl, {{var}}, n = 1) 
  
  # Subtract the sampled value obtained from no_NA_sampler from 
  # both of our guesses. 
  
  your_error <- abs(your_guess - sampled_value)
  my_error <- abs(my_guess - sampled_value)
  
  # If the difference between your guess and the sampled value is 
  # less than the difference between my guess and the sampled value
  # (meaning that your guess was closer to the truth), the function
  # returns the message "Guess, your_guess, wins!".
  
  if(your_error < my_error){ 
    return(paste("Guess", your_guess, "wins!"))
  }
  
  # If your error exceeds my error (meaning that your guess was
  # further than the truth than mine), the function prints the 
  # message "Guess, my_guess, wins!" 
  
  if(your_error > my_error){ 
    return(paste("Guess", my_guess, "wins!"))
  }
  
  # If we guess the same number, and our error rates are therefore
  # identical, we return the message "A tie!". 
  
  if(your_error == my_error){ 
    return("A tie!")
  }

}
```

```{r}
replicate(5, prediction_game(guesses = c(500, 600), kenya, rv13))
```


In general, we will want to store the results in a tibble, which makes later analysis and plotting easier.

```{r}
tibble(ID = 1:3) %>% 
  mutate(result = map_chr(ID, ~ 
                            prediction_game(guesses = c(500, 600),
                                            kenya, 
                                            rv13)))
```

Who wins the game the most if we play 1,000 times?

```{r, echo = FALSE}
set.seed(9)
```


```{r}
tibble(ID = 1:1000) %>% 
  mutate(result = map_chr(ID, ~ 
                            prediction_game(guesses = c(500, 600),
                                            kenya, 
                                            rv13))) %>% 
  ggplot(aes(result)) +
    geom_bar()
```

It is hardly surprising that 500 wins more often than 600 since the mean of `rv13` is `r mean(kenya$rv13)`. The mean seems like a pretty good guess! But it is not the best guess. 

To test whether the mean or the median is a better guess, we will use our created `prediction_game` function with the guesses of 442 (the median) and 539 (the mean) and plot the results. 

```{r}
tibble(ID = 1:1000) %>% 
  mutate(result = map_chr(ID, 
                          ~ prediction_game(c(442, 539),
                                            kenya,
                                            rv13))) %>% 
  ggplot(aes(result)) +
    geom_bar()
```

The mean is not a bad prediction. But the best prediction is (surprisingly?) the median, which is `r median(kenya$rv13)`.

#### Playing within a tibble

In other cases, it is more convenient to play portions of the Prediction Game within a tibble. Imagine that we are trying to guess the biggest value out of 10 random samples. 

```{r}
tibble(ID = 1:3, guess_1 = 800, guess_2 = 900) %>% 
  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10)))
```

We can now manipulate the `result` column and then see which prediction did better. Using the same structure as before, we subtract our guesses from the variable we were guessing; in this case, the biggest value in 10 random samples.  

```{r}
tibble(ID = 1:3, guess_1 = 800, guess_2 = 900) %>% 
  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) %>% 
  mutate(biggest = map_dbl(result, ~ max(.))) %>% 
  mutate(error_1 = abs(guess_1 - biggest)) %>% 
  mutate(error_2 = abs(guess_2 - biggest)) %>% 
  mutate(winner = case_when(error_1 < error_2 ~ "Guess one wins!",
                            error_1 > error_2 ~ "Guess two wins!",
                            TRUE ~ "A tie!"))
```

Run the test 1,000 times.

```{r}
tibble(ID = 1:1000, guess_1 = 800, guess_2 = 900) %>% 
  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) %>% 
  mutate(biggest = map_dbl(result, ~ max(.))) %>% 
  mutate(error_1 = abs(guess_1 - biggest)) %>% 
  mutate(error_2 = abs(guess_2 - biggest)) %>% 
  mutate(winner = case_when(error_1 < error_2 ~ "Guess one wins!",
                            error_1 > error_2 ~ "Guess two wins!",
                            TRUE ~ "A tie!")) %>% 
  ggplot(aes(winner)) +
    geom_bar()
```

Empirically, we see than 900 is a much better guess than 800.

#### Measures of the center of a distribution

Let's play a new game. Pretend that we don't have access to the entire `kenya` data set. Instead, someone is going to draw 25 values of `rv13` at random. Our job is to calculate the `mean()` and `median()` of each of those samples and explore which of the two measures is more "stable."

```{r}
tibble(ID = 1:3) %>% 
  mutate(samp = map(ID, ~ no_NA_sampler(kenya, rv13, 25))) %>% 
  mutate(median_rv13 = map_dbl(samp, ~ median(.))) %>% 
  mutate(mean_rv13 = map_dbl(samp, ~ mean(.)))         
```

As usual with right-skewed distributions, the median is smaller than the mean. Let's do this 1,000 times and look at the distribution of these estimates.

```{r, cache = TRUE}
tibble(ID = 1:1000) %>% 
  mutate(samp = map(ID, ~ no_NA_sampler(kenya, rv13, 25))) %>% 
  mutate(median_rv13 = map_dbl(samp, ~ median(.))) %>% 
  mutate(mean_rv13 = map_dbl(samp, ~ mean(.))) %>% 
  pivot_longer(cols = median_rv13:mean_rv13,
               names_to = "measure",
               values_to = "estimate") %>% 
  ggplot(aes(estimate, fill = measure)) +
    geom_histogram(alpha = 0.5, 
                   bins = 100, 
                   position = "identity") +
    labs(title = "Distribution of Measures of the Center",
         subtitle = "The median is a more stable measure than the mean",
         x = "Estimate",
         y = "Count") + 
    scale_x_continuous(labels = scales::number_format())
```


<!-- DK: Not even sure if this shows what we want! But finishing off with a chart like this is probably a good idea. -->







## Summary


### Lists and list-columns

The introduction of the chapter was centered on a discussion of lists and list-columns. Key points from this section include:

- A list is different from an atomic vector. Atomic vectors are familiar to us: each element of the vector has one value, and thus if an atomic vector is a column in your data set, each observation gets a single value.  Lists, however, can contain vectors as elements.
- There are various ways to create lists. We can directly input values inside `list()`, or wrap the values in `c()` first to create a vector element.
- Any function that returns multiple values can be used to create a list output. To turn the output into a list, simply wrap the line of code with `list()`.
- We can take a list column and, by applying an anonymous function to it with `map()`, create another list column. This is similar to taking a tibble and piping it into a `dplyr` function (such as `mutate()`) which gives you a new tibble that you can work with.
- You can also use `map_*` functions to take a list column as an input and return an atomic vector -- a column with a single value per observation -- as an output. 

### Custom functions

This chapter, we focused on functions. 

In particular, when building a function, you want to remember these key lessons:
- Optimize usefulness by adding more formal arguments when needed. A function that only gives an option for `n` may not be as helpful as a function that allows us to enter options for a data set, variable, and n value. 
- As there is no *structural* relationship between our argument names (`n`) and that of `sample()` (which also includes `n`), ensure that argument names are sensible. This helps not only your readers, but also you. In short: using "bob_ross" to denote the number of samples you want... is a bad idea. 
- Make sure we account for possible values of NA in our functions. This can be accomplished by placing something like `drop_na()` as another line of code within the function's body, or by adding an `na.rm =` argument to our function. We might even enforce our preferred default -- but at least we're giving the user a way to control the behavior around `NA`s.
- By default, a function returns the result of the last line of the body.
- When starting a function, remember that smaller steps are easier than trying to build everything in one motion. In general: start by writing the body, test the body in a basic function, and then add formal arguments.  
- Use double curly braces around `var`s, since R does not know how to interpret variables when passed in an argument. The double curly braces tell R that `var` is a variable in the tibble created by our function.  


### Distributions


- The word "distribution" can mean two things. First, it is an object --- a mathematical formula, an imaginary urn --- from which you can draw values. Second, it is a list of such values.   
- The two most important aspects of a distribution are its *center* and its *variability*.
- The median is often a more stable measure of the center than the mean. The mad (scaled median absolute deviation) is often a more stable measure of variation than the standard deviation. 
- Outliers cause a lack a stability. In a distribution without outliers, the mean/median and mad/sd are so close in value that it does not matter much which ones you use.


<!-- DK: We have not (yet?) talked about the stability of mad. We have barely address the stability of the median. Is there room to get into this? Even if not, simply asserting these facts in the Summary is probably a good idea! -->



