# N Parameters {#n-parameters}

<!-- Intro before Wisdom talks in generalities and asks the key question, but does not discuss data. Phrase the question as coming from a normal person. MF: Done. -->

<!-- Two goals this week: -->

<!-- Wisdom: Begin with the ideal Preceptor Table. Depends on the question. Might be all voters in Massachusetts. Might be just the Republican voters. This determines if we are causal or predictive.  Then look for data and explore that data. What must I assume about the population, to allow me to go forward? If the data is not "close enough" to my ideal Preceptor Table, I give up. Wisdom is about knowing when to quit. MF: Done.

Justice: Actual Preceptor Table. Then, representativeness (rows), validity (columns). Only the main functional form, with no variable names. Discussion of the error term. MF: Done. 

Courage: For each model, write down the math, estimate the model, say something sensible about the regression table. MF: done.

<!-- Replace hard coded numbers. --> 

<!-- Add a testing is nonsense section. --> 

<!-- * Should start with  single variable which actually requires lots of parameters. For example, death_age as a function of state and come up with a different average death_age for each state. This is cheating in that it is too many parameters, or rather that some of the parameters need to be estimated for too little data.  But it would allow us to explain a bunch of stuff! Including Bayesian shrinkage. Saving this till the end of the model causes us to miss stuff. But the rest of the analysis used lived_after. Which is better. Yes? Should we discuss? Use both? -->

<!-- Consider getting rid of state example, or moving it as a teaser to end of chapter 10. Could we make a similar point with an example from shaming? 

Temperance. Answer the question. Be aware of the limits to the accuracy of your answer. Preceptor's Posterior. Testing. All the cool themes. Bring it all together. Make the students say "Ahhh!" -->

<!-- p_00 = fcn_1(primary_00), should be p_00 = primary_00 == "Yes", -->

<!-- Use fit_ instead of model_. Think about other conventions you want enforce. -->

*This chapter is still a DRAFT.*

Having created models with one parameter in Chapter \@ref(one-parameter), two parameters in Chapter \@ref(two-parameters), three parameters in Chapter \@ref(three-parameters), four parameters in Chapter \@ref(four-parameters) and five parameters in Chapter \@ref(five-parameters), you are now ready to make the jump to $N$ parameters. 

In this chapter, we will consider models with many parameters and the complexities that arise therefrom. As our models grow in complexity, we need to pay extra attention to basic considerations like validity, population, and representativeness. It is easy to jump right in and start interpreting! It is harder, but necessary, to ensure that our models are really answering our questions. 

Imagine you are running for Governor and want to do a better job of getting your voters to vote. How can you encourage voters to go out to the polls on election day? 


## Wisdom

```{r echo = FALSE, fig.cap = "Wisdom"}
knitr::include_graphics("other/images/Wisdom.jpg")
```

As you research ways to increase voting, you come across a large-scale experiment showing the effect of sending out a voting reminder that "shames" citizens who do not vote. You are considering sending out a "shaming" voting reminder yourself. 

We will be looking at the `shaming` dataset from the **primer.data** package. This is dataset is from "Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment" by Gerber, Green, and Larimer (2008). [Check out the paper here](https://github.com/PPBDS/primer.data/blob/master/inst/papers/shaming.pdf). You can, and should, familiarize yourself with the data by typing `?shaming`. 

Recall our initial question: how can we encourage voters to go out to the polls on election day? We now need to translate this into a "data science" question. 

Our question:

*What is the causal effect, on the likelihood of voting, of different postcards on voters of different levels of political engagement?*


### EDA of `shaming`

Let's now do an EDA, starting off by running `glimpse()`.

```{r}
library(tidyverse)
library(primer.data)
library(rstanarm)
library(ggthemes)
library(ggdist)
library(gt)
library(janitor)
library(broom.mixed)
library(gtsummary)
glimpse(shaming)
```

`glimpse()` gives us a look at the raw data contained within the `shaming` data set. At the very top of the output, we can see the number of rows and columns, or observations and variables respectively. We see that there are 344,084 observations, with each row corresponding to a unique respondent. This summary provides an idea of some of the variables we will be working with. 

Variables of particular interest to us are `sex`, `hh_size`, and `primary_06`. The variable `hh_size` tells us the size of the respondent's household, `sex` tells us the sex of the respondent, and `primary_06` tells us whether or not the respondent voted in the 2006 Primary election. 
There are a few things to note while exploring this data set. You may -- or may not -- have noticed that the only response to the `general_04` variable is "Yes". In their published article, the authors note that "Only registered voters who voted in November 2004 were selected for our sample" (Gerber, Green, Larimer, 2008). After this, the authors found their history then sent out the mailings. Thus, non-registered voters are excluded from our data.

It is also important to identify the dependent variable and its meaning. In this shaming experiment, the dependent variable is `primary_06`, which is a variable coded either 0 or 1 for whether or not the respondent voted in the 2006 primary election. This is the dependent variable because the authors are trying to measure the **effect** that the treatments have on voting behavior in the 2006 general election.

<!-- HV: Should I include discussion of the left-hand variable (treatment?) here? Or wait until we move into the regressions? -->

We have not yet discussed the most important variable of them all: `treatment`. The `treatment` variable is a factor variable with 5 levels, including the control. Since we are curious as to how sending mailings affects voter turnout, the treatment variable will tell us about the impact each type of mailing can make. Let's start off by taking a broad look at the different treatments.

<!-- HV: Is it okay to say the first sentence of this paragraph? -->

```{r}
shaming %>%
  count(treatment)
```

Four types of treatments were used in the experiment, with voters receiving one of the four types of mailing. All of the mailing treatments carried the message, "DO YOUR CIVIC DUTY - VOTE!". 

The first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote." This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.

In the second treatment, Hawthorne, households received a mailing which told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure to the households receiving this mailing.

In the third treatment, Self, the mailing includes the recent voting record of each member of the household, placing the word "Voted" next to their name if they did in fact vote in the 2004 election or a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart" with the voting record of the household members after the 2006 primary. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.

The fourth treatment, Neighbors, provides the household members' voting records, as well as the voting records of those who live nearby. This mailing also told recipients, "we intend to mail an updated chart" of who voted in the 2006 election to the entire neighborhood.

### Ideal Preceptor Table

Recall the ideal Preceptor Table. What rows and columns of data do you need such that, if you had them all, the calculation of the number of interest would be trivial? If you want to know the average height of an adult in India, then the ideal Preceptor Table would include a row for each adult and a column for their height. 

One key aspect of this ideal Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. Mainly: do we need a causal model, one which estimates that attitude under both treatment and control? The Preceptor Table would require two columns for the outcome. In this case, where we are trying to see the **causal effect** of mailed voting reminders on voting. 

Are we are modeling (just) for prediction or are we (also) modeling for causation? Predictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model. Here, we **are** looking at causation. 

So, what would our ideal table look like? Assuming we are running for governor in the United States, we would ideally have data for every citizen of voting age. This means we would have approximately 200 million rows. 

Because there is no missing data in an ideal Preceptor Table, we would also know the outcomes under both treatment (receiving a reminder) and control (not receiving a reminder). Here is a sample row from our table:

```{r, echo = FALSE, fig.align = 'left'}
# First, we create a tibble with the values we want for the table

tibble(ID = "Citizen 1",
       ytreat = "Did vote",
       ycontrol = "Did not vote") %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(ID = md("ID"),
                ytreat = "Behavior in Treatment",
                ycontrol = "Behavior in Control") %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(ID)))  %>%
  tab_style(style = cell_text(align = "left", v_align = "middle"), 
            locations = cells_column_labels(columns = vars(ID))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  cols_align(align = "left", columns = vars(ID)) %>%
  tab_spanner(label = "Outcomes", columns = vars(ytreat, ycontrol))
```

This is a good start! However, we may want even more information in our ideal Preceptor Table. Perhaps a column for sex would be informative. A column for age? Political affiliation? In a perfect world, we would know all of these pieces of information. In a perfect world, we could measure the *exact causal effect* of voting reminders for different subsets of the US population.

We may also want to narrow our ideal Preceptor Table. If we are running for governor in Florida, we may only want to study citizens in Florida. If we are running as a Democrat, we may only want to study citizens who are registered Democrats. 

However, the main point of this exercise is to see *what we want to know* compared with *what we actually do know*. 

### Population

One of the most important components of Wisdom is the concept of the "population". Recall the questions we asked earlier:

*What will happen if we do send a voting reminder today? Will more voters show up to the polls? Additionally, on the day of the election, a female citizen is randomly selected. What is the probability she will vote?*

As we have discussed before, the population *is not* the set of people, or voters, for which we have data. This is the dataset. The population is the larger --- potentially much larger --- set of individuals about whom we want to make inferences. 

There are many different populations, each with its own $\mu$, in which we might be interested. For instance:

* The population for registered voters that voted in the 2004 election in the United States. This is the population for our dataset. 
* The population for registered voters that voted in any election from 1970 to 2040. We are often interested in the future. We want to make predictions about what *will* happen to voter habits, not what has already happened. 
* The population for registered **and** non-registered voters in the United States. Though it is helpful to look at registered voters, candidates also want to get non-registered voters to vote for them!
* The population for voters --- registered or not-registered --- around the world. We might expect the habits of Americans to be similar to the habits of people in other countries. We might also believe that this is too broad a group to make inferences about.
* Many more!

In this case, we are viewing the data from the perspective of someone running for Governor this year that wants to increase voter turnout. We want to increase turnout **now**, not for people voting in 2006! We also may want to increase turnout in those citizens who are not registered to vote, a group that is excluded from our dataset. Is it reasonable to generate conclusions for this group? Most likely, no. However, we have limited data to work with and we have to determine how far we are willing to generalize to other groups. 

It is a judgment call, a matter of Wisdom, as to whether or not that data we have is “close enough” to the population we are interested in to justify making a model.

Even though the original question is about “voters” in general, and does not specifically refer to specific states in which we might be interested, we will assume that the data we have for random voters is, uh, representative enough of the population we are interested in. If we did not believe that, then we should stop right now. *The major part of Wisdom is deciding what questions you can’t answer because of the data you just don’t have.*

<!-- DK: Add discussion of what you see here. No need to drop missing values since there aren't any. I think this next discussion can be dropped. -->

<!-- DK: Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 5 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. -->


## Justice 

```{r echo = FALSE, out.width="60%", fig.align='center', fig.cap = "Justice"}
knitr::include_graphics("other/images/Justice.jpg")
```

Justice emphasizes a few key concepts: 
* 1) The actual Preceptor Table, a structure which includes a row for every unit in the population – the data we want to have and the data which we actually have. 
2) Is our data representative of the population? 
3) Is the meaning of the columns consistent, i.e., can we assume validity? 

We then make an assumption about the data generating mechanism. 

### Actual Preceptor Table

Recall that in an actual Preceptor Table, we will have a bunch of missing data! We can not use simple arithmetic to calculate the causal effect of voting reminders on voting behavior. Instead, we will be required to estimate it. This is our estimand, a variable in the real world that we are trying to measure.An estimand is not the value you calculated, but is rather the unknown variable you want to estimate.

Let's build a basic visualization for the actual Preceptor Table for this scenario:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(subject = c("Citizen 1", "Citizen 23", "Citizen 40", "Citizen 53", "Citizen 80"),
       ytreat = c("Voted", "Did not vote", "?", "?", "Voted"),
       ycontrol = c("?", "?", "Voted", "Did not vote", "?"),
       ydiff = c("?", "?", "?", "?", "?")) %>%
  
  # Then, we use the gt function to make it pretty
  
  gt() %>%
  cols_label(subject = md("ID"),
                ytreat = md("Treatment"),
                ycontrol = md("Control"),
                ydiff = md("Treatment - Control")) %>%
  cols_move(columns = vars(ytreat, ycontrol), after = vars(subject)) %>%
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = vars(subject))) %>%
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = vars(subject))) %>%
  cols_align(align = "center", columns = TRUE) %>%
  cols_align(align = "left", columns = vars(subject)) %>%
  fmt_markdown(columns = TRUE)  %>%
  tab_spanner(label = "$$\\text{Outcomes}$$", vars(ytreat, ycontrol))  %>%
  tab_spanner(label = "$$\\text{Estimand}$$", vars(ydiff))
```


Here, there are two possible outcomes: did vote or did not vote. What we really want to know is the Average Treatment Effect (ATE) of the treatment, the voting reminder. We want to estimate how much the voting reminder impacts the odds of someone voting. 

Note that this is a simplified version of the actual Preceptor Table. In this dataset, we have a number of other columns that we know about each of our subjects: age, sex, past voting history. In an expanded actual Preceptor Table, these columns would be included. 

Now, how can we fill in the question marks? Because of the Fundamental Problem of Causal Inference, we can never know the missing values. Because we can never know the missing values, we must make assumptions. “Assumption” just means that we need a “model,” and all models have parameters.

To fill in these blanks, we need to perform some datascience. This part of the process is included in courage. Let's take a look!

<!-- Justice: Actual Preceptor Table. Then, representativeness (rows), validity (columns). Only the main functional form, with no variable names. Discussion of the error term. --> 

## Courage
```{r echo = FALSE, out.width="60%", fig.align='center', fig.cap = "Courage"}
knitr::include_graphics("other/images/Courage.jpg")
```


### Set-up

We start by creating a function named `fcn_1()` that takes in a vector of "Yes"/"No" responses and returns a binary integer vector where 1 replaces "Yes", 0 replaces "No", and anything else becomes `NA`. The function should alert the user if they are passing in a problematic vector, i.e. one that contains lowercase "yes" and/or "no". It will also alert the user if there are not at least once observation equal to "Yes". You may find the `case_when()` function to be useful. `NA` can take on different types depending on the classification of the vector. Read more on this at `?NA`. 

```{r fcn_1}
fcn_1 <- function(x){
  
  # In effort to prevent people from mis-applying this function, we want to 
  # force users to confront the precise state of their data. This function will
  # properly encode capitalized "Yes", capitalized "No" as 0, and will code 
  # anything else as NA. We check that there are no lowercase "yes" or "no" 
  # instances and that the column provided has at least 1 instance of the
  # properly formatted "Yes" so that there are no missed y/n values and to
  # ensure that the function is being usefully applied.
  
  # We can't check for at least 1 "No" due to the fact that the group was
  # selected such that everyone voted in the 04 general election, so checking
  # for at least 1 "Yes" will have to suffice.
  
  stopifnot((sum(x == "yes") == 0),
            (sum(x == "Yes")  > 0),
            (sum(x == "no")  == 0))
  
  # You can use an orphaned case_when here or you can assign to an object and 
  # return. It works both ways.
  
  case_when(x == "Yes" ~ as.integer(1),
            x == "No" ~ as.integer(0),
            TRUE ~ NA_integer_)}

```

Now, we will create an object named `object_1` that includes a 3-level factor classifying voters by level of civic engagement. 

* Using the function we just created, convert all primary and general election variables *that are not already 1/0 binary* to binary format. 

* Create a new column named `civ_engage` that sums up each person's voting behavior up to, but not including, the 2006 primary. 

* Create a column named `voter_class` that classifies voters into 3 bins: "Always Vote" for those who voted at least 5 times, "Sometimes Vote" for those who voted between 3 or 4 times, and "Rarely Vote" for those who voted 2 or fewer times. This variable should be classified as a factor.  

* Create a column called `z_age` which is the z-score for `age`. 


```{r object_1}
object_1 <- shaming %>% 
  
  # Converting the Y/N columns to binaries with the function we made note that
  # primary_06 is already binary and also that we don't need it to predict
  # construct previous voter behavior status variable.
  
  mutate(p_00 = fcn_1(primary_00),
         p_02 = fcn_1(primary_02),
         p_04 = fcn_1(primary_04),
         g_00 = fcn_1(general_00),
         g_02 = fcn_1(general_02),
         g_04 = fcn_1(general_04)) %>% 
  
  # A sum of the voting action records across the election cycle columns gives
  # us an idea (though not weighted for when across the elections) of the voters
  # general level of civic involvement.
  
  mutate(civ_engage = p_00 + p_02 + p_04 + 
                      g_00 + g_02 + g_04) %>% 
  
  # If you look closely at the data, you will note that g_04 is always Yes, so
  # the lowest possible value of civ_engage is 1. The reason for this is that
  # the sample was created by starting with a list of everyone who voted in the
  # 2004 general election. Note how that fact makes the interpretation of the
  # relevant population somewhat subtle.
  
  mutate(voter_class = case_when(civ_engage %in% c(5, 6) ~ "Always Vote",
                                 civ_engage %in% c(3, 4) ~ "Sometimes Vote",
                                 civ_engage %in% c(1, 2) ~ "Rarely Vote"),
         voter_class = factor(voter_class, levels = c("Rarely Vote", 
                                                      "Sometimes Vote", 
                                                      "Always Vote"))) %>% 
  
  # Centering and scaling the age variable. Note that it would be smart to have
  # some stopifnot() error checks at this point. For example, if civ_engage < 1
  # or > 6, then something has gone very wrong.
  
  mutate(z_age = as.numeric(scale(age))) %>% 
  select(primary_06, treatment, sex, civ_engage, voter_class, z_age)
```

Let's inspect our object: 

```{r}
object_1 %>% 
  slice(1:3)
```
Great! Now, we will create our first model: the relationship between `primary_06`, which represents whether a citizen voted or not, against sex and treatment. 

### primary_06 ~ treatment + sex


In this section, we will look at the relationship between primary voting and treatment + sex. 

*The math:*

Without variable names:

$$ y_{i} = \beta_{0} + \beta_{1}x_{i, 1} + \beta_{2}x_{i,2} ... + \beta_{n}x_{i,n} + \epsilon_{i} $$
With variable names: 

$$ y_{i} = \beta_{0} + \beta_{1}civic\_duty_i + \beta_{2}hawthorne_i + \beta_{3}self_i + \beta_{4}neighbors_i + \beta_{5}male_i + \epsilon_{i} $$

There are two ways to formalize the model used in `model_1`: with and without the variable names. The former is related to the concept of Justice as we acknowledge that the model is constructed via the linear sum of `n` parameters times the value for `n` variables, along  with an error term. In other words, it is a linear model. The only other model we have learned this semester is a logistic model, but there are other kinds of models, each defined by the mathematics and the assumptions about the error term. 
The second type of formal notation, more associated with the virtue Courage, includes the actual variable names we are using. The trickiest part is the transformation of character/factor variables into indicator variables, meaning variables with 0/1 values. Because `treatment` has 5 levels, we need 4 indicator variables. The fifth level --- which, by default, is the first variable alphabetically (for character variables) or the first level (for factor variables) --- is incorporated in the intercept.


Let's translate the model into code. 

```{r, cache=TRUE}
model_1 <- stan_glm(data = object_1,
                  formula = primary_06 ~ treatment + sex,
                  refresh = 0,
                  seed = 987)
```

```{r}
print(model_1, digits = 3)
```


We will now create a table that nicely formats the results of `fit_1` using the `tbl_regression()` function from the **gtsummary** package. It will also display the associated 95% confidence interval for each coefficient.

```{r, message = FALSE}
tbl_regression(model_1, 
               intercept = TRUE, 
               estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  
  # Using Beta as the name of the parameter column is weird.
  
  as_gt() %>%
  tab_header(title = md("**Likelihood of Voting in the Next Election**"),
             subtitle = "How Treatment Assignment and Age Predict Likelihood of Voting") %>%
  tab_source_note(md("Source: Gerber, Green, and Larimer (2008)")) %>% 
  cols_label(estimate = md("**Parameter**"))
```

Interpretation: 
* The intercept of this model is the expected value of the probability of someone voting in the 2006 primary given that they are part of the control group and are female. In this case, we estimate that women in the control group will vote ~`r round((coef(model_1)["(Intercept)"] * 100), 1)`% of the time. 
* The coefficient for `sexMale` indicates the difference in likelihood of voting between a male and female. In other words, when comparing men and women, the `r round(coef(model_1)["sexMale"], 3)` implies that men are ~`r round((coef(model_1)["sexMale"] * 100), 1)`%  more likely to vote than women. Note that, because this is a linear model with no interactions between sex and other variables, this difference applies to any male, regardless of the treatment he received. Because sex can not be manipulated (by assumption), we should not use a causal interpretation of the coefficient. 
* The coefficients of the treatments, on the other hand, do have a causal interpretation. For a single individual, of either sex, being sent the Self postcard increases your probability of voting by `r round((coef(model_1)["treatmentSelf"] * 100), 1)`%. It appears that the `Neighbors` treatment is the most effective at ~`r round((coef(model_1)["treatmentNeighbors"] * 100), 1)`% and `Civic Duty` is the least effective at ~`r round((coef(model_1)["treatmentCivic Duty"] * 100), 1)`%. 


### primary_06 ~ z_age + sex + treatment + voter_class + voter_class*treatment

It is time to look at interactions! Create another model named `fit_2` that estimates `primary_06` as a function of `z_age`, `sex`, `treatment`, `voter_class`, and the interaction between treatment and voter classification. 

The math: 
$$y_{i} = \beta_{0} + \beta_{1}z\_age + \beta_{2}male_i + \beta_{3}civic\_duty_i + \\ \beta_{4}hawthorne_i + \beta_{5}self_i + \beta_{6}neighbors_i + \\ \beta_{7}Sometimes\ vote_i + \beta_{8}Always\ vote_i + \\ \beta_{9}civic\_duty_i Sometimes\ vote_i + \beta_{10}hawthorne_i Sometimes\ vote_i + \\ \beta_{11}self_i Sometimes\ vote_i + \beta_{11}neighbors_i Sometimes\ vote_i + \\ \beta_{12}civic\_duty_i Always\ vote_i + \beta_{13}hawthorne_i Always\ vote_i + \\ \beta_{14}self_i Always\ vote_i + \beta_{15}neighbors_i Always\ vote_i + \epsilon_{i}$$
Translate into code: 

```{r fit_2, cache = TRUE}

fit_2 <- stan_glm(data = object_1,
                  formula = primary_06 ~ z_age + sex + treatment + voter_class + 
                            treatment*voter_class,
                  family = gaussian,
                  refresh = 0,
                  seed = 789)

```

```{r}
print(fit_2, digits = 3)
```

As we did with our first model, create a regression table to observe our findings: 

```{r table_2}
tbl_regression(fit_2, 
               intercept = TRUE, 
               estimate_fun = function(x) style_sigfig(x, digits = 3)) %>%
  as_gt() %>%
  tab_header(title = md("**Likelihood of Voting in the Next Election**"),
             subtitle = "How Treatment Assignment and Other Variables Predict Likelihood of Voting") %>%
  tab_source_note(md("Source: Gerber, Green, and Larimer (2008)")) %>% 
  cols_label(estimate = md("**Parameter**"))
```

Now that we have a summarized visual for our data, let's interpret the findings: 
* The intercept of `fit_2` is the expected probability of voting in the upcoming election for a woman of average age  (~ 50 years old in this data), who is assigned to the Control group, and is a Rarely Voter. The
estimate is `r round((coef(fit_2)["(Intercept)"] * 100), 1)`%. 
* The coefficient of z_age, `r round(coef(fit_2)["z_age"], 1)`, implies a change of ~`r round((coef(fit_2)["z_age"] * 100), 1)`% in likelihood of voting for each increment of one standard deviation (~ 14.45 years). For example: when comparing someone 50 years old with someone 65, the latter is about `r round((coef(fit_2)["z_age"] * 100), 1)`% more likely to vote.
* Exposure to the Neighbors treatment shows a ~`r round((coef(fit_2)["treatmentNeighbors"] * 100), 1)`% increase in voting likelihood for someone in the Rarely Vote category. Because of random assignment of treatment, we can interpret that coefficient as an estimate
of the average treatment effect. 
* If someone were from a different voter classification, the calculation is more complex because we need to account for the interaction term. For example, for individuals who Sometimes Vote, the treatment effect of Neighbors is `r round(coef(fit_2)["treatmentNeighbors"] + coef(fit_2)["treatmentNeighbors:voter_classSometimes Vote"], 1)`%. For Always Vote Neighbors, it is `r round(coef(fit_2)["treatmentNeighbors"] + coef(fit_2)["treatmentNeighbors:voter_classAlways Vote"], 1)`%. 

## Temperance

```{r echo = FALSE, fig.cap = "Temperance"}
knitr::include_graphics("other/images/Temperance.jpg")
```

Finally, let's remember the virtue of Temperance. The gist of temperance is: be humble with our inferences, as our inferences are always, certainly, and unfortunately not going to match the real world. How does this apply to our shaming scenario?

Recall our initial question: *What is the causal effect, on the likelihood of voting, of different postcards on voters of different levels of political engagement?*

To answer the question, we want to look at different average treatment effects. In the real world, the treatment effect for person A is almost always different than the treatment effect for person B.

Regardless, let's create a plot that displays the posterior probability distributions of the average treatment effects for men of average age across all combinations of 4 treatments and 3 voter classifications:

```{r plot_2}
# Recall that the z-scored age of 0 is ~50 yrs. old in this case. We could look
# at lots of ages and both Male and Female. But that would not change our
# estimates of the treatment effects. The model is linear, so terms associated
# with z_age and sex disappear when we do the subtraction. This is one of the
# great advantages of linear models.

sex <- "Male"
z_age <- 0
treatment <- c("Control",
               "Civic Duty",
               "Hawthorne",
               "Self",
               "Neighbors")
voter_class <- c("Always Vote",
                 "Sometimes Vote",
                 "Rarely Vote")

# This question requires quite the complicated tibble! Speaking both
# hypothetically and from experience, keeping track of loads of nondescript
# column names after running posterior_epred() while doing ATE calculations
# leaves you prone to simple, but critical, errors. expand_grid() was created
# for cases just like this - we want all combinations of treatments and voter
# classifications in the same way that our model displays the interaction term
# parameters.

newobs <- expand_grid(sex, z_age, treatment, voter_class) %>% 
  
  # This is a handy setup for the following piece of code that allows us to
  # mutate the ATE columns with self-contained variable names. This is what
  # helps to ensure that the desired calculations are indeed being done. If you
  # aren't familiar, check out the help page for paste() at `?paste`.
  
  mutate(names = paste(treatment, voter_class, sep = "_"))

pe <- posterior_epred(fit_2,
                        newdata = newobs) %>% 
  as_tibble() %>% 
  
  # Here we can stick the names that we created in newobs onto the otherwise
  # unfortunately named posterior_epred() output. 
  
  set_names(newobs$names)

plot_data <- pe %>% 
  
  # Using our cleaned naming system, ATE calculations are simple enough. Note
  # how much easier the code reads because we have taken the trouble to line up
  # the columns.
  
  mutate(`Always Civic-Duty`    = `Civic Duty_Always Vote`     - `Control_Always Vote`,
         `Always Hawthorne`     = `Hawthorne_Always Vote`      - `Control_Always Vote`,
         `Always Self`          = `Self_Always Vote`           - `Control_Always Vote`,
         `Always Neighbors`     = `Neighbors_Always Vote`      - `Control_Always Vote`,
         `Sometimes Civic-Duty` = `Civic Duty_Sometimes Vote`  - `Control_Sometimes Vote`,
         `Sometimes Hawthorne`  = `Hawthorne_Sometimes Vote`   - `Control_Sometimes Vote`,
         `Sometimes Self`       = `Self_Sometimes Vote`        - `Control_Sometimes Vote`,
         `Sometimes Neighbors`  = `Neighbors_Sometimes Vote`   - `Control_Sometimes Vote`,
         `Rarely Civic-Duty`    = `Civic Duty_Rarely Vote`     - `Control_Rarely Vote`,
         `Rarely Hawthorne`     = `Hawthorne_Rarely Vote`      - `Control_Rarely Vote`,
         `Rarely Self`          = `Self_Rarely Vote`           - `Control_Rarely Vote`,
         `Rarely Neighbors`     = `Neighbors_Rarely Vote`      - `Control_Rarely Vote`) %>% 
  
  # This is a critical step, we need to be able to reference voter
  # classification separately from the treatment assignment, so pivoting in the
  # following manner reconstructs the relevant columns for each of these
  # individually. 
  
  pivot_longer(names_to = c("Voter Class", "Group"),
               names_sep = " ",
               values_to = "values",
               cols = `Always Civic-Duty`:`Rarely Neighbors`) %>% 
  
    # Reordering the factors of voter classification forces them to be displayed
    # in a sensible order in the plot later.
  
    mutate(`Voter Class` = fct_relevel(factor(`Voter Class`),
                                     c("Rarely",
                                       "Sometimes",
                                       "Always")))

plot_data  %>% 
  
  # Reordering the y axis values allows a smoother visual interpretation - 
  # you can see the treatments in sequential ATE.
  
  ggplot(aes(x = values, y = fct_reorder(Group, values))) +
  
  # position = "dodge" is the only sure way to see all 3 treatment distributions
  # identity, single, or any others drop "Sometimes" - topic for further study
  
    stat_slab(aes(fill = `Voter Class`),
              position = 'dodge') +
    scale_fill_calc() +
  
    # more frequent breaks on the x-axis provides a better reader interpretation
    # of the the shift across age groups, as opposed to intervals of 10%
    
    scale_x_continuous(labels = scales::percent_format(accuracy = 1),
                       breaks = seq(-0.05, 0.11, 0.01)) +
    labs(title = "Treatment Effects on The Probability of Voting",
         subtitle = "Postcards work less well on those who rarely vote",
         y = "Postcard Type",
         x = "Average Treatment Effect",
         caption = "Source: Gerber, Green, and Larimer (2008)") +
    theme_clean() +
    theme(legend.position = "bottom")

```

This is interesting! It shows us a few valuable bits of information:

* We are interested in the average treatment effect of postcards. There are 4 different postcards, each of which can be compared to what would have happened if the voter did not receive any postcard. 
* These four treatment effects, however, are heterogeneous. They vary depending on an individual's voting history, which we organize into three categories: Rarely Vote, Sometimes Vote and Always Vote. So, we have 12 different
average treatment effects, one for each possible combination of postcard and voting history. 
* For each of these combinations, the graphic shows our posterior distribution.
* Consider the highest yellow distribution, which is the posterior distribution for the average treatment effect of receiving the Neighbors postcard (compared to not getting a postcard) for Always Voters. The posterior is centered around 9% with a 95% confidence interval of, roughly, 8% to 10%. 
* Overall, the Civic Duty and Hawthorne postcards had small average treatment effects, across all three categories of voter.  The causal effect on Rarely Voters was much smaller, regardless of treatment. It was also much less precisely estimated because there were many fewer Rarely Voters in the data. 
*The best way to increase turnover, assuming there are limits to how many postcards you can send, is to focus on Sometimes/Always voters and to use the Neighbors postcard. 

If we wanted to increase voter turnout, then, we would focus our efforts on the Neighbors postcard for citizens who already demonstrate a tendency to vote. 

Does this mean this is guaranteed to work? Unfortunately, no matter how hard we try, *we cannot predict the future*. Though we now have conclusions about how shaming impacted voters in the 2006 primary elections, we do not have the confidence to say that what worked or didn't work then would work now. 

For instance, perhaps the impact of your neighbors knowing your voting history is greater in the midst of a pandemic, where you may be locked inside with few interactions outside of your immediate proximity. Perhaps the opposite is true. These *unknown unknowns* cannot be accounted for in our models. We cannot predict a pandemic, nor can we predict how this will change the way that people vote or respond to fliers. 

There is also the issue of representativeness. Do the voters of the 2006 primary election (who have already demonstrated a willingness to vote in the 2004 primary election) truly represent the people voting in **our** gubernatorial election? 

These complications are why we must make inferences with a grain of salt. That is not to say that all data science is unhelpful! On the contrary, acknowledging our deficits will only make our inferences (and the actions we take because of them) stronger. 

<!-- What we need to add: Preceptor table, causal explanation, math before models, posterior_epred. -->

## Summary

*Key commands*:
* Use the `tidy()` function from the **broom.mixed** package to make models with $N$ parameters easier to interpret. 
* A function we are familiar with, `stan_glm()`, is used to create models with $N$ parameters. 

*Remember*:
* It is important to remember that the data does not equal the truth. 
* The population we would like to make inferences about *is not the population for which we have data*. It is a matter of wisdom whether the data we do have maps closely enough to the population we are studying.
* When dealing with models with many parameters, double check that you know how to find the true slope and intercepts --- often, this requires adding numerous values to the coefficient you are studying. 
