---
title: "Style Guideline"
author: "David Kane"
date: "5/9/2020"
output: html_document
---

## Style Conventions

* Never use just a single `#` after using it for the chapter title. The first subpart uses a `##`. There should be 5 to 8 subparts for each chapter. Within each subpart, you may have sub-subparts, indicated with `###`. There should be 3 to 10 of those. It is unclear how `####` would appear and/or if we want to use this.

* Section headings (other than Chapter titles) are in sentence case (with only the first word capitalized, unless it is something always capitalized) rather than title case (in which all words except small words like "the" and "of" are capitalized. Chapter titles (Continuous Response) are in title case. Section heading do not end with a period.

* If your chapter is the first to use dataset X, then provide an EDA subpart, e.g., [like this](https://davidkane9.github.io/PPBDS/two-parameters.html#eda-for-trains). Name that section something like 

`## `trains` EDA {#trains-EDA}`

The EDA will always feature the use of important functions like `glimse()` and `skim()`. It will also have one or two simple graphics which highlight the variables which we will then be using in the model.  If you are using a dataset which has already been introduced, then provide a link to that original EDA using something like: "\@ref(trains-EDA)". 

* "We" are writing this book. Replace all "I" with "we."  

* Use lots of Tufte tricks: https://rstudio.github.io/tufte/

* Add lots of memes and videos and cartoons.

* Package names are in bold: **ggplot2** is a package for doing graphics.

* R code, anything you might type in the console, is always within backticks: `mtcars` is a built-in dataset.

* Function names always include the parentheses: we write `pivot_wider()`, not `pivot_wider`.

* Do not use code chunk names because it messes up building the book because of limits in **bookdown**.

* All tables should be created with the **gt** package. No more `kable()` or, even worse, hand-made tables.

* All images are loaded with `knitr::include_graphics().`

* Interim data sets should be called `x` or something sensible to the situation, like `ch7` for a data set you are working with in Chapter 7. Do not use names like `data` and `df`, both of which are R commands.

* message=FALSE is a useful code chunk option when loading packages since it prevents all the warning messages from printing out

* include=FALSE should be used when you want the code to run --- example: load some needed libraries which you don't want to explain to students --- but also show nothing.

* Remove library(moderndive) everywhere. Need to use different data sets.

* Remove all intra-book references. The vast majority are broken anyway, because of all the changes we have made. We can decide later what to add back in.

* Remove stuff like \index{dplyr!filter}. We don't even have an index!

* Try to do more calculations on the fly rather than save/load data. We need to explore the use of cache=TRUE.

* There are no length restrictions, so never do too much code in a single step. Show the results of each step. Show what each column (mod, data, etc) looks like as it is added to the tibble.

* Never hard code stuff like `A tibble: 336,776 x 19`. What happens when you update the data? Instead, calculate all numbers on the fly.

* No comments in code blocks that are visible to the reader. Explanations in those cases belong in the text. But hidden code blocks should be fully commented.


## What is the Problem?

Every chapter 5+ begins with a problem, and the decision we must make. These are often toy, highly stylized problems. The decisions are not realistic. But, in structure, these problems parallel the real problems that people face, the actual decisions which they must make.

The problem is specified at the end of the "preamble," the untitled part of the chapter after the title and before the first subpart. Example from Chapter 8:

> A person arrives at a Boston commuter station. The only thing you know is their political party. How old are they? Two people arrive: A Democrat and a Republican. What are the odds that the Democrat is 10% older than the Republican?

> A different person arrives at the station. You know nothing about them. What will their attitude toward immigration be if they are exposed to Spanish-speakers on the platform? What will it be if they are not? How certain are you? 

Is this an actual problem that someone might face? No! But it is like such problems. The first requires the creation of a predictive model. The second necessitates a causal model. The rest of the chapter teaches the reader how to create such models. The end of the chapter harkens back to the questions from the beginning.

Might it be nice to put more meat on the story than that? Perhaps. In an ideal world, the "decision" you faced would be more complex than just playing the prediction game. Begin with a decision. What real world problem are you trying to solve? What are the costs and benefits of different approaches? What unknown thing are you trying to estimate? With Sampling, it might be: How many people should I call? With estimating one parameter --- like vote share as the ballots come in --- it might be: How much should I bet on the election outcome? 

The data we have might not be directly connected to our problem. For example, we might be running a Senate campaign and trying to decide what to spend money on. The Spanish-speakers-on-a-train-platform data set is not directly related to that problem, but it isn't unrelated. Indeed, the first theme of "valadity" is directly related to this issue: Is the data we have relevant to the problem we want to solve? (Tukey quote about some data and a burning desire to answer the question being enought.)

Yet, this seems a bridge too far for the summer of 2020, although we might revisit. For now, the start of each chapter asks a question which is a simple application of the prediction game. 


## Recommendations

* Make ample use of comments, placed with the handy CMD-Shift-/ shortcut. These are notes for everyone else working on the chapter, and for future you.

* At the start of the chapter:
  + Create a list of all packages used. List just **tidyverse**, not all the sub-packages. Almost every chapter will use **tidyverse** and **PPBDS.data**.
  + Create a list of every new command that the chapter introduces. This will help us in creating the tutorial for that chapter. You can also include commands which, while perhaps not new, are important.
  + List the packages and commands that you are thinking of including.
  + List the datasets which will be used. With luck, all of these will come from the PPBDS.data.

* Students are sometimes tentative. Don't be! Edit aggressively. If you don't like what is there, delete it. (If I disagree with your decision, I can always get the text back from Github.) Move things around. Make the chapter yours, while keeping to the style of the other chapters. Note that 90% of the prose here was not written by me. As the Acknowledgment page explains, the book is currently a collection from different open-sourced textbooks. Cut anything you don't like.

* To insert a .gif into your chapter, first convert from mp4 to .gif (I used this website: https://convertio.co/mp4-gif/). Next, add this line to your rmarkdown (without comments):

<!-- `r include_graphics("chapter-name/images/file-name.gif")` -->

To include static pictures, use the same `include_graphics()` function, making sure the file extension is .png or otherwise appropriate.

* Explore different ways of doing things. If you find a better way of inserting images or formatting code, let us know!
 

### Topics


* Every chapter has an "Advanced" section at the end of the chapter. This is for material which we will not cover in the course officially --- it won't appear in class, problem sets or exams --- but which stronger students might be interested in. Eventually, this might allow us to not require a textbook for Gov 52.

* Everything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potentional outcomes to define precisely what "true value" you are talking about. And so on. We use the bootstrap to show that lm() produces the same answers, and then just use lm() because it is quicker.

* Ought to discuss over-fitting versus under-fitting in chapter 9+. Ought to think about in-sample and out-of-sample. The problem we are trying to solve, not the tool we are using to solve it.

*  We could use tidymodels syntax each time. Each model would have both a many-small-models example and a machine learning example. Predicted (or fitted) values and residuals (or errors). 

* Make more clear that  use the bootstrap for two reasons. First, to build intuition and provide justification for functions like lm. Second, to solve problems which can't be solved by standard functions. Hmm. Not sure that this works. The second case is just: Use simulation when you need to. 


## Open Questions

* Decide on standard number formatting: 2400 or 2,400 or $2400$ or `r scales::comma(my_number)` or . . . .

* Use tidybayes package for better graphics throughout? Note that some major surgery is happening with this package, splitting it into two parts. Maybe the geom_dist() is all we will need?

* Should we be using **rstanarm** or **brms** for Bayesian analysis?

* Look at the **flair** package to format the code. Or does that require us to have two copies of code: working copy and colored copy? https://education.rstudio.com/blog/2020/05/flair/

* Does using **flipbookr** make sense in the middle of a chapter?

* https://github.com/yonicd/carbonate -- perhaps useful for some nicer formatting of source code.

* https://education.rstudio.com/blog/2020/05/learnr-for-remote/





## Functions and Data with Chapters

The purpose of this section is to discuss, for each of the later chapters, which data sets and functions we should use. Each time we present a data set for the first time, we have an Exploratory Data Analysis section which, at a minimum, runs glimpse(), sample_n(), skimr::skim() and probably creates a plot or two. This section might begin by restricting the analysis to a subset of the original data. Either way, we never manipulate the original data. We don't do stuff to `trains`. Instead, we create something like `ch9` which is a subset of the columns/rows from `trains`.

Ideally, you have two datasets in each chapter, only one of which is new. That is, each chapter features an EDA of one, but just one, new data set. It is also nice to have one causal and one non-causal data set. But that is not necessary. We should always use data in PPBDS.data, unless we have a good reason not to. Indeed, the main purpose of PPBDS.data is precisely to create/clean/distribute such data sets.

Sampling: 6. rep_sample_n(). Save bootstraps? Only urns. Or also use nes in place of Obama example? There is a lot going on in this chapter already. And we have not examined.

One Parameter: 7. lm(x ~ 1). bootstraps(). also median, 3rd biggest, et cetera. qscores is the main data set. Might we have a second example, using polling results, which estimates who is going to win and updates as the new responses come in? I don't think we need to do anything causal in this chapter, but we should make clear that we are not.

Two Parameters: 8. lm(age ~ -1 + party). bootstraps() to explore something other than mean. trains is the main data set. 

N Parameters: 9. Start with shaming and review what we did in chapter 8, but with more parameters. Discuss interactions and hetergenous treatment effects. Then introduce nes. (Save cces for chapter 11?) lm(something ~ lots of categories). see how bad it works. stan_glm(). no bootstraps. How to transition?

10. lm(continuous ~ continuous). start with qscores. no more bootstraps() since have we already made that point. loess(). overfitting/underfitting. tidymodels. penalty. decisions. How do we pick the model we want? Which should we use, lm() or loess()? 

11. Three models which we explain and then explore with the full tidyverse machinery. regression()

12. Three models which we explain and then explore with the full tidyverse machinery. logistic regression(), random forest and deep learning?


