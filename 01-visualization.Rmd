
<!-- DK: There are way too many warnings in this chapter. A few are fine, all the better to explain what warnings are and how to think about them. But the rest should go! Maybe a better tibble choice would solve this problem. -->


# Visualization

Everyone loves visualizations. 

<!-- DK: Should simplify this plot a bit. No fct_relevel since we don't learn that till chapter 2, for example. -->

```{r, echo = FALSE, out.width = "100%", message = FALSE}
library(tidyverse)
library(scales)
library(gapminder)

gap_p <- gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  mutate(continent = fct_relevel(continent, 
                                 c("Americas", "Europe"), 
                                 after = 0)) %>% 
  ggplot(aes(gdpPercap, lifeExp, size = pop, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita in 2007",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy",
         caption = "Data Source: gapminder R package") +
    scale_x_log10(breaks = c(500, 5000, 50000),
                  labels = scales::dollar_format(accuracy = 1)) +
    theme(legend.position = "none")

gap_p
```

Once you have read this chapter, and completed the associated tutorials, you will be able to create graphics like this one with your data. Join us on the journey.


## Getting Started

This chapter focuses on **ggplot2**, one of the core packages in the **tidyverse**. To access the datasets, help pages, and functions that we will use in this chapter, load the tidyverse by running this code:

```{r} 
library(tidyverse)
```

That one line of code loads all the packages associated with the **tidyverse**, packages which you will use in almost every data analysis. It also tells you which functions from the tidyverse conflict with functions in base R or with other packages you might have loaded. (We hide these and other messages because they are ugly.)


If you run this code and get the error message “there is no package called ‘tidyverse’”, you’ll need to install the package, and then run library() once again.


```{r eval=FALSE}
install.packages("tidyverse")
```

After installing, run library(tidyverse). Recall, we only need to install a package once. We just need to load it every time we use it. We will also use the package `gapminder` in this chapter.

Recall, we only need to install a package once. We need to load it every time we use it. We will also use another package `gapminder`.

```{r}
library(gapminder)
```

If we need to be explicit about where a function (or dataset) comes from, we’ll use the special form: `package::function()`. For example, `ggplot2::ggplot()` tells you explicitly that we’re using the `ggplot()` function from the **ggplot2** package.



### How do I code in R?

Unlike other statistical software programs like Excel, SPSS, or Minitab that provide [point-and-click](https://en.wikipedia.org/wiki/Point_and_click) interfaces, R is an [interpreted language](https://en.wikipedia.org/wiki/Interpreted_language). This means you have to type in commands written in *R code*. In other words, you have to code/program in R. Note that we'll use the terms "coding" and "programming" interchangeably in this book.


If you are new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out the short book, [*Getting Used to R, RStudio, and R Markdown*](https://rbasics.netlify.com/). @usedtor2016 includes screencast recordings that you can follow along and pause as you learn. This book also contains an introduction to R Markdown, a tool used for reproducible research in R.


While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that new R users need to understand. 


### Tips

Learning to code/program is quite similar to learning a foreign language. It can be daunting and frustrating at first. However, putting in the effort goes a long way.

Here are a few useful tips to keep in mind as you learn to program:

* **Remember that computers are not actually that smart**: You may think your computer or smartphone is "smart". In reality, you have to tell a computer everything it needs to do. The instructions you give your computer can't have any mistakes in them.

* **Take the "copy, paste, and tweak" approach**: When you learn your first programming language, it is often much easier to take existing code that you know works and modify it to suit your ends. This is as opposed to trying to type out the code from scratch. We suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. After you start feeling more confident, you can slowly move away from this approach and write code from scratch.

* **Practice is key**:  Just as the only method to improve your foreign language skills is through lots of practice and speaking, the only method to improving your coding skills is through lots of practice. 

### Basic programming

We now introduce some basic programming concepts and terminology. Instead of asking you to memorize all these concepts and terminology right now, we'll guide you so that you'll "learn by doing." To help you learn, we will always use a different font to distinguish regular text from `computer_code`. 

* *Console pane*: where you enter in commands.
* *Running code*: the act of telling R to perform an act by giving it commands in the console.
* *Objects*: where values are saved in R. You can *assign* values to objects and display the contents of objects. 
* *Data types*: integers, doubles/numerics, logicals, and characters. Integers are values like -1, 0, 2, 4092. Doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8. Logicals are either `TRUE` or `FALSE` while characters are text such as "cabbage", "Hamilton", "The Wire is the greatest TV show ever", and "This ramen is delicious." Note that characters are often denoted with the quotation marks around them.
* *Vectors*: a series of values. These are created using the `c()` function, where `c()` stands for "combine" or "concatenate." For example, `c(6, 11, 13, 31, 90, 92)` creates a six element series of positive integer values.
* *Factors*: *categorical data* are commonly represented in R as factors. Categorical data can also be represented as *strings*. We will go into detail about these and other variable types in Chapter 2.
* *Data frames*: rectangular spreadsheets. They are representations of datasets in R where the rows correspond to *observations* and the columns correspond to *variables* that describe the observations. Modern data frames are called *tibbles*.  
* *Boolean algebra*: `TRUE/FALSE` statements and mathematical operators such as `<` (less than), `<=` (less than or equal), and `!=` (not equal to). For example, `4 + 2 >= 3` will return `TRUE`, but `3 + 5 <= 1` will return `FALSE`. Testing for inclusion with the `%in%` operator. For example, `"B" %in% c("A", "B")` returns `TRUE` while `"C" %in% c("A", "B")` returns `FALSE`. We test for equality in R using `==` (and not `=`, which is typically used for assignment). For example, `2 + 1 == 3` compares `2 + 1` to `3` and is correct R code, while `2 + 1 = 3` will return an error. 
* *Logical operators*: `&` representing "and" as well as `|` representing "or." For example, `(2 + 1 == 3) & (2 + 1 == 4)` returns `FALSE` since both clauses are not `TRUE` (only the first clause is `TRUE`). On the other hand, `(2 + 1 == 3) | (2 + 1 == 4)` returns `TRUE` since at least one of the two clauses is `TRUE`. 
* *Functions*, also called *commands*: perform tasks in R. They take in inputs called *arguments* and return outputs. You can either manually specify a function's arguments or use the function's *default values*. For example, the function `seq()` in R generates a sequence of numbers. If you just run `seq()` it will return the value 1. That doesn't seem very useful! This is because the default arguments are set as `seq(from = 1, to = 1)`. Thus, if you don't pass in different values for `from` and `to` to change this behavior, R just assumes all you want is the number 1. You can change the argument values by updating the values after the `=` sign. If we try out `seq(from = 2, to = 5)` we get the result `2 3 4 5`, as we would expect. 
* *Help files*: provide documentation for various functions and datasets. You can bring up help files by adding a `?` before the name of a function or data frame and then run this in the console. You will then be presented with a page showing the corresponding documentation.    

### Errors, warnings, and messages

R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad.

R will show red text in the console pane in three different situations:

* **Errors**: Red text prefaced with "Error in…" . It will try to explain what went wrong. Generally when there's an error, the code will not run. For example, if you see `Error in ggplot(...) : could not find function "ggplot"`, it means that the `ggplot()` function is not accessible because the package that contains the function, **ggplot2**, was not loaded with `library(ggplot2)`.

* **Warnings**: Red text prefaced with "Warning:". It will try to explain why there's a warning. Generally your code will still work, but with some caveats. For example, if you create a scatterplot based on a dataset where two of the rows of data have missing entries, you will see this warning: `Warning: Removed 2 rows containing missing values (geom_point)`. R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren't there.

* **Messages**: When the red text doesn't start with either "Error" or "Warning", it's *just a friendly message*. You'll see these messages when you load *R packages* or when you read data saved in spreadsheet files with the `read_csv()` function as you'll see in Chapter 2. These are helpful diagnostic messages. They don't stop your code from working. Additionally, you'll see these messages when you install packages too using `install.packages()`.

Remember, when you see red text in the console, *don't panic*. It doesn't necessarily mean anything is wrong. Rather:

* If the text starts with "Error", figure out what's causing it. <span style="color:red">Think of errors as a red traffic light: something is wrong!</span>
* If the text starts with "Warning", figure out if it's something to worry about.  <span style="color:gold">Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention.</span>
* Otherwise, the text is just a message.<span style="color:green">Think of messages as a green traffic light: everything is working fine and keep on going!</span>

### Examining `gapminder`

Let's put everything we've learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers.  Throughout this book, we'll focus on datasets that are saved in "spreadsheet"-type format. This is probably the most common way data are collected and saved in many fields. These "spreadsheet"-type datasets are called _data frames_ in R. We'll focus on working with data saved as data frames throughout this book. Again, "tibble" is the more modern term for "data frame," but we will use both interchangeably.


We'll begin by exploring the `gapminder` data frame. This dataset is an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.

Run the following code in your console, either by typing it or by cutting-and-pasting it. It displays the contents of the `trains` data frame in your console. Note that depending on the size of your monitor, the output may vary slightly. 

```{r}
library(gapminder)
gapminder
```

Let's unpack this output:

* `A tibble: 1,704 x 6`: A `tibble` is a specific kind of data frame in R. This particular data frame has `1,704` rows corresponding to different *observations*. The tibble also has `6` columns corresponding to 6 *variables* describing each observation.
* `country`, `continent`, `year`, `lifeExp`, `pop`, and `gdpPercap` are the different variables of this dataset. 
* We see, dy default, the top 10 rows, but these ten are followed `... with 1,694 more rows`, indicating to us that 1,694 more rows of data could not fit in this screen. R is only showing the first 10 rows, since that is all that you probably want to see at first. You can see more (or fewer) rows with the `print()` command, i.e.,

```{r}
print(gapminder, n = 15)
```

### Exploring data frames

There are many ways to get a feel for the data contained in a data frame such as `trains`. We present two functions that take as their "argument" (their input) the data frame in question. We also include a third method for exploring one particular column of a data frame:

1. Using the `view()` function, which brings up RStudio's built-in data viewer.
1. Using the `glimpse()` function, which is included in the **dplyr** package.
1. Using the `$` "extraction operator," which is used to view a single variable/column in a data frame.

**1. `View()`**:

Run `view(gapminder)` in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter.

By running `view(gapminder)`, we can explore the different *variables* listed in the columns. Observe that there are many different types of variables.  Some of the variables including `year`, `lifeExp`, `pop`, `year` and `gdPercap` are *quantitative* variables. These variables are numerical in nature.  Other variables here, including `country`, `continent` are *categorical*.

If you look in the leftmost column of the `View(trains)` output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row is representing. This will allow you to identify what object is being described in a given row by taking note of the values of the columns in that specific row. This is often called the *observational unit*.

You can identify the observational unit by determining what "thing" is being measured or described by each of the variables. 

**2. `glimpse()`**:

The second way we'll cover to explore a data frame is using the `glimpse()` which provides us with an alternative perspective for exploring a data frame than the `View()` function:

```{r}
glimpse(gapminder)
```

Observe that `glimpse()` will give you the first few entries of each variable in a row after the variable name.  In addition, the *data type* of the variable is given immediately after each variable's name inside `< >`. 

Here, `dbl` refers to "double", which is computer coding terminology for quantitative/numerical variables. `int` refers to "integer" and is another data type that also represents quantitative/numerical variables.`fct` refers to "factor" and describes a variable that is nominal, or in this case the `country` and `continent` variables.

Not mentioned here is `chr`, which refers to "character", which is computer terminology for text data.

**3. `$` operator**

Lastly, the `$` operator allows us to extract and then explore a single variable within a data frame. For example, run the following in your console

```{r}
gapminder$country
```

We used the `$` operator to extract only the `country` variable and return it as a vector. We'll only be occasionally exploring data frames using the `$` operator, instead favoring the `view()` and `glimpse()` functions.


## Basic Plots

We begin the development of your data science toolbox with data visualization. By visualizing data, we gain valuable insights we couldn't initially obtain from just looking at the raw data values.  We'll use the **ggplot2** package, as it provides an easy way to customize your plots. 

At their most basic, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way to explore the patterns in data, such as the presence of *outliers*, *distributions* of individual variables, and *relationships* between groups of variables. Graphics are designed to emphasize the findings and insights you want your audience to understand. This requires a balancing act. On the one hand, you want to highlight as many interesting findings as possible, but not to the extent where your audience becomes overwhelmed.  


We can break a graphic into the following three essential components:

1. `data`: the dataset containing the variables of interest.
2. `geom`: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars.
3. `aes`: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size.  Aesthetic attributes are *mapped* to variables in the dataset.

These three components are specified in the `ggplot()` function included in the **ggplot2** package. For the purposes of this book, we'll always provide the `ggplot()` function with the following arguments (i.e., inputs):

* The data frame where the variables exist: the `data` argument.
* The mapping of the variables to aesthetic attributes: the `mapping` argument which specifies the `aes`thetic attributes involved.

After we've specified these components, we then add *layers* to the plot using the `+` sign. The most essential layer to add to a plot is the layer that specifies which type of `geom`etric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets.


```{r, echo=FALSE}
gapminder_2007 <- gapminder %>% 
  filter(year == 2007) %>% 
  select(-year) %>% 
  rename(Country = country,
         Continent = continent,
         `Life Expectancy` = lifeExp,
         `Population` = pop,
         `GDP per Capita` = gdpPercap)
```

Let's take a look at a subset of the `gapminder` data. Each row in this table corresponds to a country in 2007.

```{r, echo=FALSE}
print(gapminder_2007, n = 3)
```

Now consider the following scatterplot, which plots our subset of data from 2007.

```{r, echo=FALSE}
ggplot(data = gapminder_2007, 
       mapping = aes(x = `GDP per Capita`, 
                     y = `Life Expectancy`, 
                     size = Population, 
                     color = Continent)) +
  geom_point() +
  labs(x = "GDP per capita", y = "Life expectancy", 
       caption = "Life expectancy over GDP per capita in 2007.") +
  scale_size_continuous(labels = scales::label_comma())
```

Let's view this plot through the grammar of graphics:

1. The `data` variable **GDP per Capita** gets mapped to the `x`-position `aes`thetic of the points.
1. The `data` variable **Life Expectancy** gets mapped to the `y`-position `aes`thetic of the points.
1. The `data` variable **Population** gets mapped to the `size` `aes`thetic of the points.
1. The `data` variable **Continent** gets mapped to the `color` `aes`thetic of the points.

We'll see shortly that `data` corresponds to the particular data frame where our data is saved and that "data variables" correspond to particular columns in the data frame. Furthermore, the type of `geom`etric object considered in this plot are points. However, graphics are not limited to just points. We can also use lines, bars, and other geometric objects.

Let's take a tour of some of the more useful geoms.


### `geom_point()`

*Scatterplots*, also called *bivariate plots*, allow you to visualize the *relationship* between two numerical variables. Specifically, we will visualize the relationship between the following two numerical variables in the `gapminder` data frame.

1. `lifeExp`: average life expectancy of a country
1. `gdpPercap`: gross domestic product of a country

Let's go over the code that will creates the desired scatterplot below, piece-by-piece.

```{r, eval = FALSE}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point()
```

**Step 1:** Within the `ggplot()` function, we specify two of the plot's components as arguments (i.e., inputs):

* The `data` as the `gapminder` data frame via `data = gapminder`.
* The `aes`thetic `mapping` by setting `mapping = aes(x = gdpPercap, y = lifeExp)`. Specifically, the variable `gdpPercap` maps to the `x` position aesthetic, while the variable `lifeExp` maps to the `y` position.
        
**Step 2:** Add a layer to the `ggplot()` function call using the `+` sign. The added layer in question specifies the third component: the `geom`etric object. In this case, the geometric object is set to be points by specifying `geom_point()`.

*Note:* The `+` sign comes at the end of the code line and not at the beginning. When adding layers to a plot, start a new line after the `+` so that the code for each layer is on a new line.

In the graphic above, a *positive relationship* exists between `lifeExp` and `gdpPercap`: as gross domestic product increases, life expectancy tends to also increase. If we do not specify the `geom`etric object, we have a blank plot which is not very useful!

```{r fig.align = "center"}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) 
```

### `geom_jitter()`

In the scatterplot above, there is a large mass of points. This makes it hard to tell the true number of points that are plotted. This is the result of a phenomenon called *overplotting*, where points are being plotted on top of each other over and over again. There are two methods to address the issue of overplotting.

1. Adjust the transparency of the points 
1. Add a little random "jitter" or random "nudges", to each of the points

**Method 1: Changing the transparency**

We can change the transparency/opacity of the points by setting the `alpha` argument in `geom_point()`. The`alpha` argument can be any value between `0` and `1`, where `0` sets the points to be 100% transparent and `1` sets the points to be 100% opaque. By default, `alpha` is set to `1`. In other words, if we don't explicitly set an `alpha` value, R will use `alpha = 1`.

*Note:* how the following code is identical to the code that created the scatterplot with overplotting, but with `alpha = 0.2` added to the `geom_point()` function:

```{r fig.align = "center", warning = FALSE}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point(alpha = .2)
```

The key feature to note in this plot is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no `aes()` surrounding `alpha = 0.2`. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of `alpha`. In fact, you'll receive an error if you try to change the second line to read `geom_point(aes(alpha = 0.2))`.

**Method 2: Jittering the points**

We can jitter points by using `geom_jitter()` instead of `geom_point()`. Keep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged. 

In order to specify how much jitter to add, we adjusted the `width` and `height` arguments to `geom_jitter()`.  This corresponds to how hard you'd like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. It is important to add just enough jitter to break any overlap in points, but not to the extent where you alter the original pattern in points.

```{r}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_jitter(width = 5, height = 5)
```


As can be seen in the resulting plot, in this case jittering doesn't really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting `alpha` proved more effective.  When would it be better to use a jittered scatterplot? There is no single right answer. We suggest you make both types of plots and see which one better emphasizes the point you are trying to make. 

### `geom_line()`

Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the *explanatory* variable, is of a sequential nature. In other words, there is an inherent ordering to the variable. 

```{r echo=FALSE, fig.margin=TRUE}
knitr::include_graphics("https://imgs.xkcd.com/comics/decline.png")
```

The most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called *time series* plots. 

<!-- BG: need data set to show graph here. I tried it with gapminder and it didnt work well -->



### `geom_histogram()`

A histogram is a plot that visualizes the *distribution* of a numerical value as follows:

1. We first cut up the x-axis into a series of *bins*, where each bin represents a range of values. 
1. For each bin, we count the number of observations that fall in the range corresponding to that bin.
1. Then for each bin, we draw a bar whose height marks the corresponding count.

Let's consider the `lifeExp` in Asia in the `gapminder` data frame once again, but unlike with the linegraphs, let's say we don't care about its relationship with time, but rather we only care about how the values of `lifeExp` *distribute*. In other words:


```{r echo = FALSE, fig.align = "center"}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram()
 
```

Unlike with scatterplots and linegraphs, there is now only one variable being mapped in `aes()`: the single numerical variable `lifeExp`. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a `geom_histogram()`. After running the following code, you'll see the histogram as well as warning messages. We'll discuss the warning messages first. 
* The warning message is telling us that the histogram was constructed using `bins = 30` for 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We'll see in the next section how to change the number of bins to another value than the default.

It's hard to get a sense for which range of life expectancy is spanned by each bin; everything is one giant amorphous blob. So let's add white vertical borders demarcating the bins by adding a `color = "white"` argument to `geom_histogram()` and ignore the warning about setting the number of bins to a better value:

```{r warning = FALSE, message = FALSE, fig.align = "center"}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram(color = "white", fill = "steelblue")
```

We can also adjust the number of bins in our histogram in one of two ways:

1. By adjusting the number of bins via the `bins` argument to `geom_histogram()`. 
2. By adjusting the width of the bins via the `binwidth` argument to `geom_histogram()`. 

As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows. Notice the warning message disappeared!

```{r, eval = FALSE}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram(bins = 40, color = "white",fill = "steelblue" )
```

Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the `binwidth` argument in the `geom_histogram()` layer. For example, let's set the width of each bin to be 10&deg;F.

```{r, eval = FALSE}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram(binwidth = 5, color = "white",fill = "steelblue")
```

We compare both resulting histograms side-by-side. 

```{r warning=FALSE, message=FALSE, echo=FALSE, fig.align="center"}
x <- ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram(bins = 40, color = "white",fill = "steelblue")

y <- ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram(binwidth = 5, color = "white",fill = "steelblue")

x + y
```


### `geom_bar()` 

Similarly to histograms, `geom_bar()` visualizes the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the *levels* of the categorical variable. Often the best way to visualize these different counts, also known as *frequencies*, is with barplots (also called barcharts).

Run the following code that manually creates a data frame representing a collection of fruit: 3 apples and 2 oranges. Notice how `fruits` lists the fruit individually.

```{r}
fruits <- tibble(fruit = c("apple", "apple", "orange", 
                           "apple", "orange"))

fruits
```

Using the `fruits` data frame where all 5 fruits are listed individually in 5 rows, we map the `fruit` variable to the x-position aesthetic and add a `geom_bar()` layer:

```{r}
ggplot(data = fruits, mapping = aes(x = fruit)) +
  geom_bar()
```



#### No pie charts!

One of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. @robbins2013 argues that we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another.  

While pie charts present information in a way such that comparisons must be made by comparing angles, barplots are more effective because they present the information in a way such that comparisons between categories can be made with single horizontal lines.



#### Two categorical variables

Barplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the *joint* distribution of two categorical variables at the same time. Let's examine the *joint* distribution of different types of manufacturers' cars by `class` as well as `model`. In other words, the number of cars for each `class` and `model` combination. 

The code below creates a barplot of `class` frequency:

```{r, eval = FALSE}
ggplot(data = mpg, mapping = aes(x = class)) + 
  geom_bar()
```

We can now map the additional variable `model` by adding a `fill = model` inside the `aes()` aesthetic mapping.

```{r eval = FALSE}
ggplot(data = mpg, mapping = aes(x = class, fill = model)) +
  geom_bar()
```

This is an example of a *stacked barplot*.  While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of cars from each model between classes.

Before we continue, let's address some common points of confusion among new R users. First, the `fill` aesthetic corresponds to the color used to fill the bars, while the `color` aesthetic corresponds to the color of the outline of the bars. Observe that mapping `model` to `color` and not `fill` yields grey bars with different colored outlines.

```{r eval = FALSE}
ggplot(data = mpg, mapping = aes(x = class, color = model)) +
  geom_bar()
```

Second, note that `fill` is another aesthetic mapping much like `x`-position; thus we were careful to include it within the parentheses of the `aes()` mapping. The following code, where the `fill` aesthetic is specified outside the `aes()` mapping will yield an error. This is a fairly common error that new `ggplot` users make:

```{r, eval = FALSE}
ggplot(data = mog, mapping = aes(x = class), fill = model) +
  geom_bar()
```

An alternative to stacked barplots are *side-by-side barplots*, also known as *dodged barplots*. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a `position = "dodge"` argument added to `geom_bar()`. In other words, we are overriding the default barplot type, which is a *stacked* barplot, and specifying it to be a side-by-side barplot instead.

```{r eval = FALSE}
ggplot(data = mpg, mapping = aes(x = class, fill = model)) +
  geom_bar(position = "dodge")
```



Note the width of the bars vary among one another. We can make one tweak to the `position` argument to get them to be the same size in terms of width as the other bars by using the more robust `position_dodge()` function.

```{r eval = FALSE}
ggplot(data = mpg, mapping = aes(x = class, fill = model)) +
  geom_bar(position = position_dodge(preserve = "single"))
```




### `geom_smooth()`

Now let's go back to the `geom_point()` section and add a smooth line using the `geom_smooth()` function. 

Here's the scatterplot we created from the `geom_point()` section:

```{r fig.align="center", warning=TRUE, echo=FALSE}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point()
```

Let's try adding a regression line on a scatterplot using the `geom_smooth()` function in combination with the argument `method = lm`, where `lm` stands for linear model. We can add `geom_smooth(method = lm)` as another layer in our plot.  

```{r fig.align = "center", echo = FALSE}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point() +
  geom_smooth(method = lm)
```

Another method to fit a line to a scatter plot is called the `loess` method, which computes a smooth local regression and is the default value for a small number of observations. You can read more about loess using the R code `?loess` in your console. 

Here's what the graph looks like using the Loess method for local regression fitting:

```{r fig.align = "center", warning = TRUE, echo = FALSE}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point() +
  geom_smooth(method = loess)
```

Notice the gray section surrounding both sides of both of the lines we plotted. This area is called the confidence interval, which is set to a 95% confidence interval by default. Therefore, using the 95% default confidence interval, we can be 95% certain that predictions for the model used in each plot fall within the gray area.

If we want to remove the gray area, or the confidence interval, we can set `se = FALSE` within the `geom_smooth()` function.

```{r fig.align = "center", warning = TRUE, echo = FALSE}
ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
  geom_point() +
  geom_smooth(method = loess, se = FALSE)
```

### `geom_density()`

Recall the histogram we plotted in the `geom_histogram()` section.

```{r warning=TRUE, fig.align="center"}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_histogram()
```

We can change `geom_histogram()` to `geom_density()` to make a density plot, which is a smoothed version of the histogram. This is a useful alternative to the histogram that displays continuous data in a smooth distribution.

```{r warning=TRUE, fig.align="center"}
ggplot(data = gapminder, mapping = aes(x = lifeExp)) +
  geom_density()
```


## Advanced Plots

<!-- DK: Other stuff needed here? after_stat; plotting two geoms at once; scale_x for changing labels; scales:: for axis formatting. coord_cartesian() and xlim/ylim, which allows for zooming/rescaling graphs. Other? -->

<!-- - scales in general, and in particular, RColorBrewer and viridis. -->
<!-- - using scales to change the name and labels of a legend. -->

<!-- - facet_grid() to facet by more than 1 variable. -->

<!-- 1) Plots are ordered alphabetically if it is a character and by levels if it is a factor. Dates appear differently in plots. reorder() is useful. Cover this at start of advanced plotting. -->

<!-- 2) Purpose of Advanced plotting is to show the tools for making the plots that appear in socviz.co, chapters 3 and 4.  -->

```{r, echo=FALSE, fig.cap="Data Visualization with ggplot2 Cheat Sheet", out.width="100%"}
knitr::include_graphics("01-visualization/images/ggplot2-cheatsheet.jpg")
```

In the previous section we have seen the three components that every plot must include: data, data mappings, and a geom. You may have found that the graphics from before look a bit boring. Luckily, the Grammar of Graphics allows us to add more layers on our own in order to customize our plots. We will go over some of the additional layers here, but the [Data Visualization with ggplot2 Cheat Sheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) is a great resource to refer to as your ggplot visualizations get more complicated.

<!-- Links to relevant readings in Healy.  -->

To keep it simple, we will change the Gapminder plot from the beginning of the chapter layer by layer. We begin by creating a subset of the data and then plotting that subset.

```{r}
gapminder_07 <- gapminder %>% 
                    filter(year == 2007, continent != "Oceania")
```

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point()
```

At the moment it consists of only three necessary elements: 

* A subset of the gapminder dataset
* GDP per capita on the x-axis, and life expectancy on the y-axis (mappings)
* `geom_point()`, which creates a scatterplot

### Faceting

Let's start by introducing a new concept called *faceting*.  Faceting is used when we'd like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ. 

If we look at the plot above, it is quite difficult to compare the continents despite the colors. It would be much easier if we could "split" this scatterplot by the 5 continents in the dataset. In other words, we would create plots of `gdpPercap` and `lifeExp` for each `continent` separately. We do this by adding a `facet_wrap(~ continent)` layer. Note the `~` is a "tilde" and can generally be found on the key next to the "1" key on US keyboards. The tilde is required and you'll receive the error `Error in as.quoted(facets) : object 'month' not found` if you don't include it here. 

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent)
```

This is way better! However, R chooses by default 2 plots per row, so that Asia and Europe are below the other two continents. We can specify the number of rows and columns in the grid by using the `nrow` and `ncol` arguments inside of `facet_wrap()`. Let's get all continents in a row by setting nrow = 1:

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1)
```

Note that, as expected, we can see a positive correlation between economic development and life expectancy on all continents. Now it is also clearer that Asia is on average at about the same level as the Americas, but there are more countries in Asia that are at both extremes.


### Stats

<!-- DK: This section is confusing. And irrelevant? Should we explain after_stat() here or elsewhere? -->

The next layer are stats, or `stat`istical transformations. ggplot provides us with many different ones, but most of them are only suitable for certain types of geoms. Take for example `stat_boxplot()`, which is applicable to scatterplots and adds boxplots to our plot:

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_boxplot()
```

Now we not only recognize the differences between the continents, but also within the continents better. However, the interpretation of box plots is not so intuitive, at least at first glance. An easier to understand alternative is a line of best fit, which we can add with `stat_smooth(formula = y ~ x, method = "lm", se = FALSE)`:

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE)
```

These were only two stats, but there are many more for different purposes. If you type "stat" in the console, you will get suggestions for the different options that ggplot offers.

### Coordinate Systems

Next, we can specify the type of coordinate system. In most cases we will use Cartesian coordinate systems, which are set with `coord_cartesian()`. R draws every plot by default with this method, so we don't need to determine it specifically. Depending on the data we are working with, more exotic variants like `coord_polar()` or `coord_map()` may be helpful. 

A coordinate system that is used more often is `coord_flip()`. This is actually just a Cartesian coordinate system, but as the name suggests, it simply swaps the axes:

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  coord_flip()
```

As can be seen, lifeExp is now on the x-axis and gdpPercap on the y-axis. Compared to the previous plot it is now easier to observe the distribution of life expectancy in the respective continents. For example, we can see that many countries in Africa are at about 55 years, in the Americas and Asia at 75 years, and in Europe at 80 years. However, we think it makes more sense to consider `lifeExp` as the dependent variable, so don't use `coord_flip()` in the subsequent plots.


### Positions, Axis Limits and Scales

We can also use ggplot to change the position of the plot content. Positions are rather "cosmetic" elements, which make plots easier to understand. As before, there are many different options to do this, which often only work for certain geoms. For example, when we work with barplots, we can use `position_dodge()` or `position_stack()` to specify whether we want to arrange the bars in the plot side by side or on top of each other. For scatterplots like ours, `position_jitter()` can be used:

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point(position = position_jitter(width = 10, height = 15)) +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE)
```

Notice several things about this code. First, positions should be placed in the geom they refer to (in our case `geom_point()`). Second, positions always start with `position =` followed by the position type (in our case `position_jitter()`). The width and height are optional and can be freely defined. You might wonder what the difference between `position_jitter()` and `geom_jitter()` is. The answer is: Nothing. In R there are often several functions which can lead to the same result. However, it is practical to start with `geom_point()` first, because then you still have the choice of changing the position by shaking the dots - with `geom_jitter()` the plot is constructed like this from the beginning.

Besides the position we can also manipulate the limits of the axes by using `xlim()` and `ylim()`. For example, assume that we are only interested in countries with a GDP per capita from 0 to 30000. We can tell R as follows that we only want to see this range. Note that, because `data` is the first argument and `mapping` is the second to `ggplot()`, we don't actually have to name the arguments. We can just provide them, as long as they are in the correct order.

```{r}
ggplot(gapminder_07, 
       aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  xlim(0, 30000)
```

We can see that the GDP per capita on the y-axis is now only shown from 0 to 30000. 

Finally we can change the scaling of the axes. For example, it might be useful to display the axes with `scale_x_log10()` or `scale_y_log10()` on a logarithmic scale. Let's try this out on GDP per capita. Also, note that we can (lazily!) not provide the explicit `x` and `y` argument names to `aes()` as long as we provide the values in the right order: `x` comes before `y`. 

```{r}
ggplot(gapminder_07, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10()
```

Notice how the scale of GDP per capita has changed (for now, don't worry about the overlapping labels).


### Labels and Text

The last plot we made looks quite good, but it is not perfect yet. You will have noticed that by default R simply uses the names of variables for axes and legends. Also, the plot has no title yet. We can easily change this by using `labs()`:

```{r}
ggplot(gapminder_07, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10() +
  labs(title = "Life Expectancy and GDP per Capita (2007)",
       subtitle = "Selected Nations by Continent",
       x = "GDP per Capita, USD",
       y = "Life Expectancy, Years",
       caption = "Source: Gapminder") 
```



If a `title` or other `labs()` argument is too long, you can insert a newline character --- a "\\n" --- in the middle, which will cause the `title` to take up two lines rather than one. This is the simplest way to deal with titles or axis labels which are too long.

It is up to us, which or how many of the arguments in `labs()` we use. While most arguments are probably self-explanatory, it makes sense to have a look at the last one. It determines the title of the legend, and should always be named after the aesthetic to which the legend refers. Since the legend was created by the *color* argument in the density plot, we can refer to it with `color =`.

We can also change labels within the plots. Wouldn't it be great if we knew which country each point refers to? We can do this with the help of `geom_text()`:

```{r}
ggplot(gapminder_07, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10() +
  labs(title = "Life Expectancy and GDP per Capita (2007)",
       subtitle = "Selected Nations by Continent",
       x = "GDP per Capita, USD",
       y = "Life Expectancy, Years",
       caption = "Source: Gapminder") +
  geom_text(aes(label = country), size = 2, 
            color = "black", check_overlap = TRUE)
```

Great! Notice that we need to determine an aesthetic called *label*. This defines the character variable which will be used as the basis for the labels.


### Themes

We are almost finished, but our plot still has the boring default design. ggplot provides so called *themes*, which can be used to change the overall appearance of a plot without much effort. For example, `theme_linedraw()` uses a white background and black text:

```{r}
ggplot(gapminder_07, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10() +
  labs(title = "Life Expectancy and GDP per Capita (2007)",
       subtitle = "Selected Nations by Continent",
       x = "GDP per Capita, USD",
       y = "Life Expectancy, Years",
       caption = "Source: Gapminder",
       color = "Continent") +
  geom_text(aes(label = country), size = 2, 
            color = "black", check_overlap = TRUE) +
  theme_linedraw()
```

You can find an overview of the different themes in ggplot [here](https://ggplot2.tidyverse.org/reference/ggtheme.html). The **ggthemes** package even adds some additional themes.

In addition to ready-to-use themes, the `theme()` function also offers a wide [selection](https://ggplot2.tidyverse.org/reference/theme.html) of functions for manually changing individual elements. Let's remove the legend, change the font in the plot and enlarge the axis labels a bit:

```{r}
ggplot(gapminder_07, 
       aes(gdpPercap, lifeExp, color = continent)) +
  geom_point() +
  facet_wrap(~ continent, nrow = 1) +
  stat_smooth(formula = y ~ x, method = "lm", se = FALSE) + 
  scale_x_log10() +
  labs(title = "Life Expectancy and GDP per Capita (2007)",
       subtitle = "Selected Nations by Continent",
       x = "GDP per Capita, USD",
       y = "Life Expectancy, Years",
       caption = "Source: Gapminder",
       color = "Continent") +
  geom_text(aes(label = country), size = 2, 
            color = "black", check_overlap = TRUE) +
  theme_linedraw() +
  theme(legend.position = "none", 
        text = element_text(family = "Palatino"),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 10))
```

Now take a moment to compare this plot with the one we started with:

```{r}
ggplot(data = gapminder_07, 
       mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) +
  geom_point()
```

You can see that while some things are not *necessary* to create a plot, they can make a big difference in how much information we can derive from it. By the way, do you remember when we mentioned the Grammar of Graphics and said we will build plots layer by layer? This is exactly how you do it.


## The Tidyverse

Let's go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code:

```{r, eval=FALSE}
ggplot(data = flights, mapping = aes(x = carrier)) +
  geom_bar()
```

```{r, eval=FALSE}
ggplot(flights, aes(x = carrier)) +
  geom_bar()
```

You'll notice that both code segments create the same barplot, even though in the second segment we omitted the `data = ` and `mapping = ` code argument names. This is because the `ggplot()` function by default assumes that the `data` argument comes first and the `mapping` argument comes second. As long as you specify the data frame in question first and the `aes()` mapping second, you can omit the explicit statement of the argument names `data = ` and `mapping = `. 

Going forward for the rest of this book, all `ggplot()` code will be like the second segment: with the `data = ` and `mapping = ` explicit naming of the argument omitted with the default ordering of arguments respected. We'll do this for brevity's sake; it's common to see this style when reviewing other R users' code.

Data "in the wild" is never ready for visualization. We can't use all the beautiful plots that we learned in the previous chapter until we have "wrangled" the data into a convenient shape. In this chapter, we'll introduce a series of functions from the **tidyverse** collection of packages which help with wrangling, and everything else we need to do to work with data. Such functions include:

1. `filter()` a data frame's existing rows to only pick out a subset of them. For example, the `alaska_flights` data frame.
1. `select()` specific variable columns in a data set. For example, choose the `dep_delay` and `arr_delay` variables to more easily view the relationship between the two. Additional functions like `slice()` can further subset the data.
1. `arrange()` its rows. For example, sort the rows of `weather` in ascending or descending order of `temp`.
1. `group_by()` its rows. In other words, assign different rows to be part of the same *group*. We can then combine `group_by()` with `summarize()` to report summary statistics for each group *separately*. For example, say you don't want a single overall average departure delay `dep_delay` for all three `origin` airports combined, but rather three separate average departure delays, one computed for each of the three `origin` airports.
1. `mutate()` its existing columns/variables to create new ones. For example, convert hourly temperature recordings from degrees Fahrenheit to degrees Celsius.

<!-- DK: Mention summarize() -->

Notice how we used `computer_code` font to describe the actions we want to take on our data frames. This is because the **dplyr** package, one of the packages in the **tidyverse**, has intuitively verb-named functions that are easy to remember. 

There is a further benefit to learning to use the **dplyr** package for data wrangling: its similarity to the database querying language [SQL](https://en.wikipedia.org/wiki/SQL) (pronounced "sequel" or spelled out as "S", "Q", "L"). SQL (which stands for "Structured Query Language") is used to manage large databases quickly and efficiently and is widely used by many institutions with a lot of data. While SQL is a topic left for a book or a course on database management, keep in mind that once you learn **dplyr**, you can learn SQL easily. 

### The pipe operator: `%>%`

Before we start data wrangling, let's first introduce a nifty tool that gets loaded with the **dplyr** package included in the tidyverse: the pipe operator `%>%`. The pipe operator allows us to combine multiple operations in R into a single sequential *chain* of actions.

Recall from chapter 1 that to add the `geom` layer to your ggplot you had format your code as:

```{r, eval = FALSE}
ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +
  geom_bar(position = "dodge")
```

Without the `+` at the end of the first row, your computer would not know to continue onto the second. The same would occur without a pipe operator. For instance, take a look at the following code. You can run it in your own Rstudio console. What happens? 

```{r, eval = FALSE}
alaska_flights <- flights 
  filter(carrier == "AS")
```

Without the pipe operator, the `filter()` function cannot work because the computer does not know to use the flights dataset for the operation within the parentheses. This will become clearer with more practice using **dplyr** functions.  
<!-- EF: How do I use the index. It might be nice to define argument within the index -->

```{r, eval=FALSE}
alaska_flights <- flights %>% 
  filter(carrier == "AS")
```

The result will be the transformed/modified data frame that you want. In our example, we'll save the result in a new data frame by using the `<-` assignment operator with the name `alaska_flights` via `alaska_flights <-`. Because we assigned this modified data frame to `alaska_flights`, it is a separate entity from the initial `flights` data frame. If, however, we had written the code as `flights <- flights` you would have overwritten the previous data frame, and the original `flights` data from the **nycflights13* package would have to be re-installed to access it again.

Much like when adding layers to a `ggplot()` using the `+` sign, you form a single *chain* of data wrangling operations by combining verb-named functions into a single sequence using the pipe operator `%>%`. Furthermore, much like how the `+` sign has to come at the end of lines when constructing plots, the pipe operator `%>%` has to come at the end of lines as well. Note also that the pipe operator can be used multiple times sequentially. Simply include it at the end of your line, and the following function will be immediately linked to the output of the previous line containing the operator. We call the text within the parentheses the argument(s) of a function.

It is worth noting that most **dplyr** verbs, as well as most functions in the larger **tidyverse**, achieve this effect by always having their first argument be an input tibble. For example, look at `?dplyr::filter` to see that, for example, the first argument of `filter()` is a tibble named `.data`. So, we can rewrite the above code snippet as:

```{r, eval=FALSE}
alaska_flights <- flights %>% 
  filter(.data = ., carrier == "AS")
```

The "." serves as special role when using pipes. It represents the tibble which was "passed down" from the previous step in the pipe. Here, we are telling R that "." --- which is `flights` in this case --- is the first argument to `filter()`. Since argument names do not have to be used, we can rewrite this as:

```{r, eval=FALSE}
alaska_flights <- flights %>% 
  filter(., carrier == "AS")
```

You will almost never write code that looks like this, at least with simple **dplyr** verbs like `filter()`. But, behind the scenes, this is what is going on. And, in more advanced cases, we will need to use "." to refer to the passed-in tibble.

Keep in mind, there are many more advanced data wrangling functions than just the six listed in the introduction to this chapter; you'll see some examples of these in Section \@ref(other-verbs). However, just with these six verb-named functions you'll be able to perform a broad array of data wrangling tasks for the rest of this book.


### `filter()` rows

```{r filter, echo=FALSE, fig.cap="Diagram of filter() rows operation."}
knitr::include_graphics("01-visualization/images/filter.png")
```

The `filter()` function here works much like the "Filter" option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria.

We begin by focusing only on flights from New York City to Portland, Oregon.  The `dest` destination code (or airport code) for Portland, Oregon is `"PDX"`. Run the following and look at the results in RStudio's spreadsheet viewer to ensure that only flights heading to Portland are chosen here:

```{r, eval=FALSE}
portland_flights <- flights %>% 
  filter(dest == "PDX")
View(portland_flights)
```

Note the order of the code. First, take the `flights` data frame `flights` *then* `filter()` the data frame so that only those where the `dest` equals `"PDX"` are included. We test for equality using the double equal sign `==` and not a single equal sign `=`. In other words `filter(dest = "PDX")` will yield an error. This is a convention across many programming languages. If you are new to coding, you'll probably forget to use the double equal sign `==` a few times before you get the hang of it.

You can use other operators beyond just the `==` operator that tests for equality:

- `>` corresponds to "greater than"
- `<` corresponds to "less than"
- `>=` corresponds to "greater than or equal to"
- `<=` corresponds to "less than or equal to"
- `!=` corresponds to "not equal to." The `!` is used in many programming languages to indicate "not."

Furthermore, you can combine multiple criteria using operators that make comparisons:

- `|` corresponds to "or"
- `&` corresponds to "and"

To see many of these in action, let's filter `flights` for all rows that departed from JFK *and* were heading to Burlington, Vermont (`"BTV"`) or Seattle, Washington (`"SEA"`) *and* departed in the months of October, November, or December. Run the following:

```{r, eval=FALSE}
btv_sea_flights_fall <- flights %>% 
  filter(origin == "JFK" & 
           (dest == "BTV" | dest == "SEA") & 
           month >= 10)
View(btv_sea_flights_fall)
```

Note that even though colloquially speaking one might say "all flights leaving Burlington, Vermont *and* Seattle, Washington," in terms of computer operations, we really mean "all flights leaving Burlington, Vermont *or* leaving Seattle, Washington." For a given row in the data, `dest` can be `"BTV"`, or `"SEA"`, or something else, but not both `"BTV"` and `"SEA"` at the same time. Furthermore, note the careful use of parentheses around `dest == "BTV" | dest == "SEA"`.

We can often skip the use of `&` and just separate our conditions with a comma. The previous code will return the identical output `btv_sea_flights_fall` as the following code:

```{r, eval=FALSE}
btv_sea_flights_fall <- flights %>% 
  filter(origin == "JFK", 
         (dest == "BTV" | dest == "SEA"), 
         month >= 10)         
View(btv_sea_flights_fall)
```

Let's present another example that uses the `!` "not" operator to pick rows that *don't* match a criteria. As mentioned earlier, the `!` can be read as "not." Here we are filtering rows corresponding to flights that didn't go to Burlington, VT or Seattle, WA.

```{r, eval=FALSE}
not_BTV_SEA <- flights %>% 
  filter(!(dest == "BTV" | dest == "SEA"))
View(not_BTV_SEA)
```

Again, note the careful use of parentheses around the `(dest == "BTV" | dest == "SEA")`. If we didn't use parentheses as follows:

```{r, eval=FALSE}
flights %>% filter(!dest == "BTV" | dest == "SEA")
```

We would be returning all flights not headed to `"BTV"` *or* those headed to `"SEA"`, which is an entirely different resulting data frame. 

Now say we have a larger number of airports we want to filter for, say `"SEA"`, `"SFO"`, `"PDX"`, `"BTV"`, and `"BDL"`. We could continue to use the `|` (*or*) operator:

```{r, eval=FALSE}
many_airports <- flights %>% 
  filter(dest == "SEA" | dest == "SFO" | dest == "PDX" | 
         dest == "BTV" | dest == "BDL")
```

but as we progressively include more airports, this will get unwieldy to write. A slightly shorter approach uses the `%in%` operator along with the `c()` function. Recall from Subsection \@ref(programming-concepts) that the `c()` function "combines" or "concatenates" values into a single *vector* of values.

```{r, eval=FALSE}
many_airports <- flights %>% 
  filter(dest %in% c("SEA", "SFO", "PDX", "BTV", "BDL"))
View(many_airports)
```

One of the most common mistakes is to use `==` in this `filter()` rather than `%in%`. This is diabolical since it won't fail and, in certain situations, it might not even issue a warning. Beware!

What this code is doing is filtering `flights` for all flights where `dest` is in the vector of airports `c("BTV", "SEA", "PDX", "SFO", "BDL")`. Both outputs of `many_airports` are the same, but as you can see the latter takes much less energy to code. The `%in%` operator is useful for looking for matches commonly in one vector/variable compared to another.

As a final note, we recommend that `filter()` should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations you care about. 

<!-- EF: Should we include something hear about NA values? -->

### `select` variables

```{r, echo=FALSE, fig.cap="Diagram of select() columns."}
knitr::include_graphics("01-visualization/images/select.png")
```

Using the `filter()` function we were able to pick out specific *rows* from the dataset. The `select()` function allows R users to pick specific *columns* (or variables) instead.

We've seen that the `flights` data frame in the `nycflights13` package contains 19 different variables. You can identify the names of these 19 variables by running the `glimpse()` function from the **dplyr** package:

```{r, eval=FALSE}
glimpse(flights)
```

However, say you only need two of these 19 variables, say `carrier` and `flight`. You can `select()`  these two variables:

```{r}
flights %>% 
  select(carrier, flight)
```

This function makes it easier to explore large datasets since it allows us to limit the scope to only those variables we care most about. For example, if we `select()` only a smaller number of variables, it will make viewing the dataset in RStudio's spreadsheet viewer more digestible. 

Let's say instead you want to drop, or de-select, certain variables. For example, consider the variable `year` in the `flights` data frame. This variable isn't quite a "variable" because it is always `2013` and hence doesn't change. Say you want to remove this variable from the data frame. We can deselect `year` by using the `-` sign:

```{r}
flights %>% 
  select(-year)
```

Another way of selecting columns/variables is by specifying a range of columns:

```{r, eval=FALSE}
flights %>% 
  select(year:day, air_time:minute)
```

This will `select()` all 3 columns between `year` and `day`, as well as all 4 columns ebtween  `air_time` and `minute`. 

The `select()` function can also be used to rearrange columns when used with the `everything()` helper function.  For example, suppose we want the `hour`, `minute`, and `time_hour` variables to appear immediately after the `year`, `month`, and `day` variables, while not discarding the rest of the variables. In the following code, `everything()` will pick up all remaining variables: 

```{r}
flights %>% 
  select(year, month, day, hour, minute, time_hour, everything())
```

Lastly, the helper functions `starts_with()`, `ends_with()`, and `contains()` can be used to select variables/columns that match those conditions. Examples:

```{r}
flights %>% 
  select(starts_with("a"))
```

```{r}
flights %>% 
  select(ends_with("delay"))
```

```{r}
flights %>% 
  select(contains("time"))
```


### `slice()` and `pull()` and `[]`

`slice()` and `pull()` are additional functions that you can use to pick out specific rows or columns within a data frame.

Using `slice()` gives us specific rows from the `flights` tibble:

```{r}
slice(flights, 2:5)
```

Unlike `filter()`, `slice()` relies on numeric order of the data. 

`pull()` grabs out a variable as a vector, rather than leaving it within a tibble, as `select()` does:

```{r}
slice(flights, 2:5) %>% 
  pull(dep_time)
```

This is often handy when you want to feed the data into a function, like `mean()` which requires a vector as input:

```{r}
slice(flights, 2:5) %>% 
  pull(dep_time) %>% 
  mean()
```

The most common way to subset vectors is to use the "bracket" operator `[]`. Example:

```{r}
flights$dep_time[2:5]
```

### `arrange()` rows

One of the most commonly performed data wrangling tasks is to sort a data frame's rows in the alphanumeric order of one of the variables. Unlike `filter()` or `select()`, `arrange()` does not remove any rows or columns from the data frame. Instead, the **dplyr** package's `arrange()` function allows us to sort/reorder a data frame's rows according to the values of the specified variable.

Suppose we are interested in determining the flight that covers that most distance from all domestic flights departing from New York City in 2013:

First, let's `select()` some pertinent variables to make the data more easy to read. 

```{r, eval}
flights_dist <- flights %>% 
  select(origin, dest, air_time, distance)
flights_dist
```

The order of this data appears is maintained from the original `flights` data set. Say instead we would like to see the same data, but sorted for the `distance` of each flight (from the farthest to the shortest distance).

```{r}
flights_dist %>% 
  arrange(distance)
```

This is, however, the opposite of what we want. The rows are sorted with the flights covering the least distance displayed first. This is because `arrange()` always returns rows sorted in ascending order by default. To switch the ordering to be in "descending" order instead, we use the `desc()` function like so:

```{r}
flights_dist %>% 
  arrange(desc(distance))
```
 
Let's try this one more time with a character variable. What happens when we try to sort by the destination (`dest`) variable?

```{r}
flights_dist %>%
  arrange(dest)
```

As you can see, character variables will be sorted alphabetically. Using the `desc()` helper function with a character variable, will sort the destinations reverse alphabetically. 


### `mutate()` 

```{r, echo=FALSE, fig.cap="Diagram of mutate() columns."}
knitr::include_graphics("01-visualization/images/mutate.png")
```

Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius (&deg;C) instead of degrees Fahrenheit (&deg;F). The formula to convert temperatures from &deg;F to &deg;C is

$$
\text{temp in C} = \frac{\text{temp in F} - 32}{1.8}
$$

We can apply this formula to the `temp` variable using the `mutate()` function from the **dplyr** package, which takes existing variables and mutates them to create new ones. 

```{r, eval=TRUE}
weather <- weather %>% 
  mutate(temp_in_C = (temp - 32) / 1.8)
```

In this code, we `mutate()` the `weather` data frame by creating a new variable `temp_in_C = (temp - 32) / 1.8` and then *overwrite* the original `weather` data frame. Why did we overwrite the data frame `weather`, instead of assigning the result to a new data frame like `weather_new`? As a rough rule of thumb, as long as you are not losing original information that you might need later, it's acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable `temp`, but instead created a new variable called `temp_in_C`?  Because if we did this, we would have erased the original information contained in `temp` of temperatures in Fahrenheit that may still be valuable to us.

Let's now compute monthly average temperatures in both &deg;F and &deg;C using the `group_by()` and `summarize()` code we saw in Section \@ref(groupby):

```{r}
summary_monthly_temp <- weather %>% 
  group_by(month) %>% 
  summarize(mean_temp_in_F = mean(temp, na.rm = TRUE), 
            mean_temp_in_C = mean(temp_in_C, na.rm = TRUE))
summary_monthly_temp
```

Let's consider another example. Passengers are often frustrated when their flight departs late, but aren't as annoyed if, in the end, pilots can make up some time during the flight.  This is known in the airline industry as _gain_, and we will create this variable using the `mutate()` function:

```{r}
flights <- flights %>% 
  mutate(gain = dep_delay - arr_delay)
```

Let's take a look at only the `dep_delay`, `arr_delay`, and the resulting `gain` variables for the first 5 rows in our updated `flights` data frame in Table \@ref(tab:first-five-flights).

```{r first-five-flights, echo=FALSE}
flights %>% 
  select(dep_delay, arr_delay, gain) %>% 
  slice(1:5)
```

The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its "gained time in the air" is a loss of 9 minutes, hence its `gain` is 2 - 11 = -9. On the other hand, the flight in the fourth row departed a minute early (`dep_delay` of -1) but arrived 18 minutes early (`arr_delay` of -18), so its "gained time in the air" is $-1 - (-18) = -1 + 18 = 17$ minutes, hence its `gain` is +17.

Recall from Section \@ref(histograms) that since `gain` is a numerical variable, we can visualize its distribution using a histogram.  

```{r gain-hist, message=FALSE, fig.cap="Histogram of gain variable.", fig.height=3}
ggplot(data = flights, mapping = aes(x = gain)) +
  geom_histogram(color = "white", bins = 20)
```

The resulting histogram in Figure \@ref(fig:gain-hist) provides a different perspective on the `gain` variable than the summary statistics we computed earlier. For example, note that most values of `gain` are right around 0. 

To close out our discussion on the `mutate()` function to create new variables, note that we can create multiple new variables at once in the same `mutate()` code. Furthermore, within the same `mutate()` code we can refer to new variables we just created. As an example, consider this code from @rds2016:

```{r}
flights <- flights %>% 
  mutate(gain = dep_delay - arr_delay,
         hours = air_time / 60,
         gain_per_hour = gain / hours)
```

#### `ifelse()`

<!-- Discuss case_when()? -->

<!-- DK: Edit these lines from R4DS. Are there other functions which we should mention here? Do something sensible with group_by. To get `_` to work in caption title. Found at https://github.com/rstudio/bookdown/issues/209  -->

`ifelse()` has three arguments. The first argument `test` should be a logical vector. The result will contain the value of the second argument, `yes`, when test is TRUE, and the value of the third argument, `no`, when it is FALSE. Imagine that we want to create a new variable `E`, which is TRUE when the color of the diamond is "E" and FALSE otherwise. 


```{r}
diamonds %>% 
  select(carat, color, price) %>% 
  mutate(E = ifelse(color == "E", TRUE, FALSE))
```


Alternatively to `ifelse()`, use `dplyr::case_when()`. `case_when()` is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables. Note that there is a more robust version of `ifelse()` in **dplyr**: `if_else()`. This works exactly the same as the standard version but is somewhat more robust. 

<!-- DK: Is this useful? -->

```{r groupsummarize, echo=FALSE, fig.cap="(ref:groupby)", purl=FALSE, fig.height=2.5}
knitr::include_graphics("01-visualization/images/group_summary.png")
```

### `summarize()`

The next common task when working with data frames is to compute *summary statistics*. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (also called the average) and the median (the middle value). Other examples of summary statistics that might not immediately come to mind include the *sum*, the smallest value also called the *minimum*, the largest value also called the *maximum*, and the *standard deviation*.

Return to a familiar dataset. Let's calculate two summary statistics of the `temp` temperature variable in the `weather` data frame: the mean and standard deviation. To compute these summary statistics, we need the `mean()` and `sd()` *summary functions* in R. Summary functions in R take in many values and return a single value.

Recall the output of the `summary()` function. 

```{r, echo = FALSE}
summary(weather)
```

This function offers an array of summary statistics for each of the columns of the dataset, but `summarize()` (or alternatively `summarise()`) allows us to calculate these statistics on individual columns of the dataset. 

More precisely, we'll use the `mean()` and `sd()` summary functions within the `summarize()`  function from the **dplyr** package. Note you can also use the British English spelling of `summarise()`. The `summarize()` function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics. 

We'll save the results in a new data frame called `summary_temp` that will have two columns/variables: the `mean` and the `std_dev`:

```{r, eval=TRUE}
summary_temp <- weather %>% 
  summarize(mean = mean(temp), std_dev = sd(temp))
summary_temp
```

Why are the values returned `NA`?``NA` is how R encodes *missing values*  where `NA` indicates "not available" or "not applicable." If a value for a particular row and a particular column does not exist, `NA` is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it? Perhaps the data was not collected at all because it was too difficult to do so? Perhaps there was an erroneous value that someone entered that has been corrected to read as missing? You'll often encounter issues with missing values when working with real data.

Going back to our `summary_temp` output, by default any time you try to calculate a summary statistic of a variable that has one or more `NA` missing values in R, `NA` is returned. To work around this fact, you can set the `na.rm` argument to `TRUE`, where `rm` is short for "remove"; this will ignore any `NA` missing values and only return the summary value for all non-missing values. 

The code that follows computes the mean and standard deviation of all non-missing values of `temp`:

```{r}
summary_temp <- weather %>% 
  summarize(mean = mean(temp, na.rm = TRUE), 
            std_dev = sd(temp, na.rm = TRUE))
summary_temp
```

Notice how the `na.rm = TRUE` are used as arguments to the `mean()` and `sd()` summary functions individually, and not to the `summarize()` function. 

However, one needs to be cautious whenever ignoring missing values as we've just done. There are possible ramifications of blindly sweeping rows with missing values "under the rug." This is in fact why the `na.rm` argument to any summary statistic function in R is set to `FALSE` by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis.

### Basic statistical terms

<!-- DK: Maybe ntile()? Others? Discuss na.rm = TRUE; quantile; is.na(); acting on logical vectors -->

What are other summary functions we can use inside the `summarize()` verb to compute summary statistics? You can use any function in R that takes many values and returns just one. Here are just a few:

* mean(): the average
* min() and max(): the minimum and maximum values, respectively
* sd(): the standard deviation, which is a measure of spread
* sum(): the total amount when adding multiple numbers
* n(): a count of the number of rows  
* n_distinct(): the number of distinct values



**`mean()`**

The *mean* is the most commonly reported measure of center.  It is commonly called the *average* though this term can be a little ambiguous.  The mean is the sum of all of the data elements divided by how many elements there are. If we have $n$ data points, the mean is given by: 

$$Mean = \frac{x_1 + x_2 + \cdots + x_n}{n}$$

**`median()`**

The median is calculated by first sorting a variable's data from smallest to largest.  After sorting the data, the middle element in the list is the *median*.  If the middle falls between two values, then the median is the mean of those two middle values.

**`sd()`**

We will next discuss the *standard deviation* ($sd$) of a variable.  The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far we expect a given data value will be from its mean:

$$sd = \sqrt{\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \cdots + (x_n - Mean)^2}{n - 1}}$$


Let's return to the `gain` variable from the previous chapter and look at some summary statistics by considering multiple summary functions at once in the same `summarize()` code:

```{r}
flights %>% 
  mutate(gain = arr_delay - dep_delay) %>% 
  summarize(min = min(gain, na.rm = TRUE),
            q1 = quantile(gain, 0.25, na.rm = TRUE),
            median = quantile(gain, 0.5, na.rm = TRUE),
            q3 = quantile(gain, 0.75, na.rm = TRUE),
            max = max(gain, na.rm = TRUE),
            mean = mean(gain, na.rm = TRUE),
            sd = sd(gain, na.rm = TRUE),
            missing = sum(is.na(gain)))
```

We see for example that the average gain is +5 minutes, while the largest is +109 minutes! However, this code would take some time to type out in practice. 

We can also run summary statistics `across()` multiple columns at the same time. To get a better understanding of the `across()` helper function, run `?across` in your console and see what arguments it takes. Suppose we wanted to take the `mean()` of both the `temp` and the `dewp` variables. 

```{r}
weather %>%
  summarize(across(c(temp, dewp), mean))
```

Again, we encounter `NA` values missing values, and have to set `na.rm = TRUE` as an argument for the `mean()` function. 

<!-- Explain the  ~ and . thoroughly. --> 

```{r}
weather %>%
  summarize(across(c(temp, dewp), ~mean(., na.rm = TRUE)))
```

```{r}
weather %>%
  summarize(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))
```

### `group_by()`

Say instead of a single mean temperature for the whole year, you would like 12 mean temperatures, one for each of the 12 months separately. In other words, we would like to compute the mean temperature split by month. We can do this by "grouping" temperature observations by the values of another variable, in this case by the 12 values of the variable `month`. Run the following code:

```{r}
weather %>% 
  group_by(month) %>% 
  summarize(mean = mean(temp, na.rm = TRUE), 
            std_dev = sd(temp, na.rm = TRUE))
```

<!-- DK: This whole section stinks. Please fix! -->

Notice the warning. R is trying to save us from ourselves. The warning means that the tibble which issues forth from the end of the pipe has been ungrouped, meaning that it no longer has any group attribute. This is probably what we want to have happen, which is why it is the default behavior. But there are many cases in which we want to keep the group attribute, i.e., we want the resulting tibble to still be grouped by month. The warning is urging us to be sure what we want. The proper way to handle the situation, here and everywhere else that we use `group_by()` and `summarize()`, is to specify the `.groups` argument.

```{r}
weather %>% 
  group_by(month) %>% 
  summarize(mean = mean(temp, na.rm = TRUE), 
            std_dev = sd(temp, na.rm = TRUE),
            .groups = "drop")
```

This code does the same thing as the first version, but does not issue a warning, since we have made an affirmative decision to drop any grouping variables. See `?summarize` for a discussion of other possible values for `.groups`.

This code is identical to the previous code that created `summary_temp`, but with an extra `group_by(month)` added before the `summarize()`. Grouping the `weather` dataset by `month` and then applying the `summarize()` functions yields a data frame that displays the mean and standard deviation temperature split by the 12 months of the year.

It is important to note that the `group_by()` function doesn't change data frames by itself. Rather it changes the *meta-data*, or data about the data, specifically the grouping structure. It is only after we apply the `summarize()` function that the data frame changes. 

Run this code (do not forget to load its package `nycflights13` in your console if you have not already):

```{r}
flights
```

Observe that the first line of the output reads `# A tibble: 336,776 x 20`. This is an example of meta-data, in this case the number of observations/rows and variables/columns in `flights`. The actual data itself are the subsequent table of values. Now let's pipe the `flights` data frame into `group_by(origin)`:

```{r, eval=TRUE}
flights %>% 
  group_by(origin)
```

Observe that now there is additional meta-data: `# Groups: origin [3]` indicating that the grouping structure meta-data has been set based on the 3 possible levels of the categorical variable `origin`: `"EWR"`, `"JFK"`, and `"LGA"`. On the other hand, observe that the data has not changed: it is still a table of 336,776 $\times$ 19 values.

Only by combining a `group_by()` with another data wrangling operation, in this case `summarize()`, will the data actually be transformed. 

Let's revisit the `n()` counting summary function we briefly introduced previously. Recall that the `n()` function counts rows. This is opposed to the `sum()` summary function that returns the sum of a numerical variable. For example, suppose we'd like to count how many flights departed each of the three airports in New York City:

```{r, eval=TRUE}
by_origin <- flights %>% 
  group_by(origin) %>% 
  summarize(count = n(), 
            .groups = "drop")

by_origin
```

We see that Newark (`"EWR"`) had the most flights departing in 2013 followed by `"JFK"` and lastly by LaGuardia (`"LGA"`). Note there is a subtle but important difference between `sum()` and `n()`; while `sum()` returns the sum of a numerical variable, `n()` returns a count of the number of rows/observations. 

If you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the `ungroup()` function:

```{r, eval=TRUE}
flights %>% 
  group_by(origin) %>% 
  ungroup()
```

Observe how the `# Groups: origin [3]` meta-data is no longer present. 

#### Grouping by more than one variable

You are not limited to grouping by one variable. Say you want to know the number of flights leaving each of the three New York City airports *for each month*. We can also group by a second variable `month` using `group_by(origin, month)`:

```{r}
flights %>% 
  group_by(origin, month) %>% 
  summarize(count = n(),
            .groups = "drop")
```

Observe that there are 36 rows to `by_origin_monthly` because there are 12 months for 3 airports (`EWR`, `JFK`, and `LGA`). 

Why do we `group_by(origin, month)` and not `group_by(origin)` and then `group_by(month)`? Let's investigate:

```{r}
flights %>% 
  group_by(origin) %>% 
  group_by(month) %>% 
  summarize(count = n(),
            .groups = "drop")
```

What happened here is that the second `group_by(month)` overwrote the grouping structure meta-data of the earlier `group_by(origin)`, so that in the end we are only grouping by `month`. The lesson here is if you want to `group_by()` two or more variables, you should include all the variables at the same time in the same `group_by()` adding a comma between the variable names.

## Summary

We began by showing you this plot.

```{r, echo = FALSE}
gap_p
```

This back to how impressed you were! You now have the tools to create plots like this on your own. Just run this code:

```{r, eval = FALSE}
library(tidyverse)
library(scales)
library(gapminder)

gap_p <- gapminder %>%
  filter(continent != "Oceania") %>%
  filter(year == max(year)) %>% 
  mutate(continent = fct_relevel(continent, 
                                 c("Americas", "Europe"), 
                                 after = 0)) %>% 
  ggplot(aes(gdpPercap, lifeExp, size = pop, color = continent)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", 
                formula = y ~ x,
                se = FALSE) + 
    facet_wrap(~continent, nrow = 2) +
    labs(title = "Life Expectancy and GDP per Capita in 2007",
         subtitle = "Connection between GDP and life expectancy is weakest in Africa",
         x = "GDP per Capita in USD",
         y = "Life Expectancy",
         caption = "Data Source: gapminder R package") +
    scale_x_log10(breaks = c(500, 5000, 50000),
                  labels = scales::dollar_format(accuracy = 1)) +
    theme(legend.position = "none")

gap_p
```


We looked at basic concepts and terms we are dealing with when programming with R. We learned about the three basic components that make up each plot: data, mapping, and one or more geoms. The **ggplot2** package offers a wide range of geoms that we can use to create different types of plots. We explored advanced plotting features such as axis scaling, labeling and themes. Finally, we examined the "super package" **tidyverse**, which also includes **ggplot2**. Besides tools for visualization, it offers features for importing and manipulating data, which is the main topic of the next chapter.

Finally, it should be mentioned that we have only seen a small part of what R offers. For example, we can use the **gganimate** package to bring a slightly modified version of our gapminder plot to life:

```{r, cache = TRUE, out.width = "100%"}
library(gganimate)

gapminder %>%
  filter(continent != "Oceania") %>%
  ggplot(aes(gdpPercap, lifeExp, size = pop, color = continent)) +
    geom_point(show.legend = FALSE, alpha = 0.7) +
    facet_wrap(~continent, nrow = 1) +
    scale_size(range = c(2, 12)) +
    scale_x_log10() +
    labs(subtitle = "Life Expectancy and GDP per Capita (1952-2007)",
         x = "GDP per Capita, USD",
         y = "Life Expectancy, Years") +
    theme_linedraw() +
    transition_time(year) +
    labs(title = "Year: {frame_time}") +
    shadow_wake(wake_length = 0.1, alpha = FALSE)

```

The **plotly** package makes our plot interactive. Click on it and explore!

<!-- DK: I don't like getting rid of the (concerning!) warning by force. Is plotly maintained? -->

```{r, cache = TRUE, message = FALSE, warning = FALSE, out.width = "100%"}
library(plotly)

ggplotly(gap_p)
```


A beautiful plot is just a collection of steps, each simple enough on its own. We have taught you (some of) these steps. Time to start walking on your own.
